{"version":3,"file":"index.node.mjs","sources":["index.node.mjs"],"sourcesContent":["import { _getProvider, getApp, _removeServiceInstance, _registerComponent, registerVersion, SDK_VERSION as SDK_VERSION$1 } from '@firebase/app';\nimport { Component } from '@firebase/component';\nimport { Logger, LogLevel } from '@firebase/logger';\nimport { inspect, TextEncoder, TextDecoder } from 'util';\nimport { FirebaseError, getUA, isIndexedDBAvailable, isSafari, createMockUserToken, getModularInstance, deepEqual } from '@firebase/util';\nimport { randomBytes as randomBytes$1 } from 'crypto';\nimport * as grpc from '@grpc/grpc-js';\nimport * as protoLoader from '@grpc/proto-loader';\n\nconst name = \"@firebase/firestore\";\nconst version$1 = \"3.5.0\";\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Simple wrapper around a nullable UID. Mostly exists to make code more\r\n * readable.\r\n */\r\nclass User {\r\n    constructor(uid) {\r\n        this.uid = uid;\r\n    }\r\n    isAuthenticated() {\r\n        return this.uid != null;\r\n    }\r\n    /**\r\n     * Returns a key representing this user, suitable for inclusion in a\r\n     * dictionary.\r\n     */\r\n    toKey() {\r\n        if (this.isAuthenticated()) {\r\n            return 'uid:' + this.uid;\r\n        }\r\n        else {\r\n            return 'anonymous-user';\r\n        }\r\n    }\r\n    isEqual(otherUser) {\r\n        return otherUser.uid === this.uid;\r\n    }\r\n}\r\n/** A user with a null UID. */\r\nUser.UNAUTHENTICATED = new User(null);\r\n// TODO(mikelehen): Look into getting a proper uid-equivalent for\r\n// non-FirebaseAuth providers.\r\nUser.GOOGLE_CREDENTIALS = new User('google-credentials-uid');\r\nUser.FIRST_PARTY = new User('first-party-uid');\r\nUser.MOCK_USER = new User('mock-user');\n\nconst version = \"9.10.0\";\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nlet SDK_VERSION = version;\r\nfunction setSDKVersion(version) {\r\n    SDK_VERSION = version;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Formats an object as a JSON string, suitable for logging. */\r\nfunction formatJSON(value) {\r\n    // util.inspect() results in much more readable output than JSON.stringify()\r\n    return inspect(value, { depth: 100 });\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst logClient = new Logger('@firebase/firestore');\r\n// Helper methods are needed because variables can't be exported as read/write\r\nfunction getLogLevel() {\r\n    return logClient.logLevel;\r\n}\r\n/**\r\n * Sets the verbosity of Cloud Firestore logs (debug, error, or silent).\r\n *\r\n * @param logLevel - The verbosity you set for activity and error logging. Can\r\n *   be any of the following values:\r\n *\r\n *   <ul>\r\n *     <li>`debug` for the most verbose logging level, primarily for\r\n *     debugging.</li>\r\n *     <li>`error` to log errors only.</li>\r\n *     <li><code>`silent` to turn off logging.</li>\r\n *   </ul>\r\n */\r\nfunction setLogLevel(logLevel) {\r\n    logClient.setLogLevel(logLevel);\r\n}\r\nfunction logDebug(msg, ...obj) {\r\n    if (logClient.logLevel <= LogLevel.DEBUG) {\r\n        const args = obj.map(argToString);\r\n        logClient.debug(`Firestore (${SDK_VERSION}): ${msg}`, ...args);\r\n    }\r\n}\r\nfunction logError(msg, ...obj) {\r\n    if (logClient.logLevel <= LogLevel.ERROR) {\r\n        const args = obj.map(argToString);\r\n        logClient.error(`Firestore (${SDK_VERSION}): ${msg}`, ...args);\r\n    }\r\n}\r\n/**\r\n * @internal\r\n */\r\nfunction logWarn(msg, ...obj) {\r\n    if (logClient.logLevel <= LogLevel.WARN) {\r\n        const args = obj.map(argToString);\r\n        logClient.warn(`Firestore (${SDK_VERSION}): ${msg}`, ...args);\r\n    }\r\n}\r\n/**\r\n * Converts an additional log parameter to a string representation.\r\n */\r\nfunction argToString(obj) {\r\n    if (typeof obj === 'string') {\r\n        return obj;\r\n    }\r\n    else {\r\n        try {\r\n            return formatJSON(obj);\r\n        }\r\n        catch (e) {\r\n            // Converting to JSON failed, just log the object directly\r\n            return obj;\r\n        }\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Unconditionally fails, throwing an Error with the given message.\r\n * Messages are stripped in production builds.\r\n *\r\n * Returns `never` and can be used in expressions:\r\n * @example\r\n * let futureVar = fail('not implemented yet');\r\n */\r\nfunction fail(failure = 'Unexpected state') {\r\n    // Log the failure in addition to throw an exception, just in case the\r\n    // exception is swallowed.\r\n    const message = `FIRESTORE (${SDK_VERSION}) INTERNAL ASSERTION FAILED: ` + failure;\r\n    logError(message);\r\n    // NOTE: We don't use FirestoreError here because these are internal failures\r\n    // that cannot be handled by the user. (Also it would create a circular\r\n    // dependency between the error and assert modules which doesn't work.)\r\n    throw new Error(message);\r\n}\r\n/**\r\n * Fails if the given assertion condition is false, throwing an Error with the\r\n * given message if it did.\r\n *\r\n * Messages are stripped in production builds.\r\n */\r\nfunction hardAssert(assertion, message) {\r\n    if (!assertion) {\r\n        fail();\r\n    }\r\n}\r\n/**\r\n * Fails if the given assertion condition is false, throwing an Error with the\r\n * given message if it did.\r\n *\r\n * The code of callsites invoking this function are stripped out in production\r\n * builds. Any side-effects of code within the debugAssert() invocation will not\r\n * happen in this case.\r\n *\r\n * @internal\r\n */\r\nfunction debugAssert(assertion, message) {\r\n    if (!assertion) {\r\n        fail();\r\n    }\r\n}\r\n/**\r\n * Casts `obj` to `T`. In non-production builds, verifies that `obj` is an\r\n * instance of `T` before casting.\r\n */\r\nfunction debugCast(obj, \r\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\r\nconstructor) {\r\n    return obj;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst Code = {\r\n    // Causes are copied from:\r\n    // https://github.com/grpc/grpc/blob/bceec94ea4fc5f0085d81235d8e1c06798dc341a/include/grpc%2B%2B/impl/codegen/status_code_enum.h\r\n    /** Not an error; returned on success. */\r\n    OK: 'ok',\r\n    /** The operation was cancelled (typically by the caller). */\r\n    CANCELLED: 'cancelled',\r\n    /** Unknown error or an error from a different error domain. */\r\n    UNKNOWN: 'unknown',\r\n    /**\r\n     * Client specified an invalid argument. Note that this differs from\r\n     * FAILED_PRECONDITION. INVALID_ARGUMENT indicates arguments that are\r\n     * problematic regardless of the state of the system (e.g., a malformed file\r\n     * name).\r\n     */\r\n    INVALID_ARGUMENT: 'invalid-argument',\r\n    /**\r\n     * Deadline expired before operation could complete. For operations that\r\n     * change the state of the system, this error may be returned even if the\r\n     * operation has completed successfully. For example, a successful response\r\n     * from a server could have been delayed long enough for the deadline to\r\n     * expire.\r\n     */\r\n    DEADLINE_EXCEEDED: 'deadline-exceeded',\r\n    /** Some requested entity (e.g., file or directory) was not found. */\r\n    NOT_FOUND: 'not-found',\r\n    /**\r\n     * Some entity that we attempted to create (e.g., file or directory) already\r\n     * exists.\r\n     */\r\n    ALREADY_EXISTS: 'already-exists',\r\n    /**\r\n     * The caller does not have permission to execute the specified operation.\r\n     * PERMISSION_DENIED must not be used for rejections caused by exhausting\r\n     * some resource (use RESOURCE_EXHAUSTED instead for those errors).\r\n     * PERMISSION_DENIED must not be used if the caller can not be identified\r\n     * (use UNAUTHENTICATED instead for those errors).\r\n     */\r\n    PERMISSION_DENIED: 'permission-denied',\r\n    /**\r\n     * The request does not have valid authentication credentials for the\r\n     * operation.\r\n     */\r\n    UNAUTHENTICATED: 'unauthenticated',\r\n    /**\r\n     * Some resource has been exhausted, perhaps a per-user quota, or perhaps the\r\n     * entire file system is out of space.\r\n     */\r\n    RESOURCE_EXHAUSTED: 'resource-exhausted',\r\n    /**\r\n     * Operation was rejected because the system is not in a state required for\r\n     * the operation's execution. For example, directory to be deleted may be\r\n     * non-empty, an rmdir operation is applied to a non-directory, etc.\r\n     *\r\n     * A litmus test that may help a service implementor in deciding\r\n     * between FAILED_PRECONDITION, ABORTED, and UNAVAILABLE:\r\n     *  (a) Use UNAVAILABLE if the client can retry just the failing call.\r\n     *  (b) Use ABORTED if the client should retry at a higher-level\r\n     *      (e.g., restarting a read-modify-write sequence).\r\n     *  (c) Use FAILED_PRECONDITION if the client should not retry until\r\n     *      the system state has been explicitly fixed. E.g., if an \"rmdir\"\r\n     *      fails because the directory is non-empty, FAILED_PRECONDITION\r\n     *      should be returned since the client should not retry unless\r\n     *      they have first fixed up the directory by deleting files from it.\r\n     *  (d) Use FAILED_PRECONDITION if the client performs conditional\r\n     *      REST Get/Update/Delete on a resource and the resource on the\r\n     *      server does not match the condition. E.g., conflicting\r\n     *      read-modify-write on the same resource.\r\n     */\r\n    FAILED_PRECONDITION: 'failed-precondition',\r\n    /**\r\n     * The operation was aborted, typically due to a concurrency issue like\r\n     * sequencer check failures, transaction aborts, etc.\r\n     *\r\n     * See litmus test above for deciding between FAILED_PRECONDITION, ABORTED,\r\n     * and UNAVAILABLE.\r\n     */\r\n    ABORTED: 'aborted',\r\n    /**\r\n     * Operation was attempted past the valid range. E.g., seeking or reading\r\n     * past end of file.\r\n     *\r\n     * Unlike INVALID_ARGUMENT, this error indicates a problem that may be fixed\r\n     * if the system state changes. For example, a 32-bit file system will\r\n     * generate INVALID_ARGUMENT if asked to read at an offset that is not in the\r\n     * range [0,2^32-1], but it will generate OUT_OF_RANGE if asked to read from\r\n     * an offset past the current file size.\r\n     *\r\n     * There is a fair bit of overlap between FAILED_PRECONDITION and\r\n     * OUT_OF_RANGE. We recommend using OUT_OF_RANGE (the more specific error)\r\n     * when it applies so that callers who are iterating through a space can\r\n     * easily look for an OUT_OF_RANGE error to detect when they are done.\r\n     */\r\n    OUT_OF_RANGE: 'out-of-range',\r\n    /** Operation is not implemented or not supported/enabled in this service. */\r\n    UNIMPLEMENTED: 'unimplemented',\r\n    /**\r\n     * Internal errors. Means some invariants expected by underlying System has\r\n     * been broken. If you see one of these errors, Something is very broken.\r\n     */\r\n    INTERNAL: 'internal',\r\n    /**\r\n     * The service is currently unavailable. This is a most likely a transient\r\n     * condition and may be corrected by retrying with a backoff.\r\n     *\r\n     * See litmus test above for deciding between FAILED_PRECONDITION, ABORTED,\r\n     * and UNAVAILABLE.\r\n     */\r\n    UNAVAILABLE: 'unavailable',\r\n    /** Unrecoverable data loss or corruption. */\r\n    DATA_LOSS: 'data-loss'\r\n};\r\n/** An error returned by a Firestore operation. */\r\nclass FirestoreError extends FirebaseError {\r\n    /** @hideconstructor */\r\n    constructor(\r\n    /**\r\n     * The backend error code associated with this error.\r\n     */\r\n    code, \r\n    /**\r\n     * A custom error description.\r\n     */\r\n    message) {\r\n        super(code, message);\r\n        this.code = code;\r\n        this.message = message;\r\n        // HACK: We write a toString property directly because Error is not a real\r\n        // class and so inheritance does not work correctly. We could alternatively\r\n        // do the same \"back-door inheritance\" trick that FirebaseError does.\r\n        this.toString = () => `${this.name}: [code=${this.code}]: ${this.message}`;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass Deferred {\r\n    constructor() {\r\n        this.promise = new Promise((resolve, reject) => {\r\n            this.resolve = resolve;\r\n            this.reject = reject;\r\n        });\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass OAuthToken {\r\n    constructor(value, user) {\r\n        this.user = user;\r\n        this.type = 'OAuth';\r\n        this.headers = new Map();\r\n        this.headers.set('Authorization', `Bearer ${value}`);\r\n    }\r\n}\r\n/**\r\n * A CredentialsProvider that always yields an empty token.\r\n * @internal\r\n */\r\nclass EmptyAuthCredentialsProvider {\r\n    getToken() {\r\n        return Promise.resolve(null);\r\n    }\r\n    invalidateToken() { }\r\n    start(asyncQueue, changeListener) {\r\n        // Fire with initial user.\r\n        asyncQueue.enqueueRetryable(() => changeListener(User.UNAUTHENTICATED));\r\n    }\r\n    shutdown() { }\r\n}\r\n/**\r\n * A CredentialsProvider that always returns a constant token. Used for\r\n * emulator token mocking.\r\n */\r\nclass EmulatorAuthCredentialsProvider {\r\n    constructor(token) {\r\n        this.token = token;\r\n        /**\r\n         * Stores the listener registered with setChangeListener()\r\n         * This isn't actually necessary since the UID never changes, but we use this\r\n         * to verify the listen contract is adhered to in tests.\r\n         */\r\n        this.changeListener = null;\r\n    }\r\n    getToken() {\r\n        return Promise.resolve(this.token);\r\n    }\r\n    invalidateToken() { }\r\n    start(asyncQueue, changeListener) {\r\n        this.changeListener = changeListener;\r\n        // Fire with initial user.\r\n        asyncQueue.enqueueRetryable(() => changeListener(this.token.user));\r\n    }\r\n    shutdown() {\r\n        this.changeListener = null;\r\n    }\r\n}\r\nclass FirebaseAuthCredentialsProvider {\r\n    constructor(authProvider) {\r\n        this.authProvider = authProvider;\r\n        /** Tracks the current User. */\r\n        this.currentUser = User.UNAUTHENTICATED;\r\n        /**\r\n         * Counter used to detect if the token changed while a getToken request was\r\n         * outstanding.\r\n         */\r\n        this.tokenCounter = 0;\r\n        this.forceRefresh = false;\r\n        this.auth = null;\r\n    }\r\n    start(asyncQueue, changeListener) {\r\n        let lastTokenId = this.tokenCounter;\r\n        // A change listener that prevents double-firing for the same token change.\r\n        const guardedChangeListener = user => {\r\n            if (this.tokenCounter !== lastTokenId) {\r\n                lastTokenId = this.tokenCounter;\r\n                return changeListener(user);\r\n            }\r\n            else {\r\n                return Promise.resolve();\r\n            }\r\n        };\r\n        // A promise that can be waited on to block on the next token change.\r\n        // This promise is re-created after each change.\r\n        let nextToken = new Deferred();\r\n        this.tokenListener = () => {\r\n            this.tokenCounter++;\r\n            this.currentUser = this.getUser();\r\n            nextToken.resolve();\r\n            nextToken = new Deferred();\r\n            asyncQueue.enqueueRetryable(() => guardedChangeListener(this.currentUser));\r\n        };\r\n        const awaitNextToken = () => {\r\n            const currentTokenAttempt = nextToken;\r\n            asyncQueue.enqueueRetryable(async () => {\r\n                await currentTokenAttempt.promise;\r\n                await guardedChangeListener(this.currentUser);\r\n            });\r\n        };\r\n        const registerAuth = (auth) => {\r\n            logDebug('FirebaseAuthCredentialsProvider', 'Auth detected');\r\n            this.auth = auth;\r\n            this.auth.addAuthTokenListener(this.tokenListener);\r\n            awaitNextToken();\r\n        };\r\n        this.authProvider.onInit(auth => registerAuth(auth));\r\n        // Our users can initialize Auth right after Firestore, so we give it\r\n        // a chance to register itself with the component framework before we\r\n        // determine whether to start up in unauthenticated mode.\r\n        setTimeout(() => {\r\n            if (!this.auth) {\r\n                const auth = this.authProvider.getImmediate({ optional: true });\r\n                if (auth) {\r\n                    registerAuth(auth);\r\n                }\r\n                else {\r\n                    // If auth is still not available, proceed with `null` user\r\n                    logDebug('FirebaseAuthCredentialsProvider', 'Auth not yet detected');\r\n                    nextToken.resolve();\r\n                    nextToken = new Deferred();\r\n                }\r\n            }\r\n        }, 0);\r\n        awaitNextToken();\r\n    }\r\n    getToken() {\r\n        // Take note of the current value of the tokenCounter so that this method\r\n        // can fail (with an ABORTED error) if there is a token change while the\r\n        // request is outstanding.\r\n        const initialTokenCounter = this.tokenCounter;\r\n        const forceRefresh = this.forceRefresh;\r\n        this.forceRefresh = false;\r\n        if (!this.auth) {\r\n            return Promise.resolve(null);\r\n        }\r\n        return this.auth.getToken(forceRefresh).then(tokenData => {\r\n            // Cancel the request since the token changed while the request was\r\n            // outstanding so the response is potentially for a previous user (which\r\n            // user, we can't be sure).\r\n            if (this.tokenCounter !== initialTokenCounter) {\r\n                logDebug('FirebaseAuthCredentialsProvider', 'getToken aborted due to token change.');\r\n                return this.getToken();\r\n            }\r\n            else {\r\n                if (tokenData) {\r\n                    hardAssert(typeof tokenData.accessToken === 'string');\r\n                    return new OAuthToken(tokenData.accessToken, this.currentUser);\r\n                }\r\n                else {\r\n                    return null;\r\n                }\r\n            }\r\n        });\r\n    }\r\n    invalidateToken() {\r\n        this.forceRefresh = true;\r\n    }\r\n    shutdown() {\r\n        if (this.auth) {\r\n            this.auth.removeAuthTokenListener(this.tokenListener);\r\n        }\r\n    }\r\n    // Auth.getUid() can return null even with a user logged in. It is because\r\n    // getUid() is synchronous, but the auth code populating Uid is asynchronous.\r\n    // This method should only be called in the AuthTokenListener callback\r\n    // to guarantee to get the actual user.\r\n    getUser() {\r\n        const currentUid = this.auth && this.auth.getUid();\r\n        hardAssert(currentUid === null || typeof currentUid === 'string');\r\n        return new User(currentUid);\r\n    }\r\n}\r\n/*\r\n * FirstPartyToken provides a fresh token each time its value\r\n * is requested, because if the token is too old, requests will be rejected.\r\n * Technically this may no longer be necessary since the SDK should gracefully\r\n * recover from unauthenticated errors (see b/33147818 for context), but it's\r\n * safer to keep the implementation as-is.\r\n */\r\nclass FirstPartyToken {\r\n    constructor(gapi, sessionIndex, iamToken, authTokenFactory) {\r\n        this.gapi = gapi;\r\n        this.sessionIndex = sessionIndex;\r\n        this.iamToken = iamToken;\r\n        this.authTokenFactory = authTokenFactory;\r\n        this.type = 'FirstParty';\r\n        this.user = User.FIRST_PARTY;\r\n        this._headers = new Map();\r\n    }\r\n    /** Gets an authorization token, using a provided factory function, or falling back to First Party GAPI. */\r\n    getAuthToken() {\r\n        if (this.authTokenFactory) {\r\n            return this.authTokenFactory();\r\n        }\r\n        else {\r\n            // Make sure this really is a Gapi client.\r\n            hardAssert(!!(typeof this.gapi === 'object' &&\r\n                this.gapi !== null &&\r\n                this.gapi['auth'] &&\r\n                this.gapi['auth']['getAuthHeaderValueForFirstParty']));\r\n            return this.gapi['auth']['getAuthHeaderValueForFirstParty']([]);\r\n        }\r\n    }\r\n    get headers() {\r\n        this._headers.set('X-Goog-AuthUser', this.sessionIndex);\r\n        // Use array notation to prevent minification\r\n        const authHeaderTokenValue = this.getAuthToken();\r\n        if (authHeaderTokenValue) {\r\n            this._headers.set('Authorization', authHeaderTokenValue);\r\n        }\r\n        if (this.iamToken) {\r\n            this._headers.set('X-Goog-Iam-Authorization-Token', this.iamToken);\r\n        }\r\n        return this._headers;\r\n    }\r\n}\r\n/*\r\n * Provides user credentials required for the Firestore JavaScript SDK\r\n * to authenticate the user, using technique that is only available\r\n * to applications hosted by Google.\r\n */\r\nclass FirstPartyAuthCredentialsProvider {\r\n    constructor(gapi, sessionIndex, iamToken, authTokenFactory) {\r\n        this.gapi = gapi;\r\n        this.sessionIndex = sessionIndex;\r\n        this.iamToken = iamToken;\r\n        this.authTokenFactory = authTokenFactory;\r\n    }\r\n    getToken() {\r\n        return Promise.resolve(new FirstPartyToken(this.gapi, this.sessionIndex, this.iamToken, this.authTokenFactory));\r\n    }\r\n    start(asyncQueue, changeListener) {\r\n        // Fire with initial uid.\r\n        asyncQueue.enqueueRetryable(() => changeListener(User.FIRST_PARTY));\r\n    }\r\n    shutdown() { }\r\n    invalidateToken() { }\r\n}\r\nclass AppCheckToken {\r\n    constructor(value) {\r\n        this.value = value;\r\n        this.type = 'AppCheck';\r\n        this.headers = new Map();\r\n        if (value && value.length > 0) {\r\n            this.headers.set('x-firebase-appcheck', this.value);\r\n        }\r\n    }\r\n}\r\nclass FirebaseAppCheckTokenProvider {\r\n    constructor(appCheckProvider) {\r\n        this.appCheckProvider = appCheckProvider;\r\n        this.forceRefresh = false;\r\n        this.appCheck = null;\r\n        this.latestAppCheckToken = null;\r\n    }\r\n    start(asyncQueue, changeListener) {\r\n        const onTokenChanged = tokenResult => {\r\n            if (tokenResult.error != null) {\r\n                logDebug('FirebaseAppCheckTokenProvider', `Error getting App Check token; using placeholder token instead. Error: ${tokenResult.error.message}`);\r\n            }\r\n            const tokenUpdated = tokenResult.token !== this.latestAppCheckToken;\r\n            this.latestAppCheckToken = tokenResult.token;\r\n            logDebug('FirebaseAppCheckTokenProvider', `Received ${tokenUpdated ? 'new' : 'existing'} token.`);\r\n            return tokenUpdated\r\n                ? changeListener(tokenResult.token)\r\n                : Promise.resolve();\r\n        };\r\n        this.tokenListener = (tokenResult) => {\r\n            asyncQueue.enqueueRetryable(() => onTokenChanged(tokenResult));\r\n        };\r\n        const registerAppCheck = (appCheck) => {\r\n            logDebug('FirebaseAppCheckTokenProvider', 'AppCheck detected');\r\n            this.appCheck = appCheck;\r\n            this.appCheck.addTokenListener(this.tokenListener);\r\n        };\r\n        this.appCheckProvider.onInit(appCheck => registerAppCheck(appCheck));\r\n        // Our users can initialize AppCheck after Firestore, so we give it\r\n        // a chance to register itself with the component framework.\r\n        setTimeout(() => {\r\n            if (!this.appCheck) {\r\n                const appCheck = this.appCheckProvider.getImmediate({ optional: true });\r\n                if (appCheck) {\r\n                    registerAppCheck(appCheck);\r\n                }\r\n                else {\r\n                    // If AppCheck is still not available, proceed without it.\r\n                    logDebug('FirebaseAppCheckTokenProvider', 'AppCheck not yet detected');\r\n                }\r\n            }\r\n        }, 0);\r\n    }\r\n    getToken() {\r\n        const forceRefresh = this.forceRefresh;\r\n        this.forceRefresh = false;\r\n        if (!this.appCheck) {\r\n            return Promise.resolve(null);\r\n        }\r\n        return this.appCheck.getToken(forceRefresh).then(tokenResult => {\r\n            if (tokenResult) {\r\n                hardAssert(typeof tokenResult.token === 'string');\r\n                this.latestAppCheckToken = tokenResult.token;\r\n                return new AppCheckToken(tokenResult.token);\r\n            }\r\n            else {\r\n                return null;\r\n            }\r\n        });\r\n    }\r\n    invalidateToken() {\r\n        this.forceRefresh = true;\r\n    }\r\n    shutdown() {\r\n        if (this.appCheck) {\r\n            this.appCheck.removeTokenListener(this.tokenListener);\r\n        }\r\n    }\r\n}\r\n/**\r\n * An AppCheck token provider that always yields an empty token.\r\n * @internal\r\n */\r\nclass EmptyAppCheckTokenProvider {\r\n    getToken() {\r\n        return Promise.resolve(new AppCheckToken(''));\r\n    }\r\n    invalidateToken() { }\r\n    start(asyncQueue, changeListener) { }\r\n    shutdown() { }\r\n}\r\n/**\r\n * Builds a CredentialsProvider depending on the type of\r\n * the credentials passed in.\r\n */\r\nfunction makeAuthCredentialsProvider(credentials) {\r\n    if (!credentials) {\r\n        return new EmptyAuthCredentialsProvider();\r\n    }\r\n    switch (credentials['type']) {\r\n        case 'gapi':\r\n            const client = credentials['client'];\r\n            return new FirstPartyAuthCredentialsProvider(client, credentials['sessionIndex'] || '0', credentials['iamToken'] || null, credentials['authTokenFactory'] || null);\r\n        case 'provider':\r\n            return credentials['client'];\r\n        default:\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'makeAuthCredentialsProvider failed due to invalid credential type');\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Generates `nBytes` of random bytes.\r\n *\r\n * If `nBytes < 0` , an error will be thrown.\r\n */\r\nfunction randomBytes(nBytes) {\r\n    return randomBytes$1(nBytes);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass AutoId {\r\n    static newId() {\r\n        // Alphanumeric characters\r\n        const chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';\r\n        // The largest byte value that is a multiple of `char.length`.\r\n        const maxMultiple = Math.floor(256 / chars.length) * chars.length;\r\n        let autoId = '';\r\n        const targetLength = 20;\r\n        while (autoId.length < targetLength) {\r\n            const bytes = randomBytes(40);\r\n            for (let i = 0; i < bytes.length; ++i) {\r\n                // Only accept values that are [0, maxMultiple), this ensures they can\r\n                // be evenly mapped to indices of `chars` via a modulo operation.\r\n                if (autoId.length < targetLength && bytes[i] < maxMultiple) {\r\n                    autoId += chars.charAt(bytes[i] % chars.length);\r\n                }\r\n            }\r\n        }\r\n        return autoId;\r\n    }\r\n}\r\nfunction primitiveComparator(left, right) {\r\n    if (left < right) {\r\n        return -1;\r\n    }\r\n    if (left > right) {\r\n        return 1;\r\n    }\r\n    return 0;\r\n}\r\n/** Helper to compare arrays using isEqual(). */\r\nfunction arrayEquals(left, right, comparator) {\r\n    if (left.length !== right.length) {\r\n        return false;\r\n    }\r\n    return left.every((value, index) => comparator(value, right[index]));\r\n}\r\n/**\r\n * Returns the immediate lexicographically-following string. This is useful to\r\n * construct an inclusive range for indexeddb iterators.\r\n */\r\nfunction immediateSuccessor(s) {\r\n    // Return the input string, with an additional NUL byte appended.\r\n    return s + '\\0';\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// The earliest date supported by Firestore timestamps (0001-01-01T00:00:00Z).\r\nconst MIN_SECONDS = -62135596800;\r\n// Number of nanoseconds in a millisecond.\r\nconst MS_TO_NANOS = 1e6;\r\n/**\r\n * A `Timestamp` represents a point in time independent of any time zone or\r\n * calendar, represented as seconds and fractions of seconds at nanosecond\r\n * resolution in UTC Epoch time.\r\n *\r\n * It is encoded using the Proleptic Gregorian Calendar which extends the\r\n * Gregorian calendar backwards to year one. It is encoded assuming all minutes\r\n * are 60 seconds long, i.e. leap seconds are \"smeared\" so that no leap second\r\n * table is needed for interpretation. Range is from 0001-01-01T00:00:00Z to\r\n * 9999-12-31T23:59:59.999999999Z.\r\n *\r\n * For examples and further specifications, refer to the\r\n * {@link https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto | Timestamp definition}.\r\n */\r\nclass Timestamp {\r\n    /**\r\n     * Creates a new timestamp.\r\n     *\r\n     * @param seconds - The number of seconds of UTC time since Unix epoch\r\n     *     1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to\r\n     *     9999-12-31T23:59:59Z inclusive.\r\n     * @param nanoseconds - The non-negative fractions of a second at nanosecond\r\n     *     resolution. Negative second values with fractions must still have\r\n     *     non-negative nanoseconds values that count forward in time. Must be\r\n     *     from 0 to 999,999,999 inclusive.\r\n     */\r\n    constructor(\r\n    /**\r\n     * The number of seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z.\r\n     */\r\n    seconds, \r\n    /**\r\n     * The fractions of a second at nanosecond resolution.*\r\n     */\r\n    nanoseconds) {\r\n        this.seconds = seconds;\r\n        this.nanoseconds = nanoseconds;\r\n        if (nanoseconds < 0) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp nanoseconds out of range: ' + nanoseconds);\r\n        }\r\n        if (nanoseconds >= 1e9) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp nanoseconds out of range: ' + nanoseconds);\r\n        }\r\n        if (seconds < MIN_SECONDS) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp seconds out of range: ' + seconds);\r\n        }\r\n        // This will break in the year 10,000.\r\n        if (seconds >= 253402300800) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp seconds out of range: ' + seconds);\r\n        }\r\n    }\r\n    /**\r\n     * Creates a new timestamp with the current date, with millisecond precision.\r\n     *\r\n     * @returns a new timestamp representing the current date.\r\n     */\r\n    static now() {\r\n        return Timestamp.fromMillis(Date.now());\r\n    }\r\n    /**\r\n     * Creates a new timestamp from the given date.\r\n     *\r\n     * @param date - The date to initialize the `Timestamp` from.\r\n     * @returns A new `Timestamp` representing the same point in time as the given\r\n     *     date.\r\n     */\r\n    static fromDate(date) {\r\n        return Timestamp.fromMillis(date.getTime());\r\n    }\r\n    /**\r\n     * Creates a new timestamp from the given number of milliseconds.\r\n     *\r\n     * @param milliseconds - Number of milliseconds since Unix epoch\r\n     *     1970-01-01T00:00:00Z.\r\n     * @returns A new `Timestamp` representing the same point in time as the given\r\n     *     number of milliseconds.\r\n     */\r\n    static fromMillis(milliseconds) {\r\n        const seconds = Math.floor(milliseconds / 1000);\r\n        const nanos = Math.floor((milliseconds - seconds * 1000) * MS_TO_NANOS);\r\n        return new Timestamp(seconds, nanos);\r\n    }\r\n    /**\r\n     * Converts a `Timestamp` to a JavaScript `Date` object. This conversion\r\n     * causes a loss of precision since `Date` objects only support millisecond\r\n     * precision.\r\n     *\r\n     * @returns JavaScript `Date` object representing the same point in time as\r\n     *     this `Timestamp`, with millisecond precision.\r\n     */\r\n    toDate() {\r\n        return new Date(this.toMillis());\r\n    }\r\n    /**\r\n     * Converts a `Timestamp` to a numeric timestamp (in milliseconds since\r\n     * epoch). This operation causes a loss of precision.\r\n     *\r\n     * @returns The point in time corresponding to this timestamp, represented as\r\n     *     the number of milliseconds since Unix epoch 1970-01-01T00:00:00Z.\r\n     */\r\n    toMillis() {\r\n        return this.seconds * 1000 + this.nanoseconds / MS_TO_NANOS;\r\n    }\r\n    _compareTo(other) {\r\n        if (this.seconds === other.seconds) {\r\n            return primitiveComparator(this.nanoseconds, other.nanoseconds);\r\n        }\r\n        return primitiveComparator(this.seconds, other.seconds);\r\n    }\r\n    /**\r\n     * Returns true if this `Timestamp` is equal to the provided one.\r\n     *\r\n     * @param other - The `Timestamp` to compare against.\r\n     * @returns true if this `Timestamp` is equal to the provided one.\r\n     */\r\n    isEqual(other) {\r\n        return (other.seconds === this.seconds && other.nanoseconds === this.nanoseconds);\r\n    }\r\n    /** Returns a textual representation of this `Timestamp`. */\r\n    toString() {\r\n        return ('Timestamp(seconds=' +\r\n            this.seconds +\r\n            ', nanoseconds=' +\r\n            this.nanoseconds +\r\n            ')');\r\n    }\r\n    /** Returns a JSON-serializable representation of this `Timestamp`. */\r\n    toJSON() {\r\n        return { seconds: this.seconds, nanoseconds: this.nanoseconds };\r\n    }\r\n    /**\r\n     * Converts this object to a primitive string, which allows `Timestamp` objects\r\n     * to be compared using the `>`, `<=`, `>=` and `>` operators.\r\n     */\r\n    valueOf() {\r\n        // This method returns a string of the form <seconds>.<nanoseconds> where\r\n        // <seconds> is translated to have a non-negative value and both <seconds>\r\n        // and <nanoseconds> are left-padded with zeroes to be a consistent length.\r\n        // Strings with this format then have a lexiographical ordering that matches\r\n        // the expected ordering. The <seconds> translation is done to avoid having\r\n        // a leading negative sign (i.e. a leading '-' character) in its string\r\n        // representation, which would affect its lexiographical ordering.\r\n        const adjustedSeconds = this.seconds - MIN_SECONDS;\r\n        // Note: Up to 12 decimal digits are required to represent all valid\r\n        // 'seconds' values.\r\n        const formattedSeconds = String(adjustedSeconds).padStart(12, '0');\r\n        const formattedNanoseconds = String(this.nanoseconds).padStart(9, '0');\r\n        return formattedSeconds + '.' + formattedNanoseconds;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A version of a document in Firestore. This corresponds to the version\r\n * timestamp, such as update_time or read_time.\r\n */\r\nclass SnapshotVersion {\r\n    constructor(timestamp) {\r\n        this.timestamp = timestamp;\r\n    }\r\n    static fromTimestamp(value) {\r\n        return new SnapshotVersion(value);\r\n    }\r\n    static min() {\r\n        return new SnapshotVersion(new Timestamp(0, 0));\r\n    }\r\n    static max() {\r\n        return new SnapshotVersion(new Timestamp(253402300799, 1e9 - 1));\r\n    }\r\n    compareTo(other) {\r\n        return this.timestamp._compareTo(other.timestamp);\r\n    }\r\n    isEqual(other) {\r\n        return this.timestamp.isEqual(other.timestamp);\r\n    }\r\n    /** Returns a number representation of the version for use in spec tests. */\r\n    toMicroseconds() {\r\n        // Convert to microseconds.\r\n        return this.timestamp.seconds * 1e6 + this.timestamp.nanoseconds / 1000;\r\n    }\r\n    toString() {\r\n        return 'SnapshotVersion(' + this.timestamp.toString() + ')';\r\n    }\r\n    toTimestamp() {\r\n        return this.timestamp;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst DOCUMENT_KEY_NAME = '__name__';\r\n/**\r\n * Path represents an ordered sequence of string segments.\r\n */\r\nclass BasePath {\r\n    constructor(segments, offset, length) {\r\n        if (offset === undefined) {\r\n            offset = 0;\r\n        }\r\n        else if (offset > segments.length) {\r\n            fail();\r\n        }\r\n        if (length === undefined) {\r\n            length = segments.length - offset;\r\n        }\r\n        else if (length > segments.length - offset) {\r\n            fail();\r\n        }\r\n        this.segments = segments;\r\n        this.offset = offset;\r\n        this.len = length;\r\n    }\r\n    get length() {\r\n        return this.len;\r\n    }\r\n    isEqual(other) {\r\n        return BasePath.comparator(this, other) === 0;\r\n    }\r\n    child(nameOrPath) {\r\n        const segments = this.segments.slice(this.offset, this.limit());\r\n        if (nameOrPath instanceof BasePath) {\r\n            nameOrPath.forEach(segment => {\r\n                segments.push(segment);\r\n            });\r\n        }\r\n        else {\r\n            segments.push(nameOrPath);\r\n        }\r\n        return this.construct(segments);\r\n    }\r\n    /** The index of one past the last segment of the path. */\r\n    limit() {\r\n        return this.offset + this.length;\r\n    }\r\n    popFirst(size) {\r\n        size = size === undefined ? 1 : size;\r\n        return this.construct(this.segments, this.offset + size, this.length - size);\r\n    }\r\n    popLast() {\r\n        return this.construct(this.segments, this.offset, this.length - 1);\r\n    }\r\n    firstSegment() {\r\n        return this.segments[this.offset];\r\n    }\r\n    lastSegment() {\r\n        return this.get(this.length - 1);\r\n    }\r\n    get(index) {\r\n        return this.segments[this.offset + index];\r\n    }\r\n    isEmpty() {\r\n        return this.length === 0;\r\n    }\r\n    isPrefixOf(other) {\r\n        if (other.length < this.length) {\r\n            return false;\r\n        }\r\n        for (let i = 0; i < this.length; i++) {\r\n            if (this.get(i) !== other.get(i)) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    isImmediateParentOf(potentialChild) {\r\n        if (this.length + 1 !== potentialChild.length) {\r\n            return false;\r\n        }\r\n        for (let i = 0; i < this.length; i++) {\r\n            if (this.get(i) !== potentialChild.get(i)) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    forEach(fn) {\r\n        for (let i = this.offset, end = this.limit(); i < end; i++) {\r\n            fn(this.segments[i]);\r\n        }\r\n    }\r\n    toArray() {\r\n        return this.segments.slice(this.offset, this.limit());\r\n    }\r\n    static comparator(p1, p2) {\r\n        const len = Math.min(p1.length, p2.length);\r\n        for (let i = 0; i < len; i++) {\r\n            const left = p1.get(i);\r\n            const right = p2.get(i);\r\n            if (left < right) {\r\n                return -1;\r\n            }\r\n            if (left > right) {\r\n                return 1;\r\n            }\r\n        }\r\n        if (p1.length < p2.length) {\r\n            return -1;\r\n        }\r\n        if (p1.length > p2.length) {\r\n            return 1;\r\n        }\r\n        return 0;\r\n    }\r\n}\r\n/**\r\n * A slash-separated path for navigating resources (documents and collections)\r\n * within Firestore.\r\n *\r\n * @internal\r\n */\r\nclass ResourcePath extends BasePath {\r\n    construct(segments, offset, length) {\r\n        return new ResourcePath(segments, offset, length);\r\n    }\r\n    canonicalString() {\r\n        // NOTE: The client is ignorant of any path segments containing escape\r\n        // sequences (e.g. __id123__) and just passes them through raw (they exist\r\n        // for legacy reasons and should not be used frequently).\r\n        return this.toArray().join('/');\r\n    }\r\n    toString() {\r\n        return this.canonicalString();\r\n    }\r\n    /**\r\n     * Creates a resource path from the given slash-delimited string. If multiple\r\n     * arguments are provided, all components are combined. Leading and trailing\r\n     * slashes from all components are ignored.\r\n     */\r\n    static fromString(...pathComponents) {\r\n        // NOTE: The client is ignorant of any path segments containing escape\r\n        // sequences (e.g. __id123__) and just passes them through raw (they exist\r\n        // for legacy reasons and should not be used frequently).\r\n        const segments = [];\r\n        for (const path of pathComponents) {\r\n            if (path.indexOf('//') >= 0) {\r\n                throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid segment (${path}). Paths must not contain // in them.`);\r\n            }\r\n            // Strip leading and traling slashed.\r\n            segments.push(...path.split('/').filter(segment => segment.length > 0));\r\n        }\r\n        return new ResourcePath(segments);\r\n    }\r\n    static emptyPath() {\r\n        return new ResourcePath([]);\r\n    }\r\n}\r\nconst identifierRegExp = /^[_a-zA-Z][_a-zA-Z0-9]*$/;\r\n/**\r\n * A dot-separated path for navigating sub-objects within a document.\r\n * @internal\r\n */\r\nclass FieldPath$1 extends BasePath {\r\n    construct(segments, offset, length) {\r\n        return new FieldPath$1(segments, offset, length);\r\n    }\r\n    /**\r\n     * Returns true if the string could be used as a segment in a field path\r\n     * without escaping.\r\n     */\r\n    static isValidIdentifier(segment) {\r\n        return identifierRegExp.test(segment);\r\n    }\r\n    canonicalString() {\r\n        return this.toArray()\r\n            .map(str => {\r\n            str = str.replace(/\\\\/g, '\\\\\\\\').replace(/`/g, '\\\\`');\r\n            if (!FieldPath$1.isValidIdentifier(str)) {\r\n                str = '`' + str + '`';\r\n            }\r\n            return str;\r\n        })\r\n            .join('.');\r\n    }\r\n    toString() {\r\n        return this.canonicalString();\r\n    }\r\n    /**\r\n     * Returns true if this field references the key of a document.\r\n     */\r\n    isKeyField() {\r\n        return this.length === 1 && this.get(0) === DOCUMENT_KEY_NAME;\r\n    }\r\n    /**\r\n     * The field designating the key of a document.\r\n     */\r\n    static keyField() {\r\n        return new FieldPath$1([DOCUMENT_KEY_NAME]);\r\n    }\r\n    /**\r\n     * Parses a field string from the given server-formatted string.\r\n     *\r\n     * - Splitting the empty string is not allowed (for now at least).\r\n     * - Empty segments within the string (e.g. if there are two consecutive\r\n     *   separators) are not allowed.\r\n     *\r\n     * TODO(b/37244157): we should make this more strict. Right now, it allows\r\n     * non-identifier path components, even if they aren't escaped.\r\n     */\r\n    static fromServerFormat(path) {\r\n        const segments = [];\r\n        let current = '';\r\n        let i = 0;\r\n        const addCurrentSegment = () => {\r\n            if (current.length === 0) {\r\n                throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid field path (${path}). Paths must not be empty, begin ` +\r\n                    `with '.', end with '.', or contain '..'`);\r\n            }\r\n            segments.push(current);\r\n            current = '';\r\n        };\r\n        let inBackticks = false;\r\n        while (i < path.length) {\r\n            const c = path[i];\r\n            if (c === '\\\\') {\r\n                if (i + 1 === path.length) {\r\n                    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Path has trailing escape character: ' + path);\r\n                }\r\n                const next = path[i + 1];\r\n                if (!(next === '\\\\' || next === '.' || next === '`')) {\r\n                    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Path has invalid escape sequence: ' + path);\r\n                }\r\n                current += next;\r\n                i += 2;\r\n            }\r\n            else if (c === '`') {\r\n                inBackticks = !inBackticks;\r\n                i++;\r\n            }\r\n            else if (c === '.' && !inBackticks) {\r\n                addCurrentSegment();\r\n                i++;\r\n            }\r\n            else {\r\n                current += c;\r\n                i++;\r\n            }\r\n        }\r\n        addCurrentSegment();\r\n        if (inBackticks) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Unterminated ` in path: ' + path);\r\n        }\r\n        return new FieldPath$1(segments);\r\n    }\r\n    static emptyPath() {\r\n        return new FieldPath$1([]);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * @internal\r\n */\r\nclass DocumentKey {\r\n    constructor(path) {\r\n        this.path = path;\r\n    }\r\n    static fromPath(path) {\r\n        return new DocumentKey(ResourcePath.fromString(path));\r\n    }\r\n    static fromName(name) {\r\n        return new DocumentKey(ResourcePath.fromString(name).popFirst(5));\r\n    }\r\n    static empty() {\r\n        return new DocumentKey(ResourcePath.emptyPath());\r\n    }\r\n    get collectionGroup() {\r\n        return this.path.popLast().lastSegment();\r\n    }\r\n    /** Returns true if the document is in the specified collectionId. */\r\n    hasCollectionId(collectionId) {\r\n        return (this.path.length >= 2 &&\r\n            this.path.get(this.path.length - 2) === collectionId);\r\n    }\r\n    /** Returns the collection group (i.e. the name of the parent collection) for this key. */\r\n    getCollectionGroup() {\r\n        return this.path.get(this.path.length - 2);\r\n    }\r\n    /** Returns the fully qualified path to the parent collection. */\r\n    getCollectionPath() {\r\n        return this.path.popLast();\r\n    }\r\n    isEqual(other) {\r\n        return (other !== null && ResourcePath.comparator(this.path, other.path) === 0);\r\n    }\r\n    toString() {\r\n        return this.path.toString();\r\n    }\r\n    static comparator(k1, k2) {\r\n        return ResourcePath.comparator(k1.path, k2.path);\r\n    }\r\n    static isDocumentKey(path) {\r\n        return path.length % 2 === 0;\r\n    }\r\n    /**\r\n     * Creates and returns a new document key with the given segments.\r\n     *\r\n     * @param segments - The segments of the path to the document\r\n     * @returns A new instance of DocumentKey\r\n     */\r\n    static fromSegments(segments) {\r\n        return new DocumentKey(new ResourcePath(segments.slice()));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * The initial mutation batch id for each index. Gets updated during index\r\n * backfill.\r\n */\r\nconst INITIAL_LARGEST_BATCH_ID = -1;\r\n/**\r\n * The initial sequence number for each index. Gets updated during index\r\n * backfill.\r\n */\r\nconst INITIAL_SEQUENCE_NUMBER = 0;\r\n/**\r\n * An index definition for field indexes in Firestore.\r\n *\r\n * Every index is associated with a collection. The definition contains a list\r\n * of fields and their index kind (which can be `ASCENDING`, `DESCENDING` or\r\n * `CONTAINS` for ArrayContains/ArrayContainsAny queries).\r\n *\r\n * Unlike the backend, the SDK does not differentiate between collection or\r\n * collection group-scoped indices. Every index can be used for both single\r\n * collection and collection group queries.\r\n */\r\nclass FieldIndex {\r\n    constructor(\r\n    /**\r\n     * The index ID. Returns -1 if the index ID is not available (e.g. the index\r\n     * has not yet been persisted).\r\n     */\r\n    indexId, \r\n    /** The collection ID this index applies to. */\r\n    collectionGroup, \r\n    /** The field segments for this index. */\r\n    fields, \r\n    /** Shows how up-to-date the index is for the current user. */\r\n    indexState) {\r\n        this.indexId = indexId;\r\n        this.collectionGroup = collectionGroup;\r\n        this.fields = fields;\r\n        this.indexState = indexState;\r\n    }\r\n}\r\n/** An ID for an index that has not yet been added to persistence.  */\r\nFieldIndex.UNKNOWN_ID = -1;\r\n/** Returns the ArrayContains/ArrayContainsAny segment for this index. */\r\nfunction fieldIndexGetArraySegment(fieldIndex) {\r\n    return fieldIndex.fields.find(s => s.kind === 2 /* CONTAINS */);\r\n}\r\n/** Returns all directional (ascending/descending) segments for this index. */\r\nfunction fieldIndexGetDirectionalSegments(fieldIndex) {\r\n    return fieldIndex.fields.filter(s => s.kind !== 2 /* CONTAINS */);\r\n}\r\n/**\r\n * Returns the order of the document key component for the given index.\r\n *\r\n * PORTING NOTE: This is only used in the Web IndexedDb implementation.\r\n */\r\nfunction fieldIndexGetKeyOrder(fieldIndex) {\r\n    const directionalSegments = fieldIndexGetDirectionalSegments(fieldIndex);\r\n    return directionalSegments.length === 0\r\n        ? 0 /* ASCENDING */\r\n        : directionalSegments[directionalSegments.length - 1].kind;\r\n}\r\n/**\r\n * Compares indexes by collection group and segments. Ignores update time and\r\n * index ID.\r\n */\r\nfunction fieldIndexSemanticComparator(left, right) {\r\n    let cmp = primitiveComparator(left.collectionGroup, right.collectionGroup);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    for (let i = 0; i < Math.min(left.fields.length, right.fields.length); ++i) {\r\n        cmp = indexSegmentComparator(left.fields[i], right.fields[i]);\r\n        if (cmp !== 0) {\r\n            return cmp;\r\n        }\r\n    }\r\n    return primitiveComparator(left.fields.length, right.fields.length);\r\n}\r\n/** Returns a debug representation of the field index */\r\nfunction fieldIndexToString(fieldIndex) {\r\n    return `id=${fieldIndex.indexId}|cg=${fieldIndex.collectionGroup}|f=${fieldIndex.fields.map(f => `${f.fieldPath}:${f.kind}`).join(',')}`;\r\n}\r\n/** An index component consisting of field path and index type.  */\r\nclass IndexSegment {\r\n    constructor(\r\n    /** The field path of the component. */\r\n    fieldPath, \r\n    /** The fields sorting order. */\r\n    kind) {\r\n        this.fieldPath = fieldPath;\r\n        this.kind = kind;\r\n    }\r\n}\r\nfunction indexSegmentComparator(left, right) {\r\n    const cmp = FieldPath$1.comparator(left.fieldPath, right.fieldPath);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    return primitiveComparator(left.kind, right.kind);\r\n}\r\n/**\r\n * Stores the \"high water mark\" that indicates how updated the Index is for the\r\n * current user.\r\n */\r\nclass IndexState {\r\n    constructor(\r\n    /**\r\n     * Indicates when the index was last updated (relative to other indexes).\r\n     */\r\n    sequenceNumber, \r\n    /** The the latest indexed read time, document and batch id. */\r\n    offset) {\r\n        this.sequenceNumber = sequenceNumber;\r\n        this.offset = offset;\r\n    }\r\n    /** The state of an index that has not yet been backfilled. */\r\n    static empty() {\r\n        return new IndexState(INITIAL_SEQUENCE_NUMBER, IndexOffset.min());\r\n    }\r\n}\r\n/**\r\n * Creates an offset that matches all documents with a read time higher than\r\n * `readTime`.\r\n */\r\nfunction newIndexOffsetSuccessorFromReadTime(readTime, largestBatchId) {\r\n    // We want to create an offset that matches all documents with a read time\r\n    // greater than the provided read time. To do so, we technically need to\r\n    // create an offset for `(readTime, MAX_DOCUMENT_KEY)`. While we could use\r\n    // Unicode codepoints to generate MAX_DOCUMENT_KEY, it is much easier to use\r\n    // `(readTime + 1, DocumentKey.empty())` since `> DocumentKey.empty()` matches\r\n    // all valid document IDs.\r\n    const successorSeconds = readTime.toTimestamp().seconds;\r\n    const successorNanos = readTime.toTimestamp().nanoseconds + 1;\r\n    const successor = SnapshotVersion.fromTimestamp(successorNanos === 1e9\r\n        ? new Timestamp(successorSeconds + 1, 0)\r\n        : new Timestamp(successorSeconds, successorNanos));\r\n    return new IndexOffset(successor, DocumentKey.empty(), largestBatchId);\r\n}\r\n/** Creates a new offset based on the provided document. */\r\nfunction newIndexOffsetFromDocument(document) {\r\n    return new IndexOffset(document.readTime, document.key, INITIAL_LARGEST_BATCH_ID);\r\n}\r\n/**\r\n * Stores the latest read time, document and batch ID that were processed for an\r\n * index.\r\n */\r\nclass IndexOffset {\r\n    constructor(\r\n    /**\r\n     * The latest read time version that has been indexed by Firestore for this\r\n     * field index.\r\n     */\r\n    readTime, \r\n    /**\r\n     * The key of the last document that was indexed for this query. Use\r\n     * `DocumentKey.empty()` if no document has been indexed.\r\n     */\r\n    documentKey, \r\n    /*\r\n     * The largest mutation batch id that's been processed by Firestore.\r\n     */\r\n    largestBatchId) {\r\n        this.readTime = readTime;\r\n        this.documentKey = documentKey;\r\n        this.largestBatchId = largestBatchId;\r\n    }\r\n    /** Returns an offset that sorts before all regular offsets. */\r\n    static min() {\r\n        return new IndexOffset(SnapshotVersion.min(), DocumentKey.empty(), INITIAL_LARGEST_BATCH_ID);\r\n    }\r\n    /** Returns an offset that sorts after all regular offsets. */\r\n    static max() {\r\n        return new IndexOffset(SnapshotVersion.max(), DocumentKey.empty(), INITIAL_LARGEST_BATCH_ID);\r\n    }\r\n}\r\nfunction indexOffsetComparator(left, right) {\r\n    let cmp = left.readTime.compareTo(right.readTime);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    cmp = DocumentKey.comparator(left.documentKey, right.documentKey);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    return primitiveComparator(left.largestBatchId, right.largestBatchId);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst PRIMARY_LEASE_LOST_ERROR_MSG = 'The current tab is not in the required state to perform this operation. ' +\r\n    'It might be necessary to refresh the browser tab.';\r\n/**\r\n * A base class representing a persistence transaction, encapsulating both the\r\n * transaction's sequence numbers as well as a list of onCommitted listeners.\r\n *\r\n * When you call Persistence.runTransaction(), it will create a transaction and\r\n * pass it to your callback. You then pass it to any method that operates\r\n * on persistence.\r\n */\r\nclass PersistenceTransaction {\r\n    constructor() {\r\n        this.onCommittedListeners = [];\r\n    }\r\n    addOnCommittedListener(listener) {\r\n        this.onCommittedListeners.push(listener);\r\n    }\r\n    raiseOnCommittedEvent() {\r\n        this.onCommittedListeners.forEach(listener => listener());\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Verifies the error thrown by a LocalStore operation. If a LocalStore\r\n * operation fails because the primary lease has been taken by another client,\r\n * we ignore the error (the persistence layer will immediately call\r\n * `applyPrimaryLease` to propagate the primary state change). All other errors\r\n * are re-thrown.\r\n *\r\n * @param err - An error returned by a LocalStore operation.\r\n * @returns A Promise that resolves after we recovered, or the original error.\r\n */\r\nasync function ignoreIfPrimaryLeaseLoss(err) {\r\n    if (err.code === Code.FAILED_PRECONDITION &&\r\n        err.message === PRIMARY_LEASE_LOST_ERROR_MSG) {\r\n        logDebug('LocalStore', 'Unexpectedly lost primary lease');\r\n    }\r\n    else {\r\n        throw err;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * PersistencePromise is essentially a re-implementation of Promise except\r\n * it has a .next() method instead of .then() and .next() and .catch() callbacks\r\n * are executed synchronously when a PersistencePromise resolves rather than\r\n * asynchronously (Promise implementations use setImmediate() or similar).\r\n *\r\n * This is necessary to interoperate with IndexedDB which will automatically\r\n * commit transactions if control is returned to the event loop without\r\n * synchronously initiating another operation on the transaction.\r\n *\r\n * NOTE: .then() and .catch() only allow a single consumer, unlike normal\r\n * Promises.\r\n */\r\nclass PersistencePromise {\r\n    constructor(callback) {\r\n        // NOTE: next/catchCallback will always point to our own wrapper functions,\r\n        // not the user's raw next() or catch() callbacks.\r\n        this.nextCallback = null;\r\n        this.catchCallback = null;\r\n        // When the operation resolves, we'll set result or error and mark isDone.\r\n        this.result = undefined;\r\n        this.error = undefined;\r\n        this.isDone = false;\r\n        // Set to true when .then() or .catch() are called and prevents additional\r\n        // chaining.\r\n        this.callbackAttached = false;\r\n        callback(value => {\r\n            this.isDone = true;\r\n            this.result = value;\r\n            if (this.nextCallback) {\r\n                // value should be defined unless T is Void, but we can't express\r\n                // that in the type system.\r\n                this.nextCallback(value);\r\n            }\r\n        }, error => {\r\n            this.isDone = true;\r\n            this.error = error;\r\n            if (this.catchCallback) {\r\n                this.catchCallback(error);\r\n            }\r\n        });\r\n    }\r\n    catch(fn) {\r\n        return this.next(undefined, fn);\r\n    }\r\n    next(nextFn, catchFn) {\r\n        if (this.callbackAttached) {\r\n            fail();\r\n        }\r\n        this.callbackAttached = true;\r\n        if (this.isDone) {\r\n            if (!this.error) {\r\n                return this.wrapSuccess(nextFn, this.result);\r\n            }\r\n            else {\r\n                return this.wrapFailure(catchFn, this.error);\r\n            }\r\n        }\r\n        else {\r\n            return new PersistencePromise((resolve, reject) => {\r\n                this.nextCallback = (value) => {\r\n                    this.wrapSuccess(nextFn, value).next(resolve, reject);\r\n                };\r\n                this.catchCallback = (error) => {\r\n                    this.wrapFailure(catchFn, error).next(resolve, reject);\r\n                };\r\n            });\r\n        }\r\n    }\r\n    toPromise() {\r\n        return new Promise((resolve, reject) => {\r\n            this.next(resolve, reject);\r\n        });\r\n    }\r\n    wrapUserFunction(fn) {\r\n        try {\r\n            const result = fn();\r\n            if (result instanceof PersistencePromise) {\r\n                return result;\r\n            }\r\n            else {\r\n                return PersistencePromise.resolve(result);\r\n            }\r\n        }\r\n        catch (e) {\r\n            return PersistencePromise.reject(e);\r\n        }\r\n    }\r\n    wrapSuccess(nextFn, value) {\r\n        if (nextFn) {\r\n            return this.wrapUserFunction(() => nextFn(value));\r\n        }\r\n        else {\r\n            // If there's no nextFn, then R must be the same as T\r\n            return PersistencePromise.resolve(value);\r\n        }\r\n    }\r\n    wrapFailure(catchFn, error) {\r\n        if (catchFn) {\r\n            return this.wrapUserFunction(() => catchFn(error));\r\n        }\r\n        else {\r\n            return PersistencePromise.reject(error);\r\n        }\r\n    }\r\n    static resolve(result) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            resolve(result);\r\n        });\r\n    }\r\n    static reject(error) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            reject(error);\r\n        });\r\n    }\r\n    static waitFor(\r\n    // Accept all Promise types in waitFor().\r\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n    all) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            let expectedCount = 0;\r\n            let resolvedCount = 0;\r\n            let done = false;\r\n            all.forEach(element => {\r\n                ++expectedCount;\r\n                element.next(() => {\r\n                    ++resolvedCount;\r\n                    if (done && resolvedCount === expectedCount) {\r\n                        resolve();\r\n                    }\r\n                }, err => reject(err));\r\n            });\r\n            done = true;\r\n            if (resolvedCount === expectedCount) {\r\n                resolve();\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * Given an array of predicate functions that asynchronously evaluate to a\r\n     * boolean, implements a short-circuiting `or` between the results. Predicates\r\n     * will be evaluated until one of them returns `true`, then stop. The final\r\n     * result will be whether any of them returned `true`.\r\n     */\r\n    static or(predicates) {\r\n        let p = PersistencePromise.resolve(false);\r\n        for (const predicate of predicates) {\r\n            p = p.next(isTrue => {\r\n                if (isTrue) {\r\n                    return PersistencePromise.resolve(isTrue);\r\n                }\r\n                else {\r\n                    return predicate();\r\n                }\r\n            });\r\n        }\r\n        return p;\r\n    }\r\n    static forEach(collection, f) {\r\n        const promises = [];\r\n        collection.forEach((r, s) => {\r\n            promises.push(f.call(this, r, s));\r\n        });\r\n        return this.waitFor(promises);\r\n    }\r\n    /**\r\n     * Concurrently map all array elements through asynchronous function.\r\n     */\r\n    static mapArray(array, f) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            const expectedCount = array.length;\r\n            const results = new Array(expectedCount);\r\n            let resolvedCount = 0;\r\n            for (let i = 0; i < expectedCount; i++) {\r\n                const current = i;\r\n                f(array[current]).next(result => {\r\n                    results[current] = result;\r\n                    ++resolvedCount;\r\n                    if (resolvedCount === expectedCount) {\r\n                        resolve(results);\r\n                    }\r\n                }, err => reject(err));\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * An alternative to recursive PersistencePromise calls, that avoids\r\n     * potential memory problems from unbounded chains of promises.\r\n     *\r\n     * The `action` will be called repeatedly while `condition` is true.\r\n     */\r\n    static doWhile(condition, action) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            const process = () => {\r\n                if (condition() === true) {\r\n                    action().next(() => {\r\n                        process();\r\n                    }, reject);\r\n                }\r\n                else {\r\n                    resolve();\r\n                }\r\n            };\r\n            process();\r\n        });\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// References to `window` are guarded by SimpleDb.isAvailable()\r\n/* eslint-disable no-restricted-globals */\r\nconst LOG_TAG$i = 'SimpleDb';\r\n/**\r\n * The maximum number of retry attempts for an IndexedDb transaction that fails\r\n * with a DOMException.\r\n */\r\nconst TRANSACTION_RETRY_COUNT = 3;\r\n/**\r\n * Wraps an IDBTransaction and exposes a store() method to get a handle to a\r\n * specific object store.\r\n */\r\nclass SimpleDbTransaction {\r\n    constructor(action, transaction) {\r\n        this.action = action;\r\n        this.transaction = transaction;\r\n        this.aborted = false;\r\n        /**\r\n         * A `Promise` that resolves with the result of the IndexedDb transaction.\r\n         */\r\n        this.completionDeferred = new Deferred();\r\n        this.transaction.oncomplete = () => {\r\n            this.completionDeferred.resolve();\r\n        };\r\n        this.transaction.onabort = () => {\r\n            if (transaction.error) {\r\n                this.completionDeferred.reject(new IndexedDbTransactionError(action, transaction.error));\r\n            }\r\n            else {\r\n                this.completionDeferred.resolve();\r\n            }\r\n        };\r\n        this.transaction.onerror = (event) => {\r\n            const error = checkForAndReportiOSError(event.target.error);\r\n            this.completionDeferred.reject(new IndexedDbTransactionError(action, error));\r\n        };\r\n    }\r\n    static open(db, action, mode, objectStoreNames) {\r\n        try {\r\n            return new SimpleDbTransaction(action, db.transaction(objectStoreNames, mode));\r\n        }\r\n        catch (e) {\r\n            throw new IndexedDbTransactionError(action, e);\r\n        }\r\n    }\r\n    get completionPromise() {\r\n        return this.completionDeferred.promise;\r\n    }\r\n    abort(error) {\r\n        if (error) {\r\n            this.completionDeferred.reject(error);\r\n        }\r\n        if (!this.aborted) {\r\n            logDebug(LOG_TAG$i, 'Aborting transaction:', error ? error.message : 'Client-initiated abort');\r\n            this.aborted = true;\r\n            this.transaction.abort();\r\n        }\r\n    }\r\n    maybeCommit() {\r\n        // If the browser supports V3 IndexedDB, we invoke commit() explicitly to\r\n        // speed up index DB processing if the event loop remains blocks.\r\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n        const maybeV3IndexedDb = this.transaction;\r\n        if (!this.aborted && typeof maybeV3IndexedDb.commit === 'function') {\r\n            maybeV3IndexedDb.commit();\r\n        }\r\n    }\r\n    /**\r\n     * Returns a SimpleDbStore<KeyType, ValueType> for the specified store. All\r\n     * operations performed on the SimpleDbStore happen within the context of this\r\n     * transaction and it cannot be used anymore once the transaction is\r\n     * completed.\r\n     *\r\n     * Note that we can't actually enforce that the KeyType and ValueType are\r\n     * correct, but they allow type safety through the rest of the consuming code.\r\n     */\r\n    store(storeName) {\r\n        const store = this.transaction.objectStore(storeName);\r\n        return new SimpleDbStore(store);\r\n    }\r\n}\r\n/**\r\n * Provides a wrapper around IndexedDb with a simplified interface that uses\r\n * Promise-like return values to chain operations. Real promises cannot be used\r\n * since .then() continuations are executed asynchronously (e.g. via\r\n * .setImmediate), which would cause IndexedDB to end the transaction.\r\n * See PersistencePromise for more details.\r\n */\r\nclass SimpleDb {\r\n    /*\r\n     * Creates a new SimpleDb wrapper for IndexedDb database `name`.\r\n     *\r\n     * Note that `version` must not be a downgrade. IndexedDB does not support\r\n     * downgrading the schema version. We currently do not support any way to do\r\n     * versioning outside of IndexedDB's versioning mechanism, as only\r\n     * version-upgrade transactions are allowed to do things like create\r\n     * objectstores.\r\n     */\r\n    constructor(name, version, schemaConverter) {\r\n        this.name = name;\r\n        this.version = version;\r\n        this.schemaConverter = schemaConverter;\r\n        const iOSVersion = SimpleDb.getIOSVersion(getUA());\r\n        // NOTE: According to https://bugs.webkit.org/show_bug.cgi?id=197050, the\r\n        // bug we're checking for should exist in iOS >= 12.2 and < 13, but for\r\n        // whatever reason it's much harder to hit after 12.2 so we only proactively\r\n        // log on 12.2.\r\n        if (iOSVersion === 12.2) {\r\n            logError('Firestore persistence suffers from a bug in iOS 12.2 ' +\r\n                'Safari that may cause your app to stop working. See ' +\r\n                'https://stackoverflow.com/q/56496296/110915 for details ' +\r\n                'and a potential workaround.');\r\n        }\r\n    }\r\n    /** Deletes the specified database. */\r\n    static delete(name) {\r\n        logDebug(LOG_TAG$i, 'Removing database:', name);\r\n        return wrapRequest(window.indexedDB.deleteDatabase(name)).toPromise();\r\n    }\r\n    /** Returns true if IndexedDB is available in the current environment. */\r\n    static isAvailable() {\r\n        if (!isIndexedDBAvailable()) {\r\n            return false;\r\n        }\r\n        if (SimpleDb.isMockPersistence()) {\r\n            return true;\r\n        }\r\n        // We extensively use indexed array values and compound keys,\r\n        // which IE and Edge do not support. However, they still have indexedDB\r\n        // defined on the window, so we need to check for them here and make sure\r\n        // to return that persistence is not enabled for those browsers.\r\n        // For tracking support of this feature, see here:\r\n        // https://developer.microsoft.com/en-us/microsoft-edge/platform/status/indexeddbarraysandmultientrysupport/\r\n        // Check the UA string to find out the browser.\r\n        const ua = getUA();\r\n        // IE 10\r\n        // ua = 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)';\r\n        // IE 11\r\n        // ua = 'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko';\r\n        // Edge\r\n        // ua = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML,\r\n        // like Gecko) Chrome/39.0.2171.71 Safari/537.36 Edge/12.0';\r\n        // iOS Safari: Disable for users running iOS version < 10.\r\n        const iOSVersion = SimpleDb.getIOSVersion(ua);\r\n        const isUnsupportedIOS = 0 < iOSVersion && iOSVersion < 10;\r\n        // Android browser: Disable for userse running version < 4.5.\r\n        const androidVersion = SimpleDb.getAndroidVersion(ua);\r\n        const isUnsupportedAndroid = 0 < androidVersion && androidVersion < 4.5;\r\n        if (ua.indexOf('MSIE ') > 0 ||\r\n            ua.indexOf('Trident/') > 0 ||\r\n            ua.indexOf('Edge/') > 0 ||\r\n            isUnsupportedIOS ||\r\n            isUnsupportedAndroid) {\r\n            return false;\r\n        }\r\n        else {\r\n            return true;\r\n        }\r\n    }\r\n    /**\r\n     * Returns true if the backing IndexedDB store is the Node IndexedDBShim\r\n     * (see https://github.com/axemclion/IndexedDBShim).\r\n     */\r\n    static isMockPersistence() {\r\n        var _a;\r\n        return (typeof process !== 'undefined' &&\r\n            ((_a = process.env) === null || _a === void 0 ? void 0 : _a.USE_MOCK_PERSISTENCE) === 'YES');\r\n    }\r\n    /** Helper to get a typed SimpleDbStore from a transaction. */\r\n    static getStore(txn, store) {\r\n        return txn.store(store);\r\n    }\r\n    // visible for testing\r\n    /** Parse User Agent to determine iOS version. Returns -1 if not found. */\r\n    static getIOSVersion(ua) {\r\n        const iOSVersionRegex = ua.match(/i(?:phone|pad|pod) os ([\\d_]+)/i);\r\n        const version = iOSVersionRegex\r\n            ? iOSVersionRegex[1].split('_').slice(0, 2).join('.')\r\n            : '-1';\r\n        return Number(version);\r\n    }\r\n    // visible for testing\r\n    /** Parse User Agent to determine Android version. Returns -1 if not found. */\r\n    static getAndroidVersion(ua) {\r\n        const androidVersionRegex = ua.match(/Android ([\\d.]+)/i);\r\n        const version = androidVersionRegex\r\n            ? androidVersionRegex[1].split('.').slice(0, 2).join('.')\r\n            : '-1';\r\n        return Number(version);\r\n    }\r\n    /**\r\n     * Opens the specified database, creating or upgrading it if necessary.\r\n     */\r\n    async ensureDb(action) {\r\n        if (!this.db) {\r\n            logDebug(LOG_TAG$i, 'Opening database:', this.name);\r\n            this.db = await new Promise((resolve, reject) => {\r\n                // TODO(mikelehen): Investigate browser compatibility.\r\n                // https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API/Using_IndexedDB\r\n                // suggests IE9 and older WebKit browsers handle upgrade\r\n                // differently. They expect setVersion, as described here:\r\n                // https://developer.mozilla.org/en-US/docs/Web/API/IDBVersionChangeRequest/setVersion\r\n                const request = indexedDB.open(this.name, this.version);\r\n                request.onsuccess = (event) => {\r\n                    const db = event.target.result;\r\n                    resolve(db);\r\n                };\r\n                request.onblocked = () => {\r\n                    reject(new IndexedDbTransactionError(action, 'Cannot upgrade IndexedDB schema while another tab is open. ' +\r\n                        'Close all tabs that access Firestore and reload this page to proceed.'));\r\n                };\r\n                request.onerror = (event) => {\r\n                    const error = event.target.error;\r\n                    if (error.name === 'VersionError') {\r\n                        reject(new FirestoreError(Code.FAILED_PRECONDITION, 'A newer version of the Firestore SDK was previously used and so the persisted ' +\r\n                            'data is not compatible with the version of the SDK you are now using. The SDK ' +\r\n                            'will operate with persistence disabled. If you need persistence, please ' +\r\n                            're-upgrade to a newer version of the SDK or else clear the persisted IndexedDB ' +\r\n                            'data for your app to start fresh.'));\r\n                    }\r\n                    else if (error.name === 'InvalidStateError') {\r\n                        reject(new FirestoreError(Code.FAILED_PRECONDITION, 'Unable to open an IndexedDB connection. This could be due to running in a ' +\r\n                            'private browsing session on a browser whose private browsing sessions do not ' +\r\n                            'support IndexedDB: ' +\r\n                            error));\r\n                    }\r\n                    else {\r\n                        reject(new IndexedDbTransactionError(action, error));\r\n                    }\r\n                };\r\n                request.onupgradeneeded = (event) => {\r\n                    logDebug(LOG_TAG$i, 'Database \"' + this.name + '\" requires upgrade from version:', event.oldVersion);\r\n                    const db = event.target.result;\r\n                    this.schemaConverter\r\n                        .createOrUpgrade(db, request.transaction, event.oldVersion, this.version)\r\n                        .next(() => {\r\n                        logDebug(LOG_TAG$i, 'Database upgrade to version ' + this.version + ' complete');\r\n                    });\r\n                };\r\n            });\r\n        }\r\n        if (this.versionchangelistener) {\r\n            this.db.onversionchange = event => this.versionchangelistener(event);\r\n        }\r\n        return this.db;\r\n    }\r\n    setVersionChangeListener(versionChangeListener) {\r\n        this.versionchangelistener = versionChangeListener;\r\n        if (this.db) {\r\n            this.db.onversionchange = (event) => {\r\n                return versionChangeListener(event);\r\n            };\r\n        }\r\n    }\r\n    async runTransaction(action, mode, objectStores, transactionFn) {\r\n        const readonly = mode === 'readonly';\r\n        let attemptNumber = 0;\r\n        while (true) {\r\n            ++attemptNumber;\r\n            try {\r\n                this.db = await this.ensureDb(action);\r\n                const transaction = SimpleDbTransaction.open(this.db, action, readonly ? 'readonly' : 'readwrite', objectStores);\r\n                const transactionFnResult = transactionFn(transaction)\r\n                    .next(result => {\r\n                    transaction.maybeCommit();\r\n                    return result;\r\n                })\r\n                    .catch(error => {\r\n                    // Abort the transaction if there was an error.\r\n                    transaction.abort(error);\r\n                    // We cannot actually recover, and calling `abort()` will cause the transaction's\r\n                    // completion promise to be rejected. This in turn means that we won't use\r\n                    // `transactionFnResult` below. We return a rejection here so that we don't add the\r\n                    // possibility of returning `void` to the type of `transactionFnResult`.\r\n                    return PersistencePromise.reject(error);\r\n                })\r\n                    .toPromise();\r\n                // As noted above, errors are propagated by aborting the transaction. So\r\n                // we swallow any error here to avoid the browser logging it as unhandled.\r\n                transactionFnResult.catch(() => { });\r\n                // Wait for the transaction to complete (i.e. IndexedDb's onsuccess event to\r\n                // fire), but still return the original transactionFnResult back to the\r\n                // caller.\r\n                await transaction.completionPromise;\r\n                return transactionFnResult;\r\n            }\r\n            catch (e) {\r\n                const error = e;\r\n                // TODO(schmidt-sebastian): We could probably be smarter about this and\r\n                // not retry exceptions that are likely unrecoverable (such as quota\r\n                // exceeded errors).\r\n                // Note: We cannot use an instanceof check for FirestoreException, since the\r\n                // exception is wrapped in a generic error by our async/await handling.\r\n                const retryable = error.name !== 'FirebaseError' &&\r\n                    attemptNumber < TRANSACTION_RETRY_COUNT;\r\n                logDebug(LOG_TAG$i, 'Transaction failed with error:', error.message, 'Retrying:', retryable);\r\n                this.close();\r\n                if (!retryable) {\r\n                    return Promise.reject(error);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    close() {\r\n        if (this.db) {\r\n            this.db.close();\r\n        }\r\n        this.db = undefined;\r\n    }\r\n}\r\n/**\r\n * A controller for iterating over a key range or index. It allows an iterate\r\n * callback to delete the currently-referenced object, or jump to a new key\r\n * within the key range or index.\r\n */\r\nclass IterationController {\r\n    constructor(dbCursor) {\r\n        this.dbCursor = dbCursor;\r\n        this.shouldStop = false;\r\n        this.nextKey = null;\r\n    }\r\n    get isDone() {\r\n        return this.shouldStop;\r\n    }\r\n    get skipToKey() {\r\n        return this.nextKey;\r\n    }\r\n    set cursor(value) {\r\n        this.dbCursor = value;\r\n    }\r\n    /**\r\n     * This function can be called to stop iteration at any point.\r\n     */\r\n    done() {\r\n        this.shouldStop = true;\r\n    }\r\n    /**\r\n     * This function can be called to skip to that next key, which could be\r\n     * an index or a primary key.\r\n     */\r\n    skip(key) {\r\n        this.nextKey = key;\r\n    }\r\n    /**\r\n     * Delete the current cursor value from the object store.\r\n     *\r\n     * NOTE: You CANNOT do this with a keysOnly query.\r\n     */\r\n    delete() {\r\n        return wrapRequest(this.dbCursor.delete());\r\n    }\r\n}\r\n/** An error that wraps exceptions that thrown during IndexedDB execution. */\r\nclass IndexedDbTransactionError extends FirestoreError {\r\n    constructor(actionName, cause) {\r\n        super(Code.UNAVAILABLE, `IndexedDB transaction '${actionName}' failed: ${cause}`);\r\n        this.name = 'IndexedDbTransactionError';\r\n    }\r\n}\r\n/** Verifies whether `e` is an IndexedDbTransactionError. */\r\nfunction isIndexedDbTransactionError(e) {\r\n    // Use name equality, as instanceof checks on errors don't work with errors\r\n    // that wrap other errors.\r\n    return e.name === 'IndexedDbTransactionError';\r\n}\r\n/**\r\n * A wrapper around an IDBObjectStore providing an API that:\r\n *\r\n * 1) Has generic KeyType / ValueType parameters to provide strongly-typed\r\n * methods for acting against the object store.\r\n * 2) Deals with IndexedDB's onsuccess / onerror event callbacks, making every\r\n * method return a PersistencePromise instead.\r\n * 3) Provides a higher-level API to avoid needing to do excessive wrapping of\r\n * intermediate IndexedDB types (IDBCursorWithValue, etc.)\r\n */\r\nclass SimpleDbStore {\r\n    constructor(store) {\r\n        this.store = store;\r\n    }\r\n    put(keyOrValue, value) {\r\n        let request;\r\n        if (value !== undefined) {\r\n            logDebug(LOG_TAG$i, 'PUT', this.store.name, keyOrValue, value);\r\n            request = this.store.put(value, keyOrValue);\r\n        }\r\n        else {\r\n            logDebug(LOG_TAG$i, 'PUT', this.store.name, '<auto-key>', keyOrValue);\r\n            request = this.store.put(keyOrValue);\r\n        }\r\n        return wrapRequest(request);\r\n    }\r\n    /**\r\n     * Adds a new value into an Object Store and returns the new key. Similar to\r\n     * IndexedDb's `add()`, this method will fail on primary key collisions.\r\n     *\r\n     * @param value - The object to write.\r\n     * @returns The key of the value to add.\r\n     */\r\n    add(value) {\r\n        logDebug(LOG_TAG$i, 'ADD', this.store.name, value, value);\r\n        const request = this.store.add(value);\r\n        return wrapRequest(request);\r\n    }\r\n    /**\r\n     * Gets the object with the specified key from the specified store, or null\r\n     * if no object exists with the specified key.\r\n     *\r\n     * @key The key of the object to get.\r\n     * @returns The object with the specified key or null if no object exists.\r\n     */\r\n    get(key) {\r\n        const request = this.store.get(key);\r\n        // We're doing an unsafe cast to ValueType.\r\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n        return wrapRequest(request).next(result => {\r\n            // Normalize nonexistence to null.\r\n            if (result === undefined) {\r\n                result = null;\r\n            }\r\n            logDebug(LOG_TAG$i, 'GET', this.store.name, key, result);\r\n            return result;\r\n        });\r\n    }\r\n    delete(key) {\r\n        logDebug(LOG_TAG$i, 'DELETE', this.store.name, key);\r\n        const request = this.store.delete(key);\r\n        return wrapRequest(request);\r\n    }\r\n    /**\r\n     * If we ever need more of the count variants, we can add overloads. For now,\r\n     * all we need is to count everything in a store.\r\n     *\r\n     * Returns the number of rows in the store.\r\n     */\r\n    count() {\r\n        logDebug(LOG_TAG$i, 'COUNT', this.store.name);\r\n        const request = this.store.count();\r\n        return wrapRequest(request);\r\n    }\r\n    loadAll(indexOrRange, range) {\r\n        const iterateOptions = this.options(indexOrRange, range);\r\n        // Use `getAll()` if the browser supports IndexedDB v3, as it is roughly\r\n        // 20% faster. Unfortunately, getAll() does not support custom indices.\r\n        if (!iterateOptions.index && typeof this.store.getAll === 'function') {\r\n            const request = this.store.getAll(iterateOptions.range);\r\n            return new PersistencePromise((resolve, reject) => {\r\n                request.onerror = (event) => {\r\n                    reject(event.target.error);\r\n                };\r\n                request.onsuccess = (event) => {\r\n                    resolve(event.target.result);\r\n                };\r\n            });\r\n        }\r\n        else {\r\n            const cursor = this.cursor(iterateOptions);\r\n            const results = [];\r\n            return this.iterateCursor(cursor, (key, value) => {\r\n                results.push(value);\r\n            }).next(() => {\r\n                return results;\r\n            });\r\n        }\r\n    }\r\n    /**\r\n     * Loads the first `count` elements from the provided index range. Loads all\r\n     * elements if no limit is provided.\r\n     */\r\n    loadFirst(range, count) {\r\n        const request = this.store.getAll(range, count === null ? undefined : count);\r\n        return new PersistencePromise((resolve, reject) => {\r\n            request.onerror = (event) => {\r\n                reject(event.target.error);\r\n            };\r\n            request.onsuccess = (event) => {\r\n                resolve(event.target.result);\r\n            };\r\n        });\r\n    }\r\n    deleteAll(indexOrRange, range) {\r\n        logDebug(LOG_TAG$i, 'DELETE ALL', this.store.name);\r\n        const options = this.options(indexOrRange, range);\r\n        options.keysOnly = false;\r\n        const cursor = this.cursor(options);\r\n        return this.iterateCursor(cursor, (key, value, control) => {\r\n            // NOTE: Calling delete() on a cursor is documented as more efficient than\r\n            // calling delete() on an object store with a single key\r\n            // (https://developer.mozilla.org/en-US/docs/Web/API/IDBObjectStore/delete),\r\n            // however, this requires us *not* to use a keysOnly cursor\r\n            // (https://developer.mozilla.org/en-US/docs/Web/API/IDBCursor/delete). We\r\n            // may want to compare the performance of each method.\r\n            return control.delete();\r\n        });\r\n    }\r\n    iterate(optionsOrCallback, callback) {\r\n        let options;\r\n        if (!callback) {\r\n            options = {};\r\n            callback = optionsOrCallback;\r\n        }\r\n        else {\r\n            options = optionsOrCallback;\r\n        }\r\n        const cursor = this.cursor(options);\r\n        return this.iterateCursor(cursor, callback);\r\n    }\r\n    /**\r\n     * Iterates over a store, but waits for the given callback to complete for\r\n     * each entry before iterating the next entry. This allows the callback to do\r\n     * asynchronous work to determine if this iteration should continue.\r\n     *\r\n     * The provided callback should return `true` to continue iteration, and\r\n     * `false` otherwise.\r\n     */\r\n    iterateSerial(callback) {\r\n        const cursorRequest = this.cursor({});\r\n        return new PersistencePromise((resolve, reject) => {\r\n            cursorRequest.onerror = (event) => {\r\n                const error = checkForAndReportiOSError(event.target.error);\r\n                reject(error);\r\n            };\r\n            cursorRequest.onsuccess = (event) => {\r\n                const cursor = event.target.result;\r\n                if (!cursor) {\r\n                    resolve();\r\n                    return;\r\n                }\r\n                callback(cursor.primaryKey, cursor.value).next(shouldContinue => {\r\n                    if (shouldContinue) {\r\n                        cursor.continue();\r\n                    }\r\n                    else {\r\n                        resolve();\r\n                    }\r\n                });\r\n            };\r\n        });\r\n    }\r\n    iterateCursor(cursorRequest, fn) {\r\n        const results = [];\r\n        return new PersistencePromise((resolve, reject) => {\r\n            cursorRequest.onerror = (event) => {\r\n                reject(event.target.error);\r\n            };\r\n            cursorRequest.onsuccess = (event) => {\r\n                const cursor = event.target.result;\r\n                if (!cursor) {\r\n                    resolve();\r\n                    return;\r\n                }\r\n                const controller = new IterationController(cursor);\r\n                const userResult = fn(cursor.primaryKey, cursor.value, controller);\r\n                if (userResult instanceof PersistencePromise) {\r\n                    const userPromise = userResult.catch(err => {\r\n                        controller.done();\r\n                        return PersistencePromise.reject(err);\r\n                    });\r\n                    results.push(userPromise);\r\n                }\r\n                if (controller.isDone) {\r\n                    resolve();\r\n                }\r\n                else if (controller.skipToKey === null) {\r\n                    cursor.continue();\r\n                }\r\n                else {\r\n                    cursor.continue(controller.skipToKey);\r\n                }\r\n            };\r\n        }).next(() => PersistencePromise.waitFor(results));\r\n    }\r\n    options(indexOrRange, range) {\r\n        let indexName = undefined;\r\n        if (indexOrRange !== undefined) {\r\n            if (typeof indexOrRange === 'string') {\r\n                indexName = indexOrRange;\r\n            }\r\n            else {\r\n                range = indexOrRange;\r\n            }\r\n        }\r\n        return { index: indexName, range };\r\n    }\r\n    cursor(options) {\r\n        let direction = 'next';\r\n        if (options.reverse) {\r\n            direction = 'prev';\r\n        }\r\n        if (options.index) {\r\n            const index = this.store.index(options.index);\r\n            if (options.keysOnly) {\r\n                return index.openKeyCursor(options.range, direction);\r\n            }\r\n            else {\r\n                return index.openCursor(options.range, direction);\r\n            }\r\n        }\r\n        else {\r\n            return this.store.openCursor(options.range, direction);\r\n        }\r\n    }\r\n}\r\n/**\r\n * Wraps an IDBRequest in a PersistencePromise, using the onsuccess / onerror\r\n * handlers to resolve / reject the PersistencePromise as appropriate.\r\n */\r\nfunction wrapRequest(request) {\r\n    return new PersistencePromise((resolve, reject) => {\r\n        request.onsuccess = (event) => {\r\n            const result = event.target.result;\r\n            resolve(result);\r\n        };\r\n        request.onerror = (event) => {\r\n            const error = checkForAndReportiOSError(event.target.error);\r\n            reject(error);\r\n        };\r\n    });\r\n}\r\n// Guard so we only report the error once.\r\nlet reportedIOSError = false;\r\nfunction checkForAndReportiOSError(error) {\r\n    const iOSVersion = SimpleDb.getIOSVersion(getUA());\r\n    if (iOSVersion >= 12.2 && iOSVersion < 13) {\r\n        const IOS_ERROR = 'An internal error was encountered in the Indexed Database server';\r\n        if (error.message.indexOf(IOS_ERROR) >= 0) {\r\n            // Wrap error in a more descriptive one.\r\n            const newError = new FirestoreError('internal', `IOS_INDEXEDDB_BUG1: IndexedDb has thrown '${IOS_ERROR}'. This is likely ` +\r\n                `due to an unavoidable bug in iOS. See https://stackoverflow.com/q/56496296/110915 ` +\r\n                `for details and a potential workaround.`);\r\n            if (!reportedIOSError) {\r\n                reportedIOSError = true;\r\n                // Throw a global exception outside of this promise chain, for the user to\r\n                // potentially catch.\r\n                setTimeout(() => {\r\n                    throw newError;\r\n                }, 0);\r\n            }\r\n            return newError;\r\n        }\r\n    }\r\n    return error;\r\n}\n\nconst LOG_TAG$h = 'IndexBackiller';\r\n/** How long we wait to try running index backfill after SDK initialization. */\r\nconst INITIAL_BACKFILL_DELAY_MS = 15 * 1000;\r\n/** Minimum amount of time between backfill checks, after the first one. */\r\nconst REGULAR_BACKFILL_DELAY_MS = 60 * 1000;\r\n/** The maximum number of documents to process each time backfill() is called. */\r\nconst MAX_DOCUMENTS_TO_PROCESS = 50;\r\n/** This class is responsible for the scheduling of Index Backfiller. */\r\nclass IndexBackfillerScheduler {\r\n    constructor(asyncQueue, backfiller) {\r\n        this.asyncQueue = asyncQueue;\r\n        this.backfiller = backfiller;\r\n        this.task = null;\r\n    }\r\n    start() {\r\n        this.schedule(INITIAL_BACKFILL_DELAY_MS);\r\n    }\r\n    stop() {\r\n        if (this.task) {\r\n            this.task.cancel();\r\n            this.task = null;\r\n        }\r\n    }\r\n    get started() {\r\n        return this.task !== null;\r\n    }\r\n    schedule(delay) {\r\n        logDebug(LOG_TAG$h, `Scheduled in ${delay}ms`);\r\n        this.task = this.asyncQueue.enqueueAfterDelay(\"index_backfill\" /* IndexBackfill */, delay, async () => {\r\n            this.task = null;\r\n            try {\r\n                const documentsProcessed = await this.backfiller.backfill();\r\n                logDebug(LOG_TAG$h, `Documents written: ${documentsProcessed}`);\r\n            }\r\n            catch (e) {\r\n                if (isIndexedDbTransactionError(e)) {\r\n                    logDebug(LOG_TAG$h, 'Ignoring IndexedDB error during index backfill: ', e);\r\n                }\r\n                else {\r\n                    await ignoreIfPrimaryLeaseLoss(e);\r\n                }\r\n            }\r\n            await this.schedule(REGULAR_BACKFILL_DELAY_MS);\r\n        });\r\n    }\r\n}\r\n/** Implements the steps for backfilling indexes. */\r\nclass IndexBackfiller {\r\n    constructor(\r\n    /**\r\n     * LocalStore provides access to IndexManager and LocalDocumentView.\r\n     * These properties will update when the user changes. Consequently,\r\n     * making a local copy of IndexManager and LocalDocumentView will require\r\n     * updates over time. The simpler solution is to rely on LocalStore to have\r\n     * an up-to-date references to IndexManager and LocalDocumentStore.\r\n     */\r\n    localStore, persistence) {\r\n        this.localStore = localStore;\r\n        this.persistence = persistence;\r\n    }\r\n    async backfill(maxDocumentsToProcess = MAX_DOCUMENTS_TO_PROCESS) {\r\n        return this.persistence.runTransaction('Backfill Indexes', 'readwrite-primary', txn => this.writeIndexEntries(txn, maxDocumentsToProcess));\r\n    }\r\n    /** Writes index entries until the cap is reached. Returns the number of documents processed. */\r\n    writeIndexEntries(transation, maxDocumentsToProcess) {\r\n        const processedCollectionGroups = new Set();\r\n        let documentsRemaining = maxDocumentsToProcess;\r\n        let continueLoop = true;\r\n        return PersistencePromise.doWhile(() => continueLoop === true && documentsRemaining > 0, () => {\r\n            return this.localStore.indexManager\r\n                .getNextCollectionGroupToUpdate(transation)\r\n                .next((collectionGroup) => {\r\n                if (collectionGroup === null ||\r\n                    processedCollectionGroups.has(collectionGroup)) {\r\n                    continueLoop = false;\r\n                }\r\n                else {\r\n                    logDebug(LOG_TAG$h, `Processing collection: ${collectionGroup}`);\r\n                    return this.writeEntriesForCollectionGroup(transation, collectionGroup, documentsRemaining).next(documentsProcessed => {\r\n                        documentsRemaining -= documentsProcessed;\r\n                        processedCollectionGroups.add(collectionGroup);\r\n                    });\r\n                }\r\n            });\r\n        }).next(() => maxDocumentsToProcess - documentsRemaining);\r\n    }\r\n    /**\r\n     * Writes entries for the provided collection group. Returns the number of documents processed.\r\n     */\r\n    writeEntriesForCollectionGroup(transaction, collectionGroup, documentsRemainingUnderCap) {\r\n        // Use the earliest offset of all field indexes to query the local cache.\r\n        return this.localStore.indexManager\r\n            .getMinOffsetFromCollectionGroup(transaction, collectionGroup)\r\n            .next(existingOffset => this.localStore.localDocuments\r\n            .getNextDocuments(transaction, collectionGroup, existingOffset, documentsRemainingUnderCap)\r\n            .next(nextBatch => {\r\n            const docs = nextBatch.changes;\r\n            return this.localStore.indexManager\r\n                .updateIndexEntries(transaction, docs)\r\n                .next(() => this.getNewOffset(existingOffset, nextBatch))\r\n                .next(newOffset => {\r\n                logDebug(LOG_TAG$h, `Updating offset: ${newOffset}`);\r\n                return this.localStore.indexManager.updateCollectionGroup(transaction, collectionGroup, newOffset);\r\n            })\r\n                .next(() => docs.size);\r\n        }));\r\n    }\r\n    /** Returns the next offset based on the provided documents. */\r\n    getNewOffset(existingOffset, lookupResult) {\r\n        let maxOffset = existingOffset;\r\n        lookupResult.changes.forEach((key, document) => {\r\n            const newOffset = newIndexOffsetFromDocument(document);\r\n            if (indexOffsetComparator(newOffset, maxOffset) > 0) {\r\n                maxOffset = newOffset;\r\n            }\r\n        });\r\n        return new IndexOffset(maxOffset.readTime, maxOffset.documentKey, Math.max(lookupResult.batchId, existingOffset.largestBatchId));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * `ListenSequence` is a monotonic sequence. It is initialized with a minimum value to\r\n * exceed. All subsequent calls to next will return increasing values. If provided with a\r\n * `SequenceNumberSyncer`, it will additionally bump its next value when told of a new value, as\r\n * well as write out sequence numbers that it produces via `next()`.\r\n */\r\nclass ListenSequence {\r\n    constructor(previousValue, sequenceNumberSyncer) {\r\n        this.previousValue = previousValue;\r\n        if (sequenceNumberSyncer) {\r\n            sequenceNumberSyncer.sequenceNumberHandler = sequenceNumber => this.setPreviousValue(sequenceNumber);\r\n            this.writeNewSequenceNumber = sequenceNumber => sequenceNumberSyncer.writeSequenceNumber(sequenceNumber);\r\n        }\r\n    }\r\n    setPreviousValue(externalPreviousValue) {\r\n        this.previousValue = Math.max(externalPreviousValue, this.previousValue);\r\n        return this.previousValue;\r\n    }\r\n    next() {\r\n        const nextValue = ++this.previousValue;\r\n        if (this.writeNewSequenceNumber) {\r\n            this.writeNewSequenceNumber(nextValue);\r\n        }\r\n        return nextValue;\r\n    }\r\n}\r\nListenSequence.INVALID = -1;\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst escapeChar = '\\u0001';\r\nconst encodedSeparatorChar = '\\u0001';\r\nconst encodedNul = '\\u0010';\r\nconst encodedEscape = '\\u0011';\r\n/**\r\n * Encodes a resource path into a IndexedDb-compatible string form.\r\n */\r\nfunction encodeResourcePath(path) {\r\n    let result = '';\r\n    for (let i = 0; i < path.length; i++) {\r\n        if (result.length > 0) {\r\n            result = encodeSeparator(result);\r\n        }\r\n        result = encodeSegment(path.get(i), result);\r\n    }\r\n    return encodeSeparator(result);\r\n}\r\n/** Encodes a single segment of a resource path into the given result */\r\nfunction encodeSegment(segment, resultBuf) {\r\n    let result = resultBuf;\r\n    const length = segment.length;\r\n    for (let i = 0; i < length; i++) {\r\n        const c = segment.charAt(i);\r\n        switch (c) {\r\n            case '\\0':\r\n                result += escapeChar + encodedNul;\r\n                break;\r\n            case escapeChar:\r\n                result += escapeChar + encodedEscape;\r\n                break;\r\n            default:\r\n                result += c;\r\n        }\r\n    }\r\n    return result;\r\n}\r\n/** Encodes a path separator into the given result */\r\nfunction encodeSeparator(result) {\r\n    return result + escapeChar + encodedSeparatorChar;\r\n}\r\n/**\r\n * Decodes the given IndexedDb-compatible string form of a resource path into\r\n * a ResourcePath instance. Note that this method is not suitable for use with\r\n * decoding resource names from the server; those are One Platform format\r\n * strings.\r\n */\r\nfunction decodeResourcePath(path) {\r\n    // Event the empty path must encode as a path of at least length 2. A path\r\n    // with exactly 2 must be the empty path.\r\n    const length = path.length;\r\n    hardAssert(length >= 2);\r\n    if (length === 2) {\r\n        hardAssert(path.charAt(0) === escapeChar && path.charAt(1) === encodedSeparatorChar);\r\n        return ResourcePath.emptyPath();\r\n    }\r\n    // Escape characters cannot exist past the second-to-last position in the\r\n    // source value.\r\n    const lastReasonableEscapeIndex = length - 2;\r\n    const segments = [];\r\n    let segmentBuilder = '';\r\n    for (let start = 0; start < length;) {\r\n        // The last two characters of a valid encoded path must be a separator, so\r\n        // there must be an end to this segment.\r\n        const end = path.indexOf(escapeChar, start);\r\n        if (end < 0 || end > lastReasonableEscapeIndex) {\r\n            fail();\r\n        }\r\n        const next = path.charAt(end + 1);\r\n        switch (next) {\r\n            case encodedSeparatorChar:\r\n                const currentPiece = path.substring(start, end);\r\n                let segment;\r\n                if (segmentBuilder.length === 0) {\r\n                    // Avoid copying for the common case of a segment that excludes \\0\r\n                    // and \\001\r\n                    segment = currentPiece;\r\n                }\r\n                else {\r\n                    segmentBuilder += currentPiece;\r\n                    segment = segmentBuilder;\r\n                    segmentBuilder = '';\r\n                }\r\n                segments.push(segment);\r\n                break;\r\n            case encodedNul:\r\n                segmentBuilder += path.substring(start, end);\r\n                segmentBuilder += '\\0';\r\n                break;\r\n            case encodedEscape:\r\n                // The escape character can be used in the output to encode itself.\r\n                segmentBuilder += path.substring(start, end + 1);\r\n                break;\r\n            default:\r\n                fail();\r\n        }\r\n        start = end + 2;\r\n    }\r\n    return new ResourcePath(segments);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst DbRemoteDocumentStore$1 = 'remoteDocuments';\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Name of the IndexedDb object store.\r\n *\r\n * Note that the name 'owner' is chosen to ensure backwards compatibility with\r\n * older clients that only supported single locked access to the persistence\r\n * layer.\r\n */\r\nconst DbPrimaryClientStore = 'owner';\r\n/**\r\n * The key string used for the single object that exists in the\r\n * DbPrimaryClient store.\r\n */\r\nconst DbPrimaryClientKey = 'owner';\r\n/** Name of the IndexedDb object store.  */\r\nconst DbMutationQueueStore = 'mutationQueues';\r\n/** Keys are automatically assigned via the userId property. */\r\nconst DbMutationQueueKeyPath = 'userId';\r\n/** Name of the IndexedDb object store.  */\r\nconst DbMutationBatchStore = 'mutations';\r\n/** Keys are automatically assigned via the userId, batchId properties. */\r\nconst DbMutationBatchKeyPath = 'batchId';\r\n/** The index name for lookup of mutations by user. */\r\nconst DbMutationBatchUserMutationsIndex = 'userMutationsIndex';\r\n/** The user mutations index is keyed by [userId, batchId] pairs. */\r\nconst DbMutationBatchUserMutationsKeyPath = ['userId', 'batchId'];\r\n/**\r\n * Creates a [userId] key for use in the DbDocumentMutations index to iterate\r\n * over all of a user's document mutations.\r\n */\r\nfunction newDbDocumentMutationPrefixForUser(userId) {\r\n    return [userId];\r\n}\r\n/**\r\n * Creates a [userId, encodedPath] key for use in the DbDocumentMutations\r\n * index to iterate over all at document mutations for a given path or lower.\r\n */\r\nfunction newDbDocumentMutationPrefixForPath(userId, path) {\r\n    return [userId, encodeResourcePath(path)];\r\n}\r\n/**\r\n * Creates a full index key of [userId, encodedPath, batchId] for inserting\r\n * and deleting into the DbDocumentMutations index.\r\n */\r\nfunction newDbDocumentMutationKey(userId, path, batchId) {\r\n    return [userId, encodeResourcePath(path), batchId];\r\n}\r\n/**\r\n * Because we store all the useful information for this store in the key,\r\n * there is no useful information to store as the value. The raw (unencoded)\r\n * path cannot be stored because IndexedDb doesn't store prototype\r\n * information.\r\n */\r\nconst DbDocumentMutationPlaceholder = {};\r\nconst DbDocumentMutationStore = 'documentMutations';\r\nconst DbRemoteDocumentStore = 'remoteDocumentsV14';\r\n/**\r\n * The primary key of the remote documents store, which allows for efficient\r\n * access by collection path and read time.\r\n */\r\nconst DbRemoteDocumentKeyPath = [\r\n    'prefixPath',\r\n    'collectionGroup',\r\n    'readTime',\r\n    'documentId'\r\n];\r\n/** An index that provides access to documents by key. */\r\nconst DbRemoteDocumentDocumentKeyIndex = 'documentKeyIndex';\r\nconst DbRemoteDocumentDocumentKeyIndexPath = [\r\n    'prefixPath',\r\n    'collectionGroup',\r\n    'documentId'\r\n];\r\n/**\r\n * An index that provides access to documents by collection group and read\r\n * time.\r\n *\r\n * This index is used by the index backfiller.\r\n */\r\nconst DbRemoteDocumentCollectionGroupIndex = 'collectionGroupIndex';\r\nconst DbRemoteDocumentCollectionGroupIndexPath = [\r\n    'collectionGroup',\r\n    'readTime',\r\n    'prefixPath',\r\n    'documentId'\r\n];\r\nconst DbRemoteDocumentGlobalStore = 'remoteDocumentGlobal';\r\nconst DbRemoteDocumentGlobalKey = 'remoteDocumentGlobalKey';\r\nconst DbTargetStore = 'targets';\r\n/** Keys are automatically assigned via the targetId property. */\r\nconst DbTargetKeyPath = 'targetId';\r\n/** The name of the queryTargets index. */\r\nconst DbTargetQueryTargetsIndexName = 'queryTargetsIndex';\r\n/**\r\n * The index of all canonicalIds to the targets that they match. This is not\r\n * a unique mapping because canonicalId does not promise a unique name for all\r\n * possible queries, so we append the targetId to make the mapping unique.\r\n */\r\nconst DbTargetQueryTargetsKeyPath = ['canonicalId', 'targetId'];\r\n/** Name of the IndexedDb object store.  */\r\nconst DbTargetDocumentStore = 'targetDocuments';\r\n/** Keys are automatically assigned via the targetId, path properties. */\r\nconst DbTargetDocumentKeyPath = ['targetId', 'path'];\r\n/** The index name for the reverse index. */\r\nconst DbTargetDocumentDocumentTargetsIndex = 'documentTargetsIndex';\r\n/** We also need to create the reverse index for these properties. */\r\nconst DbTargetDocumentDocumentTargetsKeyPath = ['path', 'targetId'];\r\n/**\r\n * The key string used for the single object that exists in the\r\n * DbTargetGlobal store.\r\n */\r\nconst DbTargetGlobalKey = 'targetGlobalKey';\r\nconst DbTargetGlobalStore = 'targetGlobal';\r\n/** Name of the IndexedDb object store. */\r\nconst DbCollectionParentStore = 'collectionParents';\r\n/** Keys are automatically assigned via the collectionId, parent properties. */\r\nconst DbCollectionParentKeyPath = ['collectionId', 'parent'];\r\n/** Name of the IndexedDb object store. */\r\nconst DbClientMetadataStore = 'clientMetadata';\r\n/** Keys are automatically assigned via the clientId properties. */\r\nconst DbClientMetadataKeyPath = 'clientId';\r\n/** Name of the IndexedDb object store. */\r\nconst DbBundleStore = 'bundles';\r\nconst DbBundleKeyPath = 'bundleId';\r\n/** Name of the IndexedDb object store. */\r\nconst DbNamedQueryStore = 'namedQueries';\r\nconst DbNamedQueryKeyPath = 'name';\r\n/** Name of the IndexedDb object store. */\r\nconst DbIndexConfigurationStore = 'indexConfiguration';\r\nconst DbIndexConfigurationKeyPath = 'indexId';\r\n/**\r\n * An index that provides access to the index configurations by collection\r\n * group.\r\n *\r\n * PORTING NOTE: iOS and Android maintain this index in-memory, but this is\r\n * not possible here as the Web client supports concurrent access to\r\n * persistence via multi-tab.\r\n */\r\nconst DbIndexConfigurationCollectionGroupIndex = 'collectionGroupIndex';\r\nconst DbIndexConfigurationCollectionGroupIndexPath = 'collectionGroup';\r\n/** Name of the IndexedDb object store. */\r\nconst DbIndexStateStore = 'indexState';\r\nconst DbIndexStateKeyPath = ['indexId', 'uid'];\r\n/**\r\n * An index that provides access to documents in a collection sorted by last\r\n * update time. Used by the backfiller.\r\n *\r\n * PORTING NOTE: iOS and Android maintain this index in-memory, but this is\r\n * not possible here as the Web client supports concurrent access to\r\n * persistence via multi-tab.\r\n */\r\nconst DbIndexStateSequenceNumberIndex = 'sequenceNumberIndex';\r\nconst DbIndexStateSequenceNumberIndexPath = ['uid', 'sequenceNumber'];\r\n/** Name of the IndexedDb object store. */\r\nconst DbIndexEntryStore = 'indexEntries';\r\nconst DbIndexEntryKeyPath = [\r\n    'indexId',\r\n    'uid',\r\n    'arrayValue',\r\n    'directionalValue',\r\n    'orderedDocumentKey',\r\n    'documentKey'\r\n];\r\nconst DbIndexEntryDocumentKeyIndex = 'documentKeyIndex';\r\nconst DbIndexEntryDocumentKeyIndexPath = [\r\n    'indexId',\r\n    'uid',\r\n    'orderedDocumentKey'\r\n];\r\n/** Name of the IndexedDb object store. */\r\nconst DbDocumentOverlayStore = 'documentOverlays';\r\nconst DbDocumentOverlayKeyPath = [\r\n    'userId',\r\n    'collectionPath',\r\n    'documentId'\r\n];\r\nconst DbDocumentOverlayCollectionPathOverlayIndex = 'collectionPathOverlayIndex';\r\nconst DbDocumentOverlayCollectionPathOverlayIndexPath = [\r\n    'userId',\r\n    'collectionPath',\r\n    'largestBatchId'\r\n];\r\nconst DbDocumentOverlayCollectionGroupOverlayIndex = 'collectionGroupOverlayIndex';\r\nconst DbDocumentOverlayCollectionGroupOverlayIndexPath = [\r\n    'userId',\r\n    'collectionGroup',\r\n    'largestBatchId'\r\n];\r\n// Visible for testing\r\nconst V1_STORES = [\r\n    DbMutationQueueStore,\r\n    DbMutationBatchStore,\r\n    DbDocumentMutationStore,\r\n    DbRemoteDocumentStore$1,\r\n    DbTargetStore,\r\n    DbPrimaryClientStore,\r\n    DbTargetGlobalStore,\r\n    DbTargetDocumentStore\r\n];\r\n// Visible for testing\r\nconst V3_STORES = V1_STORES;\r\n// Note: DbRemoteDocumentChanges is no longer used and dropped with v9.\r\nconst V4_STORES = [...V3_STORES, DbClientMetadataStore];\r\nconst V6_STORES = [...V4_STORES, DbRemoteDocumentGlobalStore];\r\nconst V8_STORES = [...V6_STORES, DbCollectionParentStore];\r\nconst V11_STORES = [...V8_STORES, DbBundleStore, DbNamedQueryStore];\r\nconst V12_STORES = [...V11_STORES, DbDocumentOverlayStore];\r\nconst V13_STORES = [\r\n    DbMutationQueueStore,\r\n    DbMutationBatchStore,\r\n    DbDocumentMutationStore,\r\n    DbRemoteDocumentStore,\r\n    DbTargetStore,\r\n    DbPrimaryClientStore,\r\n    DbTargetGlobalStore,\r\n    DbTargetDocumentStore,\r\n    DbClientMetadataStore,\r\n    DbRemoteDocumentGlobalStore,\r\n    DbCollectionParentStore,\r\n    DbBundleStore,\r\n    DbNamedQueryStore,\r\n    DbDocumentOverlayStore\r\n];\r\nconst V14_STORES = V13_STORES;\r\nconst V15_STORES = [\r\n    ...V14_STORES,\r\n    DbIndexConfigurationStore,\r\n    DbIndexStateStore,\r\n    DbIndexEntryStore\r\n];\r\n/** Returns the object stores for the provided schema. */\r\nfunction getObjectStores(schemaVersion) {\r\n    if (schemaVersion === 15) {\r\n        return V15_STORES;\r\n    }\r\n    else if (schemaVersion === 14) {\r\n        return V14_STORES;\r\n    }\r\n    else if (schemaVersion === 13) {\r\n        return V13_STORES;\r\n    }\r\n    else if (schemaVersion === 12) {\r\n        return V12_STORES;\r\n    }\r\n    else if (schemaVersion === 11) {\r\n        return V11_STORES;\r\n    }\r\n    else {\r\n        fail();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass IndexedDbTransaction extends PersistenceTransaction {\r\n    constructor(simpleDbTransaction, currentSequenceNumber) {\r\n        super();\r\n        this.simpleDbTransaction = simpleDbTransaction;\r\n        this.currentSequenceNumber = currentSequenceNumber;\r\n    }\r\n}\r\nfunction getStore(txn, store) {\r\n    const indexedDbTransaction = debugCast(txn);\r\n    return SimpleDb.getStore(indexedDbTransaction.simpleDbTransaction, store);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nfunction objectSize(obj) {\r\n    let count = 0;\r\n    for (const key in obj) {\r\n        if (Object.prototype.hasOwnProperty.call(obj, key)) {\r\n            count++;\r\n        }\r\n    }\r\n    return count;\r\n}\r\nfunction forEach(obj, fn) {\r\n    for (const key in obj) {\r\n        if (Object.prototype.hasOwnProperty.call(obj, key)) {\r\n            fn(key, obj[key]);\r\n        }\r\n    }\r\n}\r\nfunction isEmpty(obj) {\r\n    for (const key in obj) {\r\n        if (Object.prototype.hasOwnProperty.call(obj, key)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// An immutable sorted map implementation, based on a Left-leaning Red-Black\r\n// tree.\r\nclass SortedMap {\r\n    constructor(comparator, root) {\r\n        this.comparator = comparator;\r\n        this.root = root ? root : LLRBNode.EMPTY;\r\n    }\r\n    // Returns a copy of the map, with the specified key/value added or replaced.\r\n    insert(key, value) {\r\n        return new SortedMap(this.comparator, this.root\r\n            .insert(key, value, this.comparator)\r\n            .copy(null, null, LLRBNode.BLACK, null, null));\r\n    }\r\n    // Returns a copy of the map, with the specified key removed.\r\n    remove(key) {\r\n        return new SortedMap(this.comparator, this.root\r\n            .remove(key, this.comparator)\r\n            .copy(null, null, LLRBNode.BLACK, null, null));\r\n    }\r\n    // Returns the value of the node with the given key, or null.\r\n    get(key) {\r\n        let node = this.root;\r\n        while (!node.isEmpty()) {\r\n            const cmp = this.comparator(key, node.key);\r\n            if (cmp === 0) {\r\n                return node.value;\r\n            }\r\n            else if (cmp < 0) {\r\n                node = node.left;\r\n            }\r\n            else if (cmp > 0) {\r\n                node = node.right;\r\n            }\r\n        }\r\n        return null;\r\n    }\r\n    // Returns the index of the element in this sorted map, or -1 if it doesn't\r\n    // exist.\r\n    indexOf(key) {\r\n        // Number of nodes that were pruned when descending right\r\n        let prunedNodes = 0;\r\n        let node = this.root;\r\n        while (!node.isEmpty()) {\r\n            const cmp = this.comparator(key, node.key);\r\n            if (cmp === 0) {\r\n                return prunedNodes + node.left.size;\r\n            }\r\n            else if (cmp < 0) {\r\n                node = node.left;\r\n            }\r\n            else {\r\n                // Count all nodes left of the node plus the node itself\r\n                prunedNodes += node.left.size + 1;\r\n                node = node.right;\r\n            }\r\n        }\r\n        // Node not found\r\n        return -1;\r\n    }\r\n    isEmpty() {\r\n        return this.root.isEmpty();\r\n    }\r\n    // Returns the total number of nodes in the map.\r\n    get size() {\r\n        return this.root.size;\r\n    }\r\n    // Returns the minimum key in the map.\r\n    minKey() {\r\n        return this.root.minKey();\r\n    }\r\n    // Returns the maximum key in the map.\r\n    maxKey() {\r\n        return this.root.maxKey();\r\n    }\r\n    // Traverses the map in key order and calls the specified action function\r\n    // for each key/value pair. If action returns true, traversal is aborted.\r\n    // Returns the first truthy value returned by action, or the last falsey\r\n    // value returned by action.\r\n    inorderTraversal(action) {\r\n        return this.root.inorderTraversal(action);\r\n    }\r\n    forEach(fn) {\r\n        this.inorderTraversal((k, v) => {\r\n            fn(k, v);\r\n            return false;\r\n        });\r\n    }\r\n    toString() {\r\n        const descriptions = [];\r\n        this.inorderTraversal((k, v) => {\r\n            descriptions.push(`${k}:${v}`);\r\n            return false;\r\n        });\r\n        return `{${descriptions.join(', ')}}`;\r\n    }\r\n    // Traverses the map in reverse key order and calls the specified action\r\n    // function for each key/value pair. If action returns true, traversal is\r\n    // aborted.\r\n    // Returns the first truthy value returned by action, or the last falsey\r\n    // value returned by action.\r\n    reverseTraversal(action) {\r\n        return this.root.reverseTraversal(action);\r\n    }\r\n    // Returns an iterator over the SortedMap.\r\n    getIterator() {\r\n        return new SortedMapIterator(this.root, null, this.comparator, false);\r\n    }\r\n    getIteratorFrom(key) {\r\n        return new SortedMapIterator(this.root, key, this.comparator, false);\r\n    }\r\n    getReverseIterator() {\r\n        return new SortedMapIterator(this.root, null, this.comparator, true);\r\n    }\r\n    getReverseIteratorFrom(key) {\r\n        return new SortedMapIterator(this.root, key, this.comparator, true);\r\n    }\r\n} // end SortedMap\r\n// An iterator over an LLRBNode.\r\nclass SortedMapIterator {\r\n    constructor(node, startKey, comparator, isReverse) {\r\n        this.isReverse = isReverse;\r\n        this.nodeStack = [];\r\n        let cmp = 1;\r\n        while (!node.isEmpty()) {\r\n            cmp = startKey ? comparator(node.key, startKey) : 1;\r\n            // flip the comparison if we're going in reverse\r\n            if (startKey && isReverse) {\r\n                cmp *= -1;\r\n            }\r\n            if (cmp < 0) {\r\n                // This node is less than our start key. ignore it\r\n                if (this.isReverse) {\r\n                    node = node.left;\r\n                }\r\n                else {\r\n                    node = node.right;\r\n                }\r\n            }\r\n            else if (cmp === 0) {\r\n                // This node is exactly equal to our start key. Push it on the stack,\r\n                // but stop iterating;\r\n                this.nodeStack.push(node);\r\n                break;\r\n            }\r\n            else {\r\n                // This node is greater than our start key, add it to the stack and move\r\n                // to the next one\r\n                this.nodeStack.push(node);\r\n                if (this.isReverse) {\r\n                    node = node.right;\r\n                }\r\n                else {\r\n                    node = node.left;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    getNext() {\r\n        let node = this.nodeStack.pop();\r\n        const result = { key: node.key, value: node.value };\r\n        if (this.isReverse) {\r\n            node = node.left;\r\n            while (!node.isEmpty()) {\r\n                this.nodeStack.push(node);\r\n                node = node.right;\r\n            }\r\n        }\r\n        else {\r\n            node = node.right;\r\n            while (!node.isEmpty()) {\r\n                this.nodeStack.push(node);\r\n                node = node.left;\r\n            }\r\n        }\r\n        return result;\r\n    }\r\n    hasNext() {\r\n        return this.nodeStack.length > 0;\r\n    }\r\n    peek() {\r\n        if (this.nodeStack.length === 0) {\r\n            return null;\r\n        }\r\n        const node = this.nodeStack[this.nodeStack.length - 1];\r\n        return { key: node.key, value: node.value };\r\n    }\r\n} // end SortedMapIterator\r\n// Represents a node in a Left-leaning Red-Black tree.\r\nclass LLRBNode {\r\n    constructor(key, value, color, left, right) {\r\n        this.key = key;\r\n        this.value = value;\r\n        this.color = color != null ? color : LLRBNode.RED;\r\n        this.left = left != null ? left : LLRBNode.EMPTY;\r\n        this.right = right != null ? right : LLRBNode.EMPTY;\r\n        this.size = this.left.size + 1 + this.right.size;\r\n    }\r\n    // Returns a copy of the current node, optionally replacing pieces of it.\r\n    copy(key, value, color, left, right) {\r\n        return new LLRBNode(key != null ? key : this.key, value != null ? value : this.value, color != null ? color : this.color, left != null ? left : this.left, right != null ? right : this.right);\r\n    }\r\n    isEmpty() {\r\n        return false;\r\n    }\r\n    // Traverses the tree in key order and calls the specified action function\r\n    // for each node. If action returns true, traversal is aborted.\r\n    // Returns the first truthy value returned by action, or the last falsey\r\n    // value returned by action.\r\n    inorderTraversal(action) {\r\n        return (this.left.inorderTraversal(action) ||\r\n            action(this.key, this.value) ||\r\n            this.right.inorderTraversal(action));\r\n    }\r\n    // Traverses the tree in reverse key order and calls the specified action\r\n    // function for each node. If action returns true, traversal is aborted.\r\n    // Returns the first truthy value returned by action, or the last falsey\r\n    // value returned by action.\r\n    reverseTraversal(action) {\r\n        return (this.right.reverseTraversal(action) ||\r\n            action(this.key, this.value) ||\r\n            this.left.reverseTraversal(action));\r\n    }\r\n    // Returns the minimum node in the tree.\r\n    min() {\r\n        if (this.left.isEmpty()) {\r\n            return this;\r\n        }\r\n        else {\r\n            return this.left.min();\r\n        }\r\n    }\r\n    // Returns the maximum key in the tree.\r\n    minKey() {\r\n        return this.min().key;\r\n    }\r\n    // Returns the maximum key in the tree.\r\n    maxKey() {\r\n        if (this.right.isEmpty()) {\r\n            return this.key;\r\n        }\r\n        else {\r\n            return this.right.maxKey();\r\n        }\r\n    }\r\n    // Returns new tree, with the key/value added.\r\n    insert(key, value, comparator) {\r\n        let n = this;\r\n        const cmp = comparator(key, n.key);\r\n        if (cmp < 0) {\r\n            n = n.copy(null, null, null, n.left.insert(key, value, comparator), null);\r\n        }\r\n        else if (cmp === 0) {\r\n            n = n.copy(null, value, null, null, null);\r\n        }\r\n        else {\r\n            n = n.copy(null, null, null, null, n.right.insert(key, value, comparator));\r\n        }\r\n        return n.fixUp();\r\n    }\r\n    removeMin() {\r\n        if (this.left.isEmpty()) {\r\n            return LLRBNode.EMPTY;\r\n        }\r\n        let n = this;\r\n        if (!n.left.isRed() && !n.left.left.isRed()) {\r\n            n = n.moveRedLeft();\r\n        }\r\n        n = n.copy(null, null, null, n.left.removeMin(), null);\r\n        return n.fixUp();\r\n    }\r\n    // Returns new tree, with the specified item removed.\r\n    remove(key, comparator) {\r\n        let smallest;\r\n        let n = this;\r\n        if (comparator(key, n.key) < 0) {\r\n            if (!n.left.isEmpty() && !n.left.isRed() && !n.left.left.isRed()) {\r\n                n = n.moveRedLeft();\r\n            }\r\n            n = n.copy(null, null, null, n.left.remove(key, comparator), null);\r\n        }\r\n        else {\r\n            if (n.left.isRed()) {\r\n                n = n.rotateRight();\r\n            }\r\n            if (!n.right.isEmpty() && !n.right.isRed() && !n.right.left.isRed()) {\r\n                n = n.moveRedRight();\r\n            }\r\n            if (comparator(key, n.key) === 0) {\r\n                if (n.right.isEmpty()) {\r\n                    return LLRBNode.EMPTY;\r\n                }\r\n                else {\r\n                    smallest = n.right.min();\r\n                    n = n.copy(smallest.key, smallest.value, null, null, n.right.removeMin());\r\n                }\r\n            }\r\n            n = n.copy(null, null, null, null, n.right.remove(key, comparator));\r\n        }\r\n        return n.fixUp();\r\n    }\r\n    isRed() {\r\n        return this.color;\r\n    }\r\n    // Returns new tree after performing any needed rotations.\r\n    fixUp() {\r\n        let n = this;\r\n        if (n.right.isRed() && !n.left.isRed()) {\r\n            n = n.rotateLeft();\r\n        }\r\n        if (n.left.isRed() && n.left.left.isRed()) {\r\n            n = n.rotateRight();\r\n        }\r\n        if (n.left.isRed() && n.right.isRed()) {\r\n            n = n.colorFlip();\r\n        }\r\n        return n;\r\n    }\r\n    moveRedLeft() {\r\n        let n = this.colorFlip();\r\n        if (n.right.left.isRed()) {\r\n            n = n.copy(null, null, null, null, n.right.rotateRight());\r\n            n = n.rotateLeft();\r\n            n = n.colorFlip();\r\n        }\r\n        return n;\r\n    }\r\n    moveRedRight() {\r\n        let n = this.colorFlip();\r\n        if (n.left.left.isRed()) {\r\n            n = n.rotateRight();\r\n            n = n.colorFlip();\r\n        }\r\n        return n;\r\n    }\r\n    rotateLeft() {\r\n        const nl = this.copy(null, null, LLRBNode.RED, null, this.right.left);\r\n        return this.right.copy(null, null, this.color, nl, null);\r\n    }\r\n    rotateRight() {\r\n        const nr = this.copy(null, null, LLRBNode.RED, this.left.right, null);\r\n        return this.left.copy(null, null, this.color, null, nr);\r\n    }\r\n    colorFlip() {\r\n        const left = this.left.copy(null, null, !this.left.color, null, null);\r\n        const right = this.right.copy(null, null, !this.right.color, null, null);\r\n        return this.copy(null, null, !this.color, left, right);\r\n    }\r\n    // For testing.\r\n    checkMaxDepth() {\r\n        const blackDepth = this.check();\r\n        if (Math.pow(2.0, blackDepth) <= this.size + 1) {\r\n            return true;\r\n        }\r\n        else {\r\n            return false;\r\n        }\r\n    }\r\n    // In a balanced RB tree, the black-depth (number of black nodes) from root to\r\n    // leaves is equal on both sides.  This function verifies that or asserts.\r\n    check() {\r\n        if (this.isRed() && this.left.isRed()) {\r\n            throw fail();\r\n        }\r\n        if (this.right.isRed()) {\r\n            throw fail();\r\n        }\r\n        const blackDepth = this.left.check();\r\n        if (blackDepth !== this.right.check()) {\r\n            throw fail();\r\n        }\r\n        else {\r\n            return blackDepth + (this.isRed() ? 0 : 1);\r\n        }\r\n    }\r\n} // end LLRBNode\r\n// Empty node is shared between all LLRB trees.\r\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\r\nLLRBNode.EMPTY = null;\r\nLLRBNode.RED = true;\r\nLLRBNode.BLACK = false;\r\n// Represents an empty node (a leaf node in the Red-Black Tree).\r\nclass LLRBEmptyNode {\r\n    constructor() {\r\n        this.size = 0;\r\n    }\r\n    get key() {\r\n        throw fail();\r\n    }\r\n    get value() {\r\n        throw fail();\r\n    }\r\n    get color() {\r\n        throw fail();\r\n    }\r\n    get left() {\r\n        throw fail();\r\n    }\r\n    get right() {\r\n        throw fail();\r\n    }\r\n    // Returns a copy of the current node.\r\n    copy(key, value, color, left, right) {\r\n        return this;\r\n    }\r\n    // Returns a copy of the tree, with the specified key/value added.\r\n    insert(key, value, comparator) {\r\n        return new LLRBNode(key, value);\r\n    }\r\n    // Returns a copy of the tree, with the specified key removed.\r\n    remove(key, comparator) {\r\n        return this;\r\n    }\r\n    isEmpty() {\r\n        return true;\r\n    }\r\n    inorderTraversal(action) {\r\n        return false;\r\n    }\r\n    reverseTraversal(action) {\r\n        return false;\r\n    }\r\n    minKey() {\r\n        return null;\r\n    }\r\n    maxKey() {\r\n        return null;\r\n    }\r\n    isRed() {\r\n        return false;\r\n    }\r\n    // For testing.\r\n    checkMaxDepth() {\r\n        return true;\r\n    }\r\n    check() {\r\n        return 0;\r\n    }\r\n} // end LLRBEmptyNode\r\nLLRBNode.EMPTY = new LLRBEmptyNode();\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * SortedSet is an immutable (copy-on-write) collection that holds elements\r\n * in order specified by the provided comparator.\r\n *\r\n * NOTE: if provided comparator returns 0 for two elements, we consider them to\r\n * be equal!\r\n */\r\nclass SortedSet {\r\n    constructor(comparator) {\r\n        this.comparator = comparator;\r\n        this.data = new SortedMap(this.comparator);\r\n    }\r\n    has(elem) {\r\n        return this.data.get(elem) !== null;\r\n    }\r\n    first() {\r\n        return this.data.minKey();\r\n    }\r\n    last() {\r\n        return this.data.maxKey();\r\n    }\r\n    get size() {\r\n        return this.data.size;\r\n    }\r\n    indexOf(elem) {\r\n        return this.data.indexOf(elem);\r\n    }\r\n    /** Iterates elements in order defined by \"comparator\" */\r\n    forEach(cb) {\r\n        this.data.inorderTraversal((k, v) => {\r\n            cb(k);\r\n            return false;\r\n        });\r\n    }\r\n    /** Iterates over `elem`s such that: range[0] &lt;= elem &lt; range[1]. */\r\n    forEachInRange(range, cb) {\r\n        const iter = this.data.getIteratorFrom(range[0]);\r\n        while (iter.hasNext()) {\r\n            const elem = iter.getNext();\r\n            if (this.comparator(elem.key, range[1]) >= 0) {\r\n                return;\r\n            }\r\n            cb(elem.key);\r\n        }\r\n    }\r\n    /**\r\n     * Iterates over `elem`s such that: start &lt;= elem until false is returned.\r\n     */\r\n    forEachWhile(cb, start) {\r\n        let iter;\r\n        if (start !== undefined) {\r\n            iter = this.data.getIteratorFrom(start);\r\n        }\r\n        else {\r\n            iter = this.data.getIterator();\r\n        }\r\n        while (iter.hasNext()) {\r\n            const elem = iter.getNext();\r\n            const result = cb(elem.key);\r\n            if (!result) {\r\n                return;\r\n            }\r\n        }\r\n    }\r\n    /** Finds the least element greater than or equal to `elem`. */\r\n    firstAfterOrEqual(elem) {\r\n        const iter = this.data.getIteratorFrom(elem);\r\n        return iter.hasNext() ? iter.getNext().key : null;\r\n    }\r\n    getIterator() {\r\n        return new SortedSetIterator(this.data.getIterator());\r\n    }\r\n    getIteratorFrom(key) {\r\n        return new SortedSetIterator(this.data.getIteratorFrom(key));\r\n    }\r\n    /** Inserts or updates an element */\r\n    add(elem) {\r\n        return this.copy(this.data.remove(elem).insert(elem, true));\r\n    }\r\n    /** Deletes an element */\r\n    delete(elem) {\r\n        if (!this.has(elem)) {\r\n            return this;\r\n        }\r\n        return this.copy(this.data.remove(elem));\r\n    }\r\n    isEmpty() {\r\n        return this.data.isEmpty();\r\n    }\r\n    unionWith(other) {\r\n        let result = this;\r\n        // Make sure `result` always refers to the larger one of the two sets.\r\n        if (result.size < other.size) {\r\n            result = other;\r\n            other = this;\r\n        }\r\n        other.forEach(elem => {\r\n            result = result.add(elem);\r\n        });\r\n        return result;\r\n    }\r\n    isEqual(other) {\r\n        if (!(other instanceof SortedSet)) {\r\n            return false;\r\n        }\r\n        if (this.size !== other.size) {\r\n            return false;\r\n        }\r\n        const thisIt = this.data.getIterator();\r\n        const otherIt = other.data.getIterator();\r\n        while (thisIt.hasNext()) {\r\n            const thisElem = thisIt.getNext().key;\r\n            const otherElem = otherIt.getNext().key;\r\n            if (this.comparator(thisElem, otherElem) !== 0) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    toArray() {\r\n        const res = [];\r\n        this.forEach(targetId => {\r\n            res.push(targetId);\r\n        });\r\n        return res;\r\n    }\r\n    toString() {\r\n        const result = [];\r\n        this.forEach(elem => result.push(elem));\r\n        return 'SortedSet(' + result.toString() + ')';\r\n    }\r\n    copy(data) {\r\n        const result = new SortedSet(this.comparator);\r\n        result.data = data;\r\n        return result;\r\n    }\r\n}\r\nclass SortedSetIterator {\r\n    constructor(iter) {\r\n        this.iter = iter;\r\n    }\r\n    getNext() {\r\n        return this.iter.getNext().key;\r\n    }\r\n    hasNext() {\r\n        return this.iter.hasNext();\r\n    }\r\n}\r\n/**\r\n * Compares two sorted sets for equality using their natural ordering. The\r\n * method computes the intersection and invokes `onAdd` for every element that\r\n * is in `after` but not `before`. `onRemove` is invoked for every element in\r\n * `before` but missing from `after`.\r\n *\r\n * The method creates a copy of both `before` and `after` and runs in O(n log\r\n * n), where n is the size of the two lists.\r\n *\r\n * @param before - The elements that exist in the original set.\r\n * @param after - The elements to diff against the original set.\r\n * @param comparator - The comparator for the elements in before and after.\r\n * @param onAdd - A function to invoke for every element that is part of `\r\n * after` but not `before`.\r\n * @param onRemove - A function to invoke for every element that is part of\r\n * `before` but not `after`.\r\n */\r\nfunction diffSortedSets(before, after, comparator, onAdd, onRemove) {\r\n    const beforeIt = before.getIterator();\r\n    const afterIt = after.getIterator();\r\n    let beforeValue = advanceIterator(beforeIt);\r\n    let afterValue = advanceIterator(afterIt);\r\n    // Walk through the two sets at the same time, using the ordering defined by\r\n    // `comparator`.\r\n    while (beforeValue || afterValue) {\r\n        let added = false;\r\n        let removed = false;\r\n        if (beforeValue && afterValue) {\r\n            const cmp = comparator(beforeValue, afterValue);\r\n            if (cmp < 0) {\r\n                // The element was removed if the next element in our ordered\r\n                // walkthrough is only in `before`.\r\n                removed = true;\r\n            }\r\n            else if (cmp > 0) {\r\n                // The element was added if the next element in our ordered walkthrough\r\n                // is only in `after`.\r\n                added = true;\r\n            }\r\n        }\r\n        else if (beforeValue != null) {\r\n            removed = true;\r\n        }\r\n        else {\r\n            added = true;\r\n        }\r\n        if (added) {\r\n            onAdd(afterValue);\r\n            afterValue = advanceIterator(afterIt);\r\n        }\r\n        else if (removed) {\r\n            onRemove(beforeValue);\r\n            beforeValue = advanceIterator(beforeIt);\r\n        }\r\n        else {\r\n            beforeValue = advanceIterator(beforeIt);\r\n            afterValue = advanceIterator(afterIt);\r\n        }\r\n    }\r\n}\r\n/**\r\n * Returns the next element from the iterator or `undefined` if none available.\r\n */\r\nfunction advanceIterator(it) {\r\n    return it.hasNext() ? it.getNext() : undefined;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Provides a set of fields that can be used to partially patch a document.\r\n * FieldMask is used in conjunction with ObjectValue.\r\n * Examples:\r\n *   foo - Overwrites foo entirely with the provided value. If foo is not\r\n *         present in the companion ObjectValue, the field is deleted.\r\n *   foo.bar - Overwrites only the field bar of the object foo.\r\n *             If foo is not an object, foo is replaced with an object\r\n *             containing foo\r\n */\r\nclass FieldMask {\r\n    constructor(fields) {\r\n        this.fields = fields;\r\n        // TODO(dimond): validation of FieldMask\r\n        // Sort the field mask to support `FieldMask.isEqual()` and assert below.\r\n        fields.sort(FieldPath$1.comparator);\r\n    }\r\n    static empty() {\r\n        return new FieldMask([]);\r\n    }\r\n    /**\r\n     * Returns a new FieldMask object that is the result of adding all the given\r\n     * fields paths to this field mask.\r\n     */\r\n    unionWith(extraFields) {\r\n        let mergedMaskSet = new SortedSet(FieldPath$1.comparator);\r\n        for (const fieldPath of this.fields) {\r\n            mergedMaskSet = mergedMaskSet.add(fieldPath);\r\n        }\r\n        for (const fieldPath of extraFields) {\r\n            mergedMaskSet = mergedMaskSet.add(fieldPath);\r\n        }\r\n        return new FieldMask(mergedMaskSet.toArray());\r\n    }\r\n    /**\r\n     * Verifies that `fieldPath` is included by at least one field in this field\r\n     * mask.\r\n     *\r\n     * This is an O(n) operation, where `n` is the size of the field mask.\r\n     */\r\n    covers(fieldPath) {\r\n        for (const fieldMaskPath of this.fields) {\r\n            if (fieldMaskPath.isPrefixOf(fieldPath)) {\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n    isEqual(other) {\r\n        return arrayEquals(this.fields, other.fields, (l, r) => l.isEqual(r));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Converts a Base64 encoded string to a binary string. */\r\nfunction decodeBase64(encoded) {\r\n    // Note: We used to validate the base64 string here via a regular expression.\r\n    // This was removed to improve the performance of indexing.\r\n    return Buffer.from(encoded, 'base64').toString('binary');\r\n}\r\n/** Converts a binary string to a Base64 encoded string. */\r\nfunction encodeBase64(raw) {\r\n    return Buffer.from(raw, 'binary').toString('base64');\r\n}\r\n/** True if and only if the Base64 conversion functions are available. */\r\nfunction isBase64Available() {\r\n    return true;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Immutable class that represents a \"proto\" byte string.\r\n *\r\n * Proto byte strings can either be Base64-encoded strings or Uint8Arrays when\r\n * sent on the wire. This class abstracts away this differentiation by holding\r\n * the proto byte string in a common class that must be converted into a string\r\n * before being sent as a proto.\r\n * @internal\r\n */\r\nclass ByteString {\r\n    constructor(binaryString) {\r\n        this.binaryString = binaryString;\r\n    }\r\n    static fromBase64String(base64) {\r\n        const binaryString = decodeBase64(base64);\r\n        return new ByteString(binaryString);\r\n    }\r\n    static fromUint8Array(array) {\r\n        // TODO(indexing); Remove the copy of the byte string here as this method\r\n        // is frequently called during indexing.\r\n        const binaryString = binaryStringFromUint8Array(array);\r\n        return new ByteString(binaryString);\r\n    }\r\n    [Symbol.iterator]() {\r\n        let i = 0;\r\n        return {\r\n            next: () => {\r\n                if (i < this.binaryString.length) {\r\n                    return { value: this.binaryString.charCodeAt(i++), done: false };\r\n                }\r\n                else {\r\n                    return { value: undefined, done: true };\r\n                }\r\n            }\r\n        };\r\n    }\r\n    toBase64() {\r\n        return encodeBase64(this.binaryString);\r\n    }\r\n    toUint8Array() {\r\n        return uint8ArrayFromBinaryString(this.binaryString);\r\n    }\r\n    approximateByteSize() {\r\n        return this.binaryString.length * 2;\r\n    }\r\n    compareTo(other) {\r\n        return primitiveComparator(this.binaryString, other.binaryString);\r\n    }\r\n    isEqual(other) {\r\n        return this.binaryString === other.binaryString;\r\n    }\r\n}\r\nByteString.EMPTY_BYTE_STRING = new ByteString('');\r\n/**\r\n * Helper function to convert an Uint8array to a binary string.\r\n */\r\nfunction binaryStringFromUint8Array(array) {\r\n    let binaryString = '';\r\n    for (let i = 0; i < array.length; ++i) {\r\n        binaryString += String.fromCharCode(array[i]);\r\n    }\r\n    return binaryString;\r\n}\r\n/**\r\n * Helper function to convert a binary string to an Uint8Array.\r\n */\r\nfunction uint8ArrayFromBinaryString(binaryString) {\r\n    const buffer = new Uint8Array(binaryString.length);\r\n    for (let i = 0; i < binaryString.length; i++) {\r\n        buffer[i] = binaryString.charCodeAt(i);\r\n    }\r\n    return buffer;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// A RegExp matching ISO 8601 UTC timestamps with optional fraction.\r\nconst ISO_TIMESTAMP_REG_EXP = new RegExp(/^\\d{4}-\\d\\d-\\d\\dT\\d\\d:\\d\\d:\\d\\d(?:\\.(\\d+))?Z$/);\r\n/**\r\n * Converts the possible Proto values for a timestamp value into a \"seconds and\r\n * nanos\" representation.\r\n */\r\nfunction normalizeTimestamp(date) {\r\n    hardAssert(!!date);\r\n    // The json interface (for the browser) will return an iso timestamp string,\r\n    // while the proto js library (for node) will return a\r\n    // google.protobuf.Timestamp instance.\r\n    if (typeof date === 'string') {\r\n        // The date string can have higher precision (nanos) than the Date class\r\n        // (millis), so we do some custom parsing here.\r\n        // Parse the nanos right out of the string.\r\n        let nanos = 0;\r\n        const fraction = ISO_TIMESTAMP_REG_EXP.exec(date);\r\n        hardAssert(!!fraction);\r\n        if (fraction[1]) {\r\n            // Pad the fraction out to 9 digits (nanos).\r\n            let nanoStr = fraction[1];\r\n            nanoStr = (nanoStr + '000000000').substr(0, 9);\r\n            nanos = Number(nanoStr);\r\n        }\r\n        // Parse the date to get the seconds.\r\n        const parsedDate = new Date(date);\r\n        const seconds = Math.floor(parsedDate.getTime() / 1000);\r\n        return { seconds, nanos };\r\n    }\r\n    else {\r\n        // TODO(b/37282237): Use strings for Proto3 timestamps\r\n        // assert(!this.options.useProto3Json,\r\n        //   'The timestamp instance format requires Proto JS.');\r\n        const seconds = normalizeNumber(date.seconds);\r\n        const nanos = normalizeNumber(date.nanos);\r\n        return { seconds, nanos };\r\n    }\r\n}\r\n/**\r\n * Converts the possible Proto types for numbers into a JavaScript number.\r\n * Returns 0 if the value is not numeric.\r\n */\r\nfunction normalizeNumber(value) {\r\n    // TODO(bjornick): Handle int64 greater than 53 bits.\r\n    if (typeof value === 'number') {\r\n        return value;\r\n    }\r\n    else if (typeof value === 'string') {\r\n        return Number(value);\r\n    }\r\n    else {\r\n        return 0;\r\n    }\r\n}\r\n/** Converts the possible Proto types for Blobs into a ByteString. */\r\nfunction normalizeByteString(blob) {\r\n    if (typeof blob === 'string') {\r\n        return ByteString.fromBase64String(blob);\r\n    }\r\n    else {\r\n        return ByteString.fromUint8Array(blob);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Represents a locally-applied ServerTimestamp.\r\n *\r\n * Server Timestamps are backed by MapValues that contain an internal field\r\n * `__type__` with a value of `server_timestamp`. The previous value and local\r\n * write time are stored in its `__previous_value__` and `__local_write_time__`\r\n * fields respectively.\r\n *\r\n * Notes:\r\n * - ServerTimestampValue instances are created as the result of applying a\r\n *   transform. They can only exist in the local view of a document. Therefore\r\n *   they do not need to be parsed or serialized.\r\n * - When evaluated locally (e.g. for snapshot.data()), they by default\r\n *   evaluate to `null`. This behavior can be configured by passing custom\r\n *   FieldValueOptions to value().\r\n * - With respect to other ServerTimestampValues, they sort by their\r\n *   localWriteTime.\r\n */\r\nconst SERVER_TIMESTAMP_SENTINEL = 'server_timestamp';\r\nconst TYPE_KEY = '__type__';\r\nconst PREVIOUS_VALUE_KEY = '__previous_value__';\r\nconst LOCAL_WRITE_TIME_KEY = '__local_write_time__';\r\nfunction isServerTimestamp(value) {\r\n    var _a, _b;\r\n    const type = (_b = (((_a = value === null || value === void 0 ? void 0 : value.mapValue) === null || _a === void 0 ? void 0 : _a.fields) || {})[TYPE_KEY]) === null || _b === void 0 ? void 0 : _b.stringValue;\r\n    return type === SERVER_TIMESTAMP_SENTINEL;\r\n}\r\n/**\r\n * Creates a new ServerTimestamp proto value (using the internal format).\r\n */\r\nfunction serverTimestamp$1(localWriteTime, previousValue) {\r\n    const mapValue = {\r\n        fields: {\r\n            [TYPE_KEY]: {\r\n                stringValue: SERVER_TIMESTAMP_SENTINEL\r\n            },\r\n            [LOCAL_WRITE_TIME_KEY]: {\r\n                timestampValue: {\r\n                    seconds: localWriteTime.seconds,\r\n                    nanos: localWriteTime.nanoseconds\r\n                }\r\n            }\r\n        }\r\n    };\r\n    if (previousValue) {\r\n        mapValue.fields[PREVIOUS_VALUE_KEY] = previousValue;\r\n    }\r\n    return { mapValue };\r\n}\r\n/**\r\n * Returns the value of the field before this ServerTimestamp was set.\r\n *\r\n * Preserving the previous values allows the user to display the last resoled\r\n * value until the backend responds with the timestamp.\r\n */\r\nfunction getPreviousValue(value) {\r\n    const previousValue = value.mapValue.fields[PREVIOUS_VALUE_KEY];\r\n    if (isServerTimestamp(previousValue)) {\r\n        return getPreviousValue(previousValue);\r\n    }\r\n    return previousValue;\r\n}\r\n/**\r\n * Returns the local time at which this timestamp was first set.\r\n */\r\nfunction getLocalWriteTime(value) {\r\n    const localWriteTime = normalizeTimestamp(value.mapValue.fields[LOCAL_WRITE_TIME_KEY].timestampValue);\r\n    return new Timestamp(localWriteTime.seconds, localWriteTime.nanos);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass DatabaseInfo {\r\n    /**\r\n     * Constructs a DatabaseInfo using the provided host, databaseId and\r\n     * persistenceKey.\r\n     *\r\n     * @param databaseId - The database to use.\r\n     * @param appId - The Firebase App Id.\r\n     * @param persistenceKey - A unique identifier for this Firestore's local\r\n     * storage (used in conjunction with the databaseId).\r\n     * @param host - The Firestore backend host to connect to.\r\n     * @param ssl - Whether to use SSL when connecting.\r\n     * @param forceLongPolling - Whether to use the forceLongPolling option\r\n     * when using WebChannel as the network transport.\r\n     * @param autoDetectLongPolling - Whether to use the detectBufferingProxy\r\n     * option when using WebChannel as the network transport.\r\n     * @param useFetchStreams Whether to use the Fetch API instead of\r\n     * XMLHTTPRequest\r\n     */\r\n    constructor(databaseId, appId, persistenceKey, host, ssl, forceLongPolling, autoDetectLongPolling, useFetchStreams) {\r\n        this.databaseId = databaseId;\r\n        this.appId = appId;\r\n        this.persistenceKey = persistenceKey;\r\n        this.host = host;\r\n        this.ssl = ssl;\r\n        this.forceLongPolling = forceLongPolling;\r\n        this.autoDetectLongPolling = autoDetectLongPolling;\r\n        this.useFetchStreams = useFetchStreams;\r\n    }\r\n}\r\n/** The default database name for a project. */\r\nconst DEFAULT_DATABASE_NAME = '(default)';\r\n/**\r\n * Represents the database ID a Firestore client is associated with.\r\n * @internal\r\n */\r\nclass DatabaseId {\r\n    constructor(projectId, database) {\r\n        this.projectId = projectId;\r\n        this.database = database ? database : DEFAULT_DATABASE_NAME;\r\n    }\r\n    static empty() {\r\n        return new DatabaseId('', '');\r\n    }\r\n    get isDefaultDatabase() {\r\n        return this.database === DEFAULT_DATABASE_NAME;\r\n    }\r\n    isEqual(other) {\r\n        return (other instanceof DatabaseId &&\r\n            other.projectId === this.projectId &&\r\n            other.database === this.database);\r\n    }\r\n}\r\nfunction databaseIdFromApp(app, database) {\r\n    if (!Object.prototype.hasOwnProperty.apply(app.options, ['projectId'])) {\r\n        throw new FirestoreError(Code.INVALID_ARGUMENT, '\"projectId\" not provided in firebase.initializeApp.');\r\n    }\r\n    return new DatabaseId(app.options.projectId, database);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Sentinel value that sorts before any Mutation Batch ID. */\r\nconst BATCHID_UNKNOWN = -1;\r\n/**\r\n * Returns whether a variable is either undefined or null.\r\n */\r\nfunction isNullOrUndefined(value) {\r\n    return value === null || value === undefined;\r\n}\r\n/** Returns whether the value represents -0. */\r\nfunction isNegativeZero(value) {\r\n    // Detect if the value is -0.0. Based on polyfill from\r\n    // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is\r\n    return value === 0 && 1 / value === 1 / -0;\r\n}\r\n/**\r\n * Returns whether a value is an integer and in the safe integer range\r\n * @param value - The value to test for being an integer and in the safe range\r\n */\r\nfunction isSafeInteger(value) {\r\n    return (typeof value === 'number' &&\r\n        Number.isInteger(value) &&\r\n        !isNegativeZero(value) &&\r\n        value <= Number.MAX_SAFE_INTEGER &&\r\n        value >= Number.MIN_SAFE_INTEGER);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst MAX_VALUE_TYPE = '__max__';\r\nconst MAX_VALUE = {\r\n    mapValue: {\r\n        fields: {\r\n            '__type__': { stringValue: MAX_VALUE_TYPE }\r\n        }\r\n    }\r\n};\r\nconst MIN_VALUE = {\r\n    nullValue: 'NULL_VALUE'\r\n};\r\n/** Extracts the backend's type order for the provided value. */\r\nfunction typeOrder(value) {\r\n    if ('nullValue' in value) {\r\n        return 0 /* NullValue */;\r\n    }\r\n    else if ('booleanValue' in value) {\r\n        return 1 /* BooleanValue */;\r\n    }\r\n    else if ('integerValue' in value || 'doubleValue' in value) {\r\n        return 2 /* NumberValue */;\r\n    }\r\n    else if ('timestampValue' in value) {\r\n        return 3 /* TimestampValue */;\r\n    }\r\n    else if ('stringValue' in value) {\r\n        return 5 /* StringValue */;\r\n    }\r\n    else if ('bytesValue' in value) {\r\n        return 6 /* BlobValue */;\r\n    }\r\n    else if ('referenceValue' in value) {\r\n        return 7 /* RefValue */;\r\n    }\r\n    else if ('geoPointValue' in value) {\r\n        return 8 /* GeoPointValue */;\r\n    }\r\n    else if ('arrayValue' in value) {\r\n        return 9 /* ArrayValue */;\r\n    }\r\n    else if ('mapValue' in value) {\r\n        if (isServerTimestamp(value)) {\r\n            return 4 /* ServerTimestampValue */;\r\n        }\r\n        else if (isMaxValue(value)) {\r\n            return 9007199254740991 /* MaxValue */;\r\n        }\r\n        return 10 /* ObjectValue */;\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\n/** Tests `left` and `right` for equality based on the backend semantics. */\r\nfunction valueEquals(left, right) {\r\n    if (left === right) {\r\n        return true;\r\n    }\r\n    const leftType = typeOrder(left);\r\n    const rightType = typeOrder(right);\r\n    if (leftType !== rightType) {\r\n        return false;\r\n    }\r\n    switch (leftType) {\r\n        case 0 /* NullValue */:\r\n            return true;\r\n        case 1 /* BooleanValue */:\r\n            return left.booleanValue === right.booleanValue;\r\n        case 4 /* ServerTimestampValue */:\r\n            return getLocalWriteTime(left).isEqual(getLocalWriteTime(right));\r\n        case 3 /* TimestampValue */:\r\n            return timestampEquals(left, right);\r\n        case 5 /* StringValue */:\r\n            return left.stringValue === right.stringValue;\r\n        case 6 /* BlobValue */:\r\n            return blobEquals(left, right);\r\n        case 7 /* RefValue */:\r\n            return left.referenceValue === right.referenceValue;\r\n        case 8 /* GeoPointValue */:\r\n            return geoPointEquals(left, right);\r\n        case 2 /* NumberValue */:\r\n            return numberEquals(left, right);\r\n        case 9 /* ArrayValue */:\r\n            return arrayEquals(left.arrayValue.values || [], right.arrayValue.values || [], valueEquals);\r\n        case 10 /* ObjectValue */:\r\n            return objectEquals(left, right);\r\n        case 9007199254740991 /* MaxValue */:\r\n            return true;\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\nfunction timestampEquals(left, right) {\r\n    if (typeof left.timestampValue === 'string' &&\r\n        typeof right.timestampValue === 'string' &&\r\n        left.timestampValue.length === right.timestampValue.length) {\r\n        // Use string equality for ISO 8601 timestamps\r\n        return left.timestampValue === right.timestampValue;\r\n    }\r\n    const leftTimestamp = normalizeTimestamp(left.timestampValue);\r\n    const rightTimestamp = normalizeTimestamp(right.timestampValue);\r\n    return (leftTimestamp.seconds === rightTimestamp.seconds &&\r\n        leftTimestamp.nanos === rightTimestamp.nanos);\r\n}\r\nfunction geoPointEquals(left, right) {\r\n    return (normalizeNumber(left.geoPointValue.latitude) ===\r\n        normalizeNumber(right.geoPointValue.latitude) &&\r\n        normalizeNumber(left.geoPointValue.longitude) ===\r\n            normalizeNumber(right.geoPointValue.longitude));\r\n}\r\nfunction blobEquals(left, right) {\r\n    return normalizeByteString(left.bytesValue).isEqual(normalizeByteString(right.bytesValue));\r\n}\r\nfunction numberEquals(left, right) {\r\n    if ('integerValue' in left && 'integerValue' in right) {\r\n        return (normalizeNumber(left.integerValue) === normalizeNumber(right.integerValue));\r\n    }\r\n    else if ('doubleValue' in left && 'doubleValue' in right) {\r\n        const n1 = normalizeNumber(left.doubleValue);\r\n        const n2 = normalizeNumber(right.doubleValue);\r\n        if (n1 === n2) {\r\n            return isNegativeZero(n1) === isNegativeZero(n2);\r\n        }\r\n        else {\r\n            return isNaN(n1) && isNaN(n2);\r\n        }\r\n    }\r\n    return false;\r\n}\r\nfunction objectEquals(left, right) {\r\n    const leftMap = left.mapValue.fields || {};\r\n    const rightMap = right.mapValue.fields || {};\r\n    if (objectSize(leftMap) !== objectSize(rightMap)) {\r\n        return false;\r\n    }\r\n    for (const key in leftMap) {\r\n        if (leftMap.hasOwnProperty(key)) {\r\n            if (rightMap[key] === undefined ||\r\n                !valueEquals(leftMap[key], rightMap[key])) {\r\n                return false;\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}\r\n/** Returns true if the ArrayValue contains the specified element. */\r\nfunction arrayValueContains(haystack, needle) {\r\n    return ((haystack.values || []).find(v => valueEquals(v, needle)) !== undefined);\r\n}\r\nfunction valueCompare(left, right) {\r\n    if (left === right) {\r\n        return 0;\r\n    }\r\n    const leftType = typeOrder(left);\r\n    const rightType = typeOrder(right);\r\n    if (leftType !== rightType) {\r\n        return primitiveComparator(leftType, rightType);\r\n    }\r\n    switch (leftType) {\r\n        case 0 /* NullValue */:\r\n        case 9007199254740991 /* MaxValue */:\r\n            return 0;\r\n        case 1 /* BooleanValue */:\r\n            return primitiveComparator(left.booleanValue, right.booleanValue);\r\n        case 2 /* NumberValue */:\r\n            return compareNumbers(left, right);\r\n        case 3 /* TimestampValue */:\r\n            return compareTimestamps(left.timestampValue, right.timestampValue);\r\n        case 4 /* ServerTimestampValue */:\r\n            return compareTimestamps(getLocalWriteTime(left), getLocalWriteTime(right));\r\n        case 5 /* StringValue */:\r\n            return primitiveComparator(left.stringValue, right.stringValue);\r\n        case 6 /* BlobValue */:\r\n            return compareBlobs(left.bytesValue, right.bytesValue);\r\n        case 7 /* RefValue */:\r\n            return compareReferences(left.referenceValue, right.referenceValue);\r\n        case 8 /* GeoPointValue */:\r\n            return compareGeoPoints(left.geoPointValue, right.geoPointValue);\r\n        case 9 /* ArrayValue */:\r\n            return compareArrays(left.arrayValue, right.arrayValue);\r\n        case 10 /* ObjectValue */:\r\n            return compareMaps(left.mapValue, right.mapValue);\r\n        default:\r\n            throw fail();\r\n    }\r\n}\r\nfunction compareNumbers(left, right) {\r\n    const leftNumber = normalizeNumber(left.integerValue || left.doubleValue);\r\n    const rightNumber = normalizeNumber(right.integerValue || right.doubleValue);\r\n    if (leftNumber < rightNumber) {\r\n        return -1;\r\n    }\r\n    else if (leftNumber > rightNumber) {\r\n        return 1;\r\n    }\r\n    else if (leftNumber === rightNumber) {\r\n        return 0;\r\n    }\r\n    else {\r\n        // one or both are NaN.\r\n        if (isNaN(leftNumber)) {\r\n            return isNaN(rightNumber) ? 0 : -1;\r\n        }\r\n        else {\r\n            return 1;\r\n        }\r\n    }\r\n}\r\nfunction compareTimestamps(left, right) {\r\n    if (typeof left === 'string' &&\r\n        typeof right === 'string' &&\r\n        left.length === right.length) {\r\n        return primitiveComparator(left, right);\r\n    }\r\n    const leftTimestamp = normalizeTimestamp(left);\r\n    const rightTimestamp = normalizeTimestamp(right);\r\n    const comparison = primitiveComparator(leftTimestamp.seconds, rightTimestamp.seconds);\r\n    if (comparison !== 0) {\r\n        return comparison;\r\n    }\r\n    return primitiveComparator(leftTimestamp.nanos, rightTimestamp.nanos);\r\n}\r\nfunction compareReferences(leftPath, rightPath) {\r\n    const leftSegments = leftPath.split('/');\r\n    const rightSegments = rightPath.split('/');\r\n    for (let i = 0; i < leftSegments.length && i < rightSegments.length; i++) {\r\n        const comparison = primitiveComparator(leftSegments[i], rightSegments[i]);\r\n        if (comparison !== 0) {\r\n            return comparison;\r\n        }\r\n    }\r\n    return primitiveComparator(leftSegments.length, rightSegments.length);\r\n}\r\nfunction compareGeoPoints(left, right) {\r\n    const comparison = primitiveComparator(normalizeNumber(left.latitude), normalizeNumber(right.latitude));\r\n    if (comparison !== 0) {\r\n        return comparison;\r\n    }\r\n    return primitiveComparator(normalizeNumber(left.longitude), normalizeNumber(right.longitude));\r\n}\r\nfunction compareBlobs(left, right) {\r\n    const leftBytes = normalizeByteString(left);\r\n    const rightBytes = normalizeByteString(right);\r\n    return leftBytes.compareTo(rightBytes);\r\n}\r\nfunction compareArrays(left, right) {\r\n    const leftArray = left.values || [];\r\n    const rightArray = right.values || [];\r\n    for (let i = 0; i < leftArray.length && i < rightArray.length; ++i) {\r\n        const compare = valueCompare(leftArray[i], rightArray[i]);\r\n        if (compare) {\r\n            return compare;\r\n        }\r\n    }\r\n    return primitiveComparator(leftArray.length, rightArray.length);\r\n}\r\nfunction compareMaps(left, right) {\r\n    if (left === MAX_VALUE.mapValue && right === MAX_VALUE.mapValue) {\r\n        return 0;\r\n    }\r\n    else if (left === MAX_VALUE.mapValue) {\r\n        return 1;\r\n    }\r\n    else if (right === MAX_VALUE.mapValue) {\r\n        return -1;\r\n    }\r\n    const leftMap = left.fields || {};\r\n    const leftKeys = Object.keys(leftMap);\r\n    const rightMap = right.fields || {};\r\n    const rightKeys = Object.keys(rightMap);\r\n    // Even though MapValues are likely sorted correctly based on their insertion\r\n    // order (e.g. when received from the backend), local modifications can bring\r\n    // elements out of order. We need to re-sort the elements to ensure that\r\n    // canonical IDs are independent of insertion order.\r\n    leftKeys.sort();\r\n    rightKeys.sort();\r\n    for (let i = 0; i < leftKeys.length && i < rightKeys.length; ++i) {\r\n        const keyCompare = primitiveComparator(leftKeys[i], rightKeys[i]);\r\n        if (keyCompare !== 0) {\r\n            return keyCompare;\r\n        }\r\n        const compare = valueCompare(leftMap[leftKeys[i]], rightMap[rightKeys[i]]);\r\n        if (compare !== 0) {\r\n            return compare;\r\n        }\r\n    }\r\n    return primitiveComparator(leftKeys.length, rightKeys.length);\r\n}\r\n/**\r\n * Generates the canonical ID for the provided field value (as used in Target\r\n * serialization).\r\n */\r\nfunction canonicalId(value) {\r\n    return canonifyValue(value);\r\n}\r\nfunction canonifyValue(value) {\r\n    if ('nullValue' in value) {\r\n        return 'null';\r\n    }\r\n    else if ('booleanValue' in value) {\r\n        return '' + value.booleanValue;\r\n    }\r\n    else if ('integerValue' in value) {\r\n        return '' + value.integerValue;\r\n    }\r\n    else if ('doubleValue' in value) {\r\n        return '' + value.doubleValue;\r\n    }\r\n    else if ('timestampValue' in value) {\r\n        return canonifyTimestamp(value.timestampValue);\r\n    }\r\n    else if ('stringValue' in value) {\r\n        return value.stringValue;\r\n    }\r\n    else if ('bytesValue' in value) {\r\n        return canonifyByteString(value.bytesValue);\r\n    }\r\n    else if ('referenceValue' in value) {\r\n        return canonifyReference(value.referenceValue);\r\n    }\r\n    else if ('geoPointValue' in value) {\r\n        return canonifyGeoPoint(value.geoPointValue);\r\n    }\r\n    else if ('arrayValue' in value) {\r\n        return canonifyArray(value.arrayValue);\r\n    }\r\n    else if ('mapValue' in value) {\r\n        return canonifyMap(value.mapValue);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction canonifyByteString(byteString) {\r\n    return normalizeByteString(byteString).toBase64();\r\n}\r\nfunction canonifyTimestamp(timestamp) {\r\n    const normalizedTimestamp = normalizeTimestamp(timestamp);\r\n    return `time(${normalizedTimestamp.seconds},${normalizedTimestamp.nanos})`;\r\n}\r\nfunction canonifyGeoPoint(geoPoint) {\r\n    return `geo(${geoPoint.latitude},${geoPoint.longitude})`;\r\n}\r\nfunction canonifyReference(referenceValue) {\r\n    return DocumentKey.fromName(referenceValue).toString();\r\n}\r\nfunction canonifyMap(mapValue) {\r\n    // Iteration order in JavaScript is not guaranteed. To ensure that we generate\r\n    // matching canonical IDs for identical maps, we need to sort the keys.\r\n    const sortedKeys = Object.keys(mapValue.fields || {}).sort();\r\n    let result = '{';\r\n    let first = true;\r\n    for (const key of sortedKeys) {\r\n        if (!first) {\r\n            result += ',';\r\n        }\r\n        else {\r\n            first = false;\r\n        }\r\n        result += `${key}:${canonifyValue(mapValue.fields[key])}`;\r\n    }\r\n    return result + '}';\r\n}\r\nfunction canonifyArray(arrayValue) {\r\n    let result = '[';\r\n    let first = true;\r\n    for (const value of arrayValue.values || []) {\r\n        if (!first) {\r\n            result += ',';\r\n        }\r\n        else {\r\n            first = false;\r\n        }\r\n        result += canonifyValue(value);\r\n    }\r\n    return result + ']';\r\n}\r\n/** Returns a reference value for the provided database and key. */\r\nfunction refValue(databaseId, key) {\r\n    return {\r\n        referenceValue: `projects/${databaseId.projectId}/databases/${databaseId.database}/documents/${key.path.canonicalString()}`\r\n    };\r\n}\r\n/** Returns true if `value` is an IntegerValue . */\r\nfunction isInteger(value) {\r\n    return !!value && 'integerValue' in value;\r\n}\r\n/** Returns true if `value` is a DoubleValue. */\r\nfunction isDouble(value) {\r\n    return !!value && 'doubleValue' in value;\r\n}\r\n/** Returns true if `value` is either an IntegerValue or a DoubleValue. */\r\nfunction isNumber(value) {\r\n    return isInteger(value) || isDouble(value);\r\n}\r\n/** Returns true if `value` is an ArrayValue. */\r\nfunction isArray(value) {\r\n    return !!value && 'arrayValue' in value;\r\n}\r\n/** Returns true if `value` is a NullValue. */\r\nfunction isNullValue(value) {\r\n    return !!value && 'nullValue' in value;\r\n}\r\n/** Returns true if `value` is NaN. */\r\nfunction isNanValue(value) {\r\n    return !!value && 'doubleValue' in value && isNaN(Number(value.doubleValue));\r\n}\r\n/** Returns true if `value` is a MapValue. */\r\nfunction isMapValue(value) {\r\n    return !!value && 'mapValue' in value;\r\n}\r\n/** Creates a deep copy of `source`. */\r\nfunction deepClone(source) {\r\n    if (source.geoPointValue) {\r\n        return { geoPointValue: Object.assign({}, source.geoPointValue) };\r\n    }\r\n    else if (source.timestampValue &&\r\n        typeof source.timestampValue === 'object') {\r\n        return { timestampValue: Object.assign({}, source.timestampValue) };\r\n    }\r\n    else if (source.mapValue) {\r\n        const target = { mapValue: { fields: {} } };\r\n        forEach(source.mapValue.fields, (key, val) => (target.mapValue.fields[key] = deepClone(val)));\r\n        return target;\r\n    }\r\n    else if (source.arrayValue) {\r\n        const target = { arrayValue: { values: [] } };\r\n        for (let i = 0; i < (source.arrayValue.values || []).length; ++i) {\r\n            target.arrayValue.values[i] = deepClone(source.arrayValue.values[i]);\r\n        }\r\n        return target;\r\n    }\r\n    else {\r\n        return Object.assign({}, source);\r\n    }\r\n}\r\n/** Returns true if the Value represents the canonical {@link #MAX_VALUE} . */\r\nfunction isMaxValue(value) {\r\n    return ((((value.mapValue || {}).fields || {})['__type__'] || {}).stringValue ===\r\n        MAX_VALUE_TYPE);\r\n}\r\n/** Returns the lowest value for the given value type (inclusive). */\r\nfunction valuesGetLowerBound(value) {\r\n    if ('nullValue' in value) {\r\n        return MIN_VALUE;\r\n    }\r\n    else if ('booleanValue' in value) {\r\n        return { booleanValue: false };\r\n    }\r\n    else if ('integerValue' in value || 'doubleValue' in value) {\r\n        return { doubleValue: NaN };\r\n    }\r\n    else if ('timestampValue' in value) {\r\n        return { timestampValue: { seconds: Number.MIN_SAFE_INTEGER } };\r\n    }\r\n    else if ('stringValue' in value) {\r\n        return { stringValue: '' };\r\n    }\r\n    else if ('bytesValue' in value) {\r\n        return { bytesValue: '' };\r\n    }\r\n    else if ('referenceValue' in value) {\r\n        return refValue(DatabaseId.empty(), DocumentKey.empty());\r\n    }\r\n    else if ('geoPointValue' in value) {\r\n        return { geoPointValue: { latitude: -90, longitude: -180 } };\r\n    }\r\n    else if ('arrayValue' in value) {\r\n        return { arrayValue: {} };\r\n    }\r\n    else if ('mapValue' in value) {\r\n        return { mapValue: {} };\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\n/** Returns the largest value for the given value type (exclusive). */\r\nfunction valuesGetUpperBound(value) {\r\n    if ('nullValue' in value) {\r\n        return { booleanValue: false };\r\n    }\r\n    else if ('booleanValue' in value) {\r\n        return { doubleValue: NaN };\r\n    }\r\n    else if ('integerValue' in value || 'doubleValue' in value) {\r\n        return { timestampValue: { seconds: Number.MIN_SAFE_INTEGER } };\r\n    }\r\n    else if ('timestampValue' in value) {\r\n        return { stringValue: '' };\r\n    }\r\n    else if ('stringValue' in value) {\r\n        return { bytesValue: '' };\r\n    }\r\n    else if ('bytesValue' in value) {\r\n        return refValue(DatabaseId.empty(), DocumentKey.empty());\r\n    }\r\n    else if ('referenceValue' in value) {\r\n        return { geoPointValue: { latitude: -90, longitude: -180 } };\r\n    }\r\n    else if ('geoPointValue' in value) {\r\n        return { arrayValue: {} };\r\n    }\r\n    else if ('arrayValue' in value) {\r\n        return { mapValue: {} };\r\n    }\r\n    else if ('mapValue' in value) {\r\n        return MAX_VALUE;\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction lowerBoundCompare(left, right) {\r\n    const cmp = valueCompare(left.value, right.value);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    if (left.inclusive && !right.inclusive) {\r\n        return -1;\r\n    }\r\n    else if (!left.inclusive && right.inclusive) {\r\n        return 1;\r\n    }\r\n    return 0;\r\n}\r\nfunction upperBoundCompare(left, right) {\r\n    const cmp = valueCompare(left.value, right.value);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    if (left.inclusive && !right.inclusive) {\r\n        return 1;\r\n    }\r\n    else if (!left.inclusive && right.inclusive) {\r\n        return -1;\r\n    }\r\n    return 0;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An ObjectValue represents a MapValue in the Firestore Proto and offers the\r\n * ability to add and remove fields (via the ObjectValueBuilder).\r\n */\r\nclass ObjectValue {\r\n    constructor(value) {\r\n        this.value = value;\r\n    }\r\n    static empty() {\r\n        return new ObjectValue({ mapValue: {} });\r\n    }\r\n    /**\r\n     * Returns the value at the given path or null.\r\n     *\r\n     * @param path - the path to search\r\n     * @returns The value at the path or null if the path is not set.\r\n     */\r\n    field(path) {\r\n        if (path.isEmpty()) {\r\n            return this.value;\r\n        }\r\n        else {\r\n            let currentLevel = this.value;\r\n            for (let i = 0; i < path.length - 1; ++i) {\r\n                currentLevel = (currentLevel.mapValue.fields || {})[path.get(i)];\r\n                if (!isMapValue(currentLevel)) {\r\n                    return null;\r\n                }\r\n            }\r\n            currentLevel = (currentLevel.mapValue.fields || {})[path.lastSegment()];\r\n            return currentLevel || null;\r\n        }\r\n    }\r\n    /**\r\n     * Sets the field to the provided value.\r\n     *\r\n     * @param path - The field path to set.\r\n     * @param value - The value to set.\r\n     */\r\n    set(path, value) {\r\n        const fieldsMap = this.getFieldsMap(path.popLast());\r\n        fieldsMap[path.lastSegment()] = deepClone(value);\r\n    }\r\n    /**\r\n     * Sets the provided fields to the provided values.\r\n     *\r\n     * @param data - A map of fields to values (or null for deletes).\r\n     */\r\n    setAll(data) {\r\n        let parent = FieldPath$1.emptyPath();\r\n        let upserts = {};\r\n        let deletes = [];\r\n        data.forEach((value, path) => {\r\n            if (!parent.isImmediateParentOf(path)) {\r\n                // Insert the accumulated changes at this parent location\r\n                const fieldsMap = this.getFieldsMap(parent);\r\n                this.applyChanges(fieldsMap, upserts, deletes);\r\n                upserts = {};\r\n                deletes = [];\r\n                parent = path.popLast();\r\n            }\r\n            if (value) {\r\n                upserts[path.lastSegment()] = deepClone(value);\r\n            }\r\n            else {\r\n                deletes.push(path.lastSegment());\r\n            }\r\n        });\r\n        const fieldsMap = this.getFieldsMap(parent);\r\n        this.applyChanges(fieldsMap, upserts, deletes);\r\n    }\r\n    /**\r\n     * Removes the field at the specified path. If there is no field at the\r\n     * specified path, nothing is changed.\r\n     *\r\n     * @param path - The field path to remove.\r\n     */\r\n    delete(path) {\r\n        const nestedValue = this.field(path.popLast());\r\n        if (isMapValue(nestedValue) && nestedValue.mapValue.fields) {\r\n            delete nestedValue.mapValue.fields[path.lastSegment()];\r\n        }\r\n    }\r\n    isEqual(other) {\r\n        return valueEquals(this.value, other.value);\r\n    }\r\n    /**\r\n     * Returns the map that contains the leaf element of `path`. If the parent\r\n     * entry does not yet exist, or if it is not a map, a new map will be created.\r\n     */\r\n    getFieldsMap(path) {\r\n        let current = this.value;\r\n        if (!current.mapValue.fields) {\r\n            current.mapValue = { fields: {} };\r\n        }\r\n        for (let i = 0; i < path.length; ++i) {\r\n            let next = current.mapValue.fields[path.get(i)];\r\n            if (!isMapValue(next) || !next.mapValue.fields) {\r\n                next = { mapValue: { fields: {} } };\r\n                current.mapValue.fields[path.get(i)] = next;\r\n            }\r\n            current = next;\r\n        }\r\n        return current.mapValue.fields;\r\n    }\r\n    /**\r\n     * Modifies `fieldsMap` by adding, replacing or deleting the specified\r\n     * entries.\r\n     */\r\n    applyChanges(fieldsMap, inserts, deletes) {\r\n        forEach(inserts, (key, val) => (fieldsMap[key] = val));\r\n        for (const field of deletes) {\r\n            delete fieldsMap[field];\r\n        }\r\n    }\r\n    clone() {\r\n        return new ObjectValue(deepClone(this.value));\r\n    }\r\n}\r\n/**\r\n * Returns a FieldMask built from all fields in a MapValue.\r\n */\r\nfunction extractFieldMask(value) {\r\n    const fields = [];\r\n    forEach(value.fields, (key, value) => {\r\n        const currentPath = new FieldPath$1([key]);\r\n        if (isMapValue(value)) {\r\n            const nestedMask = extractFieldMask(value.mapValue);\r\n            const nestedFields = nestedMask.fields;\r\n            if (nestedFields.length === 0) {\r\n                // Preserve the empty map by adding it to the FieldMask.\r\n                fields.push(currentPath);\r\n            }\r\n            else {\r\n                // For nested and non-empty ObjectValues, add the FieldPath of the\r\n                // leaf nodes.\r\n                for (const nestedPath of nestedFields) {\r\n                    fields.push(currentPath.child(nestedPath));\r\n                }\r\n            }\r\n        }\r\n        else {\r\n            // For nested and non-empty ObjectValues, add the FieldPath of the leaf\r\n            // nodes.\r\n            fields.push(currentPath);\r\n        }\r\n    });\r\n    return new FieldMask(fields);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Represents a document in Firestore with a key, version, data and whether it\r\n * has local mutations applied to it.\r\n *\r\n * Documents can transition between states via `convertToFoundDocument()`,\r\n * `convertToNoDocument()` and `convertToUnknownDocument()`. If a document does\r\n * not transition to one of these states even after all mutations have been\r\n * applied, `isValidDocument()` returns false and the document should be removed\r\n * from all views.\r\n */\r\nclass MutableDocument {\r\n    constructor(key, documentType, version, readTime, data, documentState) {\r\n        this.key = key;\r\n        this.documentType = documentType;\r\n        this.version = version;\r\n        this.readTime = readTime;\r\n        this.data = data;\r\n        this.documentState = documentState;\r\n    }\r\n    /**\r\n     * Creates a document with no known version or data, but which can serve as\r\n     * base document for mutations.\r\n     */\r\n    static newInvalidDocument(documentKey) {\r\n        return new MutableDocument(documentKey, 0 /* INVALID */, SnapshotVersion.min(), SnapshotVersion.min(), ObjectValue.empty(), 0 /* SYNCED */);\r\n    }\r\n    /**\r\n     * Creates a new document that is known to exist with the given data at the\r\n     * given version.\r\n     */\r\n    static newFoundDocument(documentKey, version, value) {\r\n        return new MutableDocument(documentKey, 1 /* FOUND_DOCUMENT */, version, SnapshotVersion.min(), value, 0 /* SYNCED */);\r\n    }\r\n    /** Creates a new document that is known to not exist at the given version. */\r\n    static newNoDocument(documentKey, version) {\r\n        return new MutableDocument(documentKey, 2 /* NO_DOCUMENT */, version, SnapshotVersion.min(), ObjectValue.empty(), 0 /* SYNCED */);\r\n    }\r\n    /**\r\n     * Creates a new document that is known to exist at the given version but\r\n     * whose data is not known (e.g. a document that was updated without a known\r\n     * base document).\r\n     */\r\n    static newUnknownDocument(documentKey, version) {\r\n        return new MutableDocument(documentKey, 3 /* UNKNOWN_DOCUMENT */, version, SnapshotVersion.min(), ObjectValue.empty(), 2 /* HAS_COMMITTED_MUTATIONS */);\r\n    }\r\n    /**\r\n     * Changes the document type to indicate that it exists and that its version\r\n     * and data are known.\r\n     */\r\n    convertToFoundDocument(version, value) {\r\n        this.version = version;\r\n        this.documentType = 1 /* FOUND_DOCUMENT */;\r\n        this.data = value;\r\n        this.documentState = 0 /* SYNCED */;\r\n        return this;\r\n    }\r\n    /**\r\n     * Changes the document type to indicate that it doesn't exist at the given\r\n     * version.\r\n     */\r\n    convertToNoDocument(version) {\r\n        this.version = version;\r\n        this.documentType = 2 /* NO_DOCUMENT */;\r\n        this.data = ObjectValue.empty();\r\n        this.documentState = 0 /* SYNCED */;\r\n        return this;\r\n    }\r\n    /**\r\n     * Changes the document type to indicate that it exists at a given version but\r\n     * that its data is not known (e.g. a document that was updated without a known\r\n     * base document).\r\n     */\r\n    convertToUnknownDocument(version) {\r\n        this.version = version;\r\n        this.documentType = 3 /* UNKNOWN_DOCUMENT */;\r\n        this.data = ObjectValue.empty();\r\n        this.documentState = 2 /* HAS_COMMITTED_MUTATIONS */;\r\n        return this;\r\n    }\r\n    setHasCommittedMutations() {\r\n        this.documentState = 2 /* HAS_COMMITTED_MUTATIONS */;\r\n        return this;\r\n    }\r\n    setHasLocalMutations() {\r\n        this.documentState = 1 /* HAS_LOCAL_MUTATIONS */;\r\n        this.version = SnapshotVersion.min();\r\n        return this;\r\n    }\r\n    setReadTime(readTime) {\r\n        this.readTime = readTime;\r\n        return this;\r\n    }\r\n    get hasLocalMutations() {\r\n        return this.documentState === 1 /* HAS_LOCAL_MUTATIONS */;\r\n    }\r\n    get hasCommittedMutations() {\r\n        return this.documentState === 2 /* HAS_COMMITTED_MUTATIONS */;\r\n    }\r\n    get hasPendingWrites() {\r\n        return this.hasLocalMutations || this.hasCommittedMutations;\r\n    }\r\n    isValidDocument() {\r\n        return this.documentType !== 0 /* INVALID */;\r\n    }\r\n    isFoundDocument() {\r\n        return this.documentType === 1 /* FOUND_DOCUMENT */;\r\n    }\r\n    isNoDocument() {\r\n        return this.documentType === 2 /* NO_DOCUMENT */;\r\n    }\r\n    isUnknownDocument() {\r\n        return this.documentType === 3 /* UNKNOWN_DOCUMENT */;\r\n    }\r\n    isEqual(other) {\r\n        return (other instanceof MutableDocument &&\r\n            this.key.isEqual(other.key) &&\r\n            this.version.isEqual(other.version) &&\r\n            this.documentType === other.documentType &&\r\n            this.documentState === other.documentState &&\r\n            this.data.isEqual(other.data));\r\n    }\r\n    mutableCopy() {\r\n        return new MutableDocument(this.key, this.documentType, this.version, this.readTime, this.data.clone(), this.documentState);\r\n    }\r\n    toString() {\r\n        return (`Document(${this.key}, ${this.version}, ${JSON.stringify(this.data.value)}, ` +\r\n            `{documentType: ${this.documentType}}), ` +\r\n            `{documentState: ${this.documentState}})`);\r\n    }\r\n}\r\n/**\r\n * Compares the value for field `field` in the provided documents. Throws if\r\n * the field does not exist in both documents.\r\n */\r\nfunction compareDocumentsByField(field, d1, d2) {\r\n    const v1 = d1.data.field(field);\r\n    const v2 = d2.data.field(field);\r\n    if (v1 !== null && v2 !== null) {\r\n        return valueCompare(v1, v2);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// Visible for testing\r\nclass TargetImpl {\r\n    constructor(path, collectionGroup = null, orderBy = [], filters = [], limit = null, startAt = null, endAt = null) {\r\n        this.path = path;\r\n        this.collectionGroup = collectionGroup;\r\n        this.orderBy = orderBy;\r\n        this.filters = filters;\r\n        this.limit = limit;\r\n        this.startAt = startAt;\r\n        this.endAt = endAt;\r\n        this.memoizedCanonicalId = null;\r\n    }\r\n}\r\n/**\r\n * Initializes a Target with a path and optional additional query constraints.\r\n * Path must currently be empty if this is a collection group query.\r\n *\r\n * NOTE: you should always construct `Target` from `Query.toTarget` instead of\r\n * using this factory method, because `Query` provides an implicit `orderBy`\r\n * property.\r\n */\r\nfunction newTarget(path, collectionGroup = null, orderBy = [], filters = [], limit = null, startAt = null, endAt = null) {\r\n    return new TargetImpl(path, collectionGroup, orderBy, filters, limit, startAt, endAt);\r\n}\r\nfunction canonifyTarget(target) {\r\n    const targetImpl = debugCast(target);\r\n    if (targetImpl.memoizedCanonicalId === null) {\r\n        let str = targetImpl.path.canonicalString();\r\n        if (targetImpl.collectionGroup !== null) {\r\n            str += '|cg:' + targetImpl.collectionGroup;\r\n        }\r\n        str += '|f:';\r\n        str += targetImpl.filters.map(f => canonifyFilter(f)).join(',');\r\n        str += '|ob:';\r\n        str += targetImpl.orderBy.map(o => canonifyOrderBy(o)).join(',');\r\n        if (!isNullOrUndefined(targetImpl.limit)) {\r\n            str += '|l:';\r\n            str += targetImpl.limit;\r\n        }\r\n        if (targetImpl.startAt) {\r\n            str += '|lb:';\r\n            str += targetImpl.startAt.inclusive ? 'b:' : 'a:';\r\n            str += targetImpl.startAt.position.map(p => canonicalId(p)).join(',');\r\n        }\r\n        if (targetImpl.endAt) {\r\n            str += '|ub:';\r\n            str += targetImpl.endAt.inclusive ? 'a:' : 'b:';\r\n            str += targetImpl.endAt.position.map(p => canonicalId(p)).join(',');\r\n        }\r\n        targetImpl.memoizedCanonicalId = str;\r\n    }\r\n    return targetImpl.memoizedCanonicalId;\r\n}\r\nfunction stringifyTarget(target) {\r\n    let str = target.path.canonicalString();\r\n    if (target.collectionGroup !== null) {\r\n        str += ' collectionGroup=' + target.collectionGroup;\r\n    }\r\n    if (target.filters.length > 0) {\r\n        str += `, filters: [${target.filters\r\n            .map(f => stringifyFilter(f))\r\n            .join(', ')}]`;\r\n    }\r\n    if (!isNullOrUndefined(target.limit)) {\r\n        str += ', limit: ' + target.limit;\r\n    }\r\n    if (target.orderBy.length > 0) {\r\n        str += `, orderBy: [${target.orderBy\r\n            .map(o => stringifyOrderBy(o))\r\n            .join(', ')}]`;\r\n    }\r\n    if (target.startAt) {\r\n        str += ', startAt: ';\r\n        str += target.startAt.inclusive ? 'b:' : 'a:';\r\n        str += target.startAt.position.map(p => canonicalId(p)).join(',');\r\n    }\r\n    if (target.endAt) {\r\n        str += ', endAt: ';\r\n        str += target.endAt.inclusive ? 'a:' : 'b:';\r\n        str += target.endAt.position.map(p => canonicalId(p)).join(',');\r\n    }\r\n    return `Target(${str})`;\r\n}\r\nfunction targetEquals(left, right) {\r\n    if (left.limit !== right.limit) {\r\n        return false;\r\n    }\r\n    if (left.orderBy.length !== right.orderBy.length) {\r\n        return false;\r\n    }\r\n    for (let i = 0; i < left.orderBy.length; i++) {\r\n        if (!orderByEquals(left.orderBy[i], right.orderBy[i])) {\r\n            return false;\r\n        }\r\n    }\r\n    if (left.filters.length !== right.filters.length) {\r\n        return false;\r\n    }\r\n    for (let i = 0; i < left.filters.length; i++) {\r\n        if (!filterEquals(left.filters[i], right.filters[i])) {\r\n            return false;\r\n        }\r\n    }\r\n    if (left.collectionGroup !== right.collectionGroup) {\r\n        return false;\r\n    }\r\n    if (!left.path.isEqual(right.path)) {\r\n        return false;\r\n    }\r\n    if (!boundEquals(left.startAt, right.startAt)) {\r\n        return false;\r\n    }\r\n    return boundEquals(left.endAt, right.endAt);\r\n}\r\nfunction targetIsDocumentTarget(target) {\r\n    return (DocumentKey.isDocumentKey(target.path) &&\r\n        target.collectionGroup === null &&\r\n        target.filters.length === 0);\r\n}\r\n/** Returns the field filters that target the given field path. */\r\nfunction targetGetFieldFiltersForPath(target, path) {\r\n    return target.filters.filter(f => f instanceof FieldFilter && f.field.isEqual(path));\r\n}\r\n/**\r\n * Returns the values that are used in ARRAY_CONTAINS or ARRAY_CONTAINS_ANY\r\n * filters. Returns `null` if there are no such filters.\r\n */\r\nfunction targetGetArrayValues(target, fieldIndex) {\r\n    const segment = fieldIndexGetArraySegment(fieldIndex);\r\n    if (segment === undefined) {\r\n        return null;\r\n    }\r\n    for (const fieldFilter of targetGetFieldFiltersForPath(target, segment.fieldPath)) {\r\n        switch (fieldFilter.op) {\r\n            case \"array-contains-any\" /* ARRAY_CONTAINS_ANY */:\r\n                return fieldFilter.value.arrayValue.values || [];\r\n            case \"array-contains\" /* ARRAY_CONTAINS */:\r\n                return [fieldFilter.value];\r\n            // Remaining filters are not array filters.\r\n        }\r\n    }\r\n    return null;\r\n}\r\n/**\r\n * Returns the list of values that are used in != or NOT_IN filters. Returns\r\n * `null` if there are no such filters.\r\n */\r\nfunction targetGetNotInValues(target, fieldIndex) {\r\n    const values = new Map();\r\n    for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n        for (const fieldFilter of targetGetFieldFiltersForPath(target, segment.fieldPath)) {\r\n            switch (fieldFilter.op) {\r\n                case \"==\" /* EQUAL */:\r\n                case \"in\" /* IN */:\r\n                    // Encode equality prefix, which is encoded in the index value before\r\n                    // the inequality (e.g. `a == 'a' && b != 'b'` is encoded to\r\n                    // `value != 'ab'`).\r\n                    values.set(segment.fieldPath.canonicalString(), fieldFilter.value);\r\n                    break;\r\n                case \"not-in\" /* NOT_IN */:\r\n                case \"!=\" /* NOT_EQUAL */:\r\n                    // NotIn/NotEqual is always a suffix. There cannot be any remaining\r\n                    // segments and hence we can return early here.\r\n                    values.set(segment.fieldPath.canonicalString(), fieldFilter.value);\r\n                    return Array.from(values.values());\r\n                // Remaining filters cannot be used as notIn bounds.\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}\r\n/**\r\n * Returns a lower bound of field values that can be used as a starting point to\r\n * scan the index defined by `fieldIndex`. Returns `MIN_VALUE` if no lower bound\r\n * exists.\r\n */\r\nfunction targetGetLowerBound(target, fieldIndex) {\r\n    const values = [];\r\n    let inclusive = true;\r\n    // For each segment, retrieve a lower bound if there is a suitable filter or\r\n    // startAt.\r\n    for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n        const segmentBound = segment.kind === 0 /* ASCENDING */\r\n            ? targetGetAscendingBound(target, segment.fieldPath, target.startAt)\r\n            : targetGetDescendingBound(target, segment.fieldPath, target.startAt);\r\n        values.push(segmentBound.value);\r\n        inclusive && (inclusive = segmentBound.inclusive);\r\n    }\r\n    return new Bound(values, inclusive);\r\n}\r\n/**\r\n * Returns an upper bound of field values that can be used as an ending point\r\n * when scanning the index defined by `fieldIndex`. Returns `MAX_VALUE` if no\r\n * upper bound exists.\r\n */\r\nfunction targetGetUpperBound(target, fieldIndex) {\r\n    const values = [];\r\n    let inclusive = true;\r\n    // For each segment, retrieve an upper bound if there is a suitable filter or\r\n    // endAt.\r\n    for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n        const segmentBound = segment.kind === 0 /* ASCENDING */\r\n            ? targetGetDescendingBound(target, segment.fieldPath, target.endAt)\r\n            : targetGetAscendingBound(target, segment.fieldPath, target.endAt);\r\n        values.push(segmentBound.value);\r\n        inclusive && (inclusive = segmentBound.inclusive);\r\n    }\r\n    return new Bound(values, inclusive);\r\n}\r\n/**\r\n * Returns the value to use as the lower bound for ascending index segment at\r\n * the provided `fieldPath` (or the upper bound for an descending segment).\r\n */\r\nfunction targetGetAscendingBound(target, fieldPath, bound) {\r\n    let value = MIN_VALUE;\r\n    let inclusive = true;\r\n    // Process all filters to find a value for the current field segment\r\n    for (const fieldFilter of targetGetFieldFiltersForPath(target, fieldPath)) {\r\n        let filterValue = MIN_VALUE;\r\n        let filterInclusive = true;\r\n        switch (fieldFilter.op) {\r\n            case \"<\" /* LESS_THAN */:\r\n            case \"<=\" /* LESS_THAN_OR_EQUAL */:\r\n                filterValue = valuesGetLowerBound(fieldFilter.value);\r\n                break;\r\n            case \"==\" /* EQUAL */:\r\n            case \"in\" /* IN */:\r\n            case \">=\" /* GREATER_THAN_OR_EQUAL */:\r\n                filterValue = fieldFilter.value;\r\n                break;\r\n            case \">\" /* GREATER_THAN */:\r\n                filterValue = fieldFilter.value;\r\n                filterInclusive = false;\r\n                break;\r\n            case \"!=\" /* NOT_EQUAL */:\r\n            case \"not-in\" /* NOT_IN */:\r\n                filterValue = MIN_VALUE;\r\n                break;\r\n            // Remaining filters cannot be used as lower bounds.\r\n        }\r\n        if (lowerBoundCompare({ value, inclusive }, { value: filterValue, inclusive: filterInclusive }) < 0) {\r\n            value = filterValue;\r\n            inclusive = filterInclusive;\r\n        }\r\n    }\r\n    // If there is an additional bound, compare the values against the existing\r\n    // range to see if we can narrow the scope.\r\n    if (bound !== null) {\r\n        for (let i = 0; i < target.orderBy.length; ++i) {\r\n            const orderBy = target.orderBy[i];\r\n            if (orderBy.field.isEqual(fieldPath)) {\r\n                const cursorValue = bound.position[i];\r\n                if (lowerBoundCompare({ value, inclusive }, { value: cursorValue, inclusive: bound.inclusive }) < 0) {\r\n                    value = cursorValue;\r\n                    inclusive = bound.inclusive;\r\n                }\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    return { value, inclusive };\r\n}\r\n/**\r\n * Returns the value to use as the upper bound for ascending index segment at\r\n * the provided `fieldPath` (or the lower bound for a descending segment).\r\n */\r\nfunction targetGetDescendingBound(target, fieldPath, bound) {\r\n    let value = MAX_VALUE;\r\n    let inclusive = true;\r\n    // Process all filters to find a value for the current field segment\r\n    for (const fieldFilter of targetGetFieldFiltersForPath(target, fieldPath)) {\r\n        let filterValue = MAX_VALUE;\r\n        let filterInclusive = true;\r\n        switch (fieldFilter.op) {\r\n            case \">=\" /* GREATER_THAN_OR_EQUAL */:\r\n            case \">\" /* GREATER_THAN */:\r\n                filterValue = valuesGetUpperBound(fieldFilter.value);\r\n                filterInclusive = false;\r\n                break;\r\n            case \"==\" /* EQUAL */:\r\n            case \"in\" /* IN */:\r\n            case \"<=\" /* LESS_THAN_OR_EQUAL */:\r\n                filterValue = fieldFilter.value;\r\n                break;\r\n            case \"<\" /* LESS_THAN */:\r\n                filterValue = fieldFilter.value;\r\n                filterInclusive = false;\r\n                break;\r\n            case \"!=\" /* NOT_EQUAL */:\r\n            case \"not-in\" /* NOT_IN */:\r\n                filterValue = MAX_VALUE;\r\n                break;\r\n            // Remaining filters cannot be used as upper bounds.\r\n        }\r\n        if (upperBoundCompare({ value, inclusive }, { value: filterValue, inclusive: filterInclusive }) > 0) {\r\n            value = filterValue;\r\n            inclusive = filterInclusive;\r\n        }\r\n    }\r\n    // If there is an additional bound, compare the values against the existing\r\n    // range to see if we can narrow the scope.\r\n    if (bound !== null) {\r\n        for (let i = 0; i < target.orderBy.length; ++i) {\r\n            const orderBy = target.orderBy[i];\r\n            if (orderBy.field.isEqual(fieldPath)) {\r\n                const cursorValue = bound.position[i];\r\n                if (upperBoundCompare({ value, inclusive }, { value: cursorValue, inclusive: bound.inclusive }) > 0) {\r\n                    value = cursorValue;\r\n                    inclusive = bound.inclusive;\r\n                }\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    return { value, inclusive };\r\n}\r\n/** Returns the number of segments of a perfect index for this target. */\r\nfunction targetGetSegmentCount(target) {\r\n    let fields = new SortedSet(FieldPath$1.comparator);\r\n    let hasArraySegment = false;\r\n    for (const filter of target.filters) {\r\n        // TODO(orquery): Use the flattened filters here\r\n        const fieldFilter = filter;\r\n        // __name__ is not an explicit segment of any index, so we don't need to\r\n        // count it.\r\n        if (fieldFilter.field.isKeyField()) {\r\n            continue;\r\n        }\r\n        // ARRAY_CONTAINS or ARRAY_CONTAINS_ANY filters must be counted separately.\r\n        // For instance, it is possible to have an index for \"a ARRAY a ASC\". Even\r\n        // though these are on the same field, they should be counted as two\r\n        // separate segments in an index.\r\n        if (fieldFilter.op === \"array-contains\" /* ARRAY_CONTAINS */ ||\r\n            fieldFilter.op === \"array-contains-any\" /* ARRAY_CONTAINS_ANY */) {\r\n            hasArraySegment = true;\r\n        }\r\n        else {\r\n            fields = fields.add(fieldFilter.field);\r\n        }\r\n    }\r\n    for (const orderBy of target.orderBy) {\r\n        // __name__ is not an explicit segment of any index, so we don't need to\r\n        // count it.\r\n        if (!orderBy.field.isKeyField()) {\r\n            fields = fields.add(orderBy.field);\r\n        }\r\n    }\r\n    return fields.size + (hasArraySegment ? 1 : 0);\r\n}\r\nclass Filter {\r\n}\r\nclass FieldFilter extends Filter {\r\n    constructor(field, op, value) {\r\n        super();\r\n        this.field = field;\r\n        this.op = op;\r\n        this.value = value;\r\n    }\r\n    /**\r\n     * Creates a filter based on the provided arguments.\r\n     */\r\n    static create(field, op, value) {\r\n        if (field.isKeyField()) {\r\n            if (op === \"in\" /* IN */ || op === \"not-in\" /* NOT_IN */) {\r\n                return this.createKeyFieldInFilter(field, op, value);\r\n            }\r\n            else {\r\n                return new KeyFieldFilter(field, op, value);\r\n            }\r\n        }\r\n        else if (op === \"array-contains\" /* ARRAY_CONTAINS */) {\r\n            return new ArrayContainsFilter(field, value);\r\n        }\r\n        else if (op === \"in\" /* IN */) {\r\n            return new InFilter(field, value);\r\n        }\r\n        else if (op === \"not-in\" /* NOT_IN */) {\r\n            return new NotInFilter(field, value);\r\n        }\r\n        else if (op === \"array-contains-any\" /* ARRAY_CONTAINS_ANY */) {\r\n            return new ArrayContainsAnyFilter(field, value);\r\n        }\r\n        else {\r\n            return new FieldFilter(field, op, value);\r\n        }\r\n    }\r\n    static createKeyFieldInFilter(field, op, value) {\r\n        return op === \"in\" /* IN */\r\n            ? new KeyFieldInFilter(field, value)\r\n            : new KeyFieldNotInFilter(field, value);\r\n    }\r\n    matches(doc) {\r\n        const other = doc.data.field(this.field);\r\n        // Types do not have to match in NOT_EQUAL filters.\r\n        if (this.op === \"!=\" /* NOT_EQUAL */) {\r\n            return (other !== null &&\r\n                this.matchesComparison(valueCompare(other, this.value)));\r\n        }\r\n        // Only compare types with matching backend order (such as double and int).\r\n        return (other !== null &&\r\n            typeOrder(this.value) === typeOrder(other) &&\r\n            this.matchesComparison(valueCompare(other, this.value)));\r\n    }\r\n    matchesComparison(comparison) {\r\n        switch (this.op) {\r\n            case \"<\" /* LESS_THAN */:\r\n                return comparison < 0;\r\n            case \"<=\" /* LESS_THAN_OR_EQUAL */:\r\n                return comparison <= 0;\r\n            case \"==\" /* EQUAL */:\r\n                return comparison === 0;\r\n            case \"!=\" /* NOT_EQUAL */:\r\n                return comparison !== 0;\r\n            case \">\" /* GREATER_THAN */:\r\n                return comparison > 0;\r\n            case \">=\" /* GREATER_THAN_OR_EQUAL */:\r\n                return comparison >= 0;\r\n            default:\r\n                return fail();\r\n        }\r\n    }\r\n    isInequality() {\r\n        return ([\r\n            \"<\" /* LESS_THAN */,\r\n            \"<=\" /* LESS_THAN_OR_EQUAL */,\r\n            \">\" /* GREATER_THAN */,\r\n            \">=\" /* GREATER_THAN_OR_EQUAL */,\r\n            \"!=\" /* NOT_EQUAL */,\r\n            \"not-in\" /* NOT_IN */\r\n        ].indexOf(this.op) >= 0);\r\n    }\r\n}\r\nfunction canonifyFilter(filter) {\r\n    // TODO(b/29183165): Technically, this won't be unique if two values have\r\n    // the same description, such as the int 3 and the string \"3\". So we should\r\n    // add the types in here somehow, too.\r\n    return (filter.field.canonicalString() +\r\n        filter.op.toString() +\r\n        canonicalId(filter.value));\r\n}\r\nfunction filterEquals(f1, f2) {\r\n    return (f1.op === f2.op &&\r\n        f1.field.isEqual(f2.field) &&\r\n        valueEquals(f1.value, f2.value));\r\n}\r\n/** Returns a debug description for `filter`. */\r\nfunction stringifyFilter(filter) {\r\n    return `${filter.field.canonicalString()} ${filter.op} ${canonicalId(filter.value)}`;\r\n}\r\n/** Filter that matches on key fields (i.e. '__name__'). */\r\nclass KeyFieldFilter extends FieldFilter {\r\n    constructor(field, op, value) {\r\n        super(field, op, value);\r\n        this.key = DocumentKey.fromName(value.referenceValue);\r\n    }\r\n    matches(doc) {\r\n        const comparison = DocumentKey.comparator(doc.key, this.key);\r\n        return this.matchesComparison(comparison);\r\n    }\r\n}\r\n/** Filter that matches on key fields within an array. */\r\nclass KeyFieldInFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"in\" /* IN */, value);\r\n        this.keys = extractDocumentKeysFromArrayValue(\"in\" /* IN */, value);\r\n    }\r\n    matches(doc) {\r\n        return this.keys.some(key => key.isEqual(doc.key));\r\n    }\r\n}\r\n/** Filter that matches on key fields not present within an array. */\r\nclass KeyFieldNotInFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"not-in\" /* NOT_IN */, value);\r\n        this.keys = extractDocumentKeysFromArrayValue(\"not-in\" /* NOT_IN */, value);\r\n    }\r\n    matches(doc) {\r\n        return !this.keys.some(key => key.isEqual(doc.key));\r\n    }\r\n}\r\nfunction extractDocumentKeysFromArrayValue(op, value) {\r\n    var _a;\r\n    return (((_a = value.arrayValue) === null || _a === void 0 ? void 0 : _a.values) || []).map(v => {\r\n        return DocumentKey.fromName(v.referenceValue);\r\n    });\r\n}\r\n/** A Filter that implements the array-contains operator. */\r\nclass ArrayContainsFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"array-contains\" /* ARRAY_CONTAINS */, value);\r\n    }\r\n    matches(doc) {\r\n        const other = doc.data.field(this.field);\r\n        return isArray(other) && arrayValueContains(other.arrayValue, this.value);\r\n    }\r\n}\r\n/** A Filter that implements the IN operator. */\r\nclass InFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"in\" /* IN */, value);\r\n    }\r\n    matches(doc) {\r\n        const other = doc.data.field(this.field);\r\n        return other !== null && arrayValueContains(this.value.arrayValue, other);\r\n    }\r\n}\r\n/** A Filter that implements the not-in operator. */\r\nclass NotInFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"not-in\" /* NOT_IN */, value);\r\n    }\r\n    matches(doc) {\r\n        if (arrayValueContains(this.value.arrayValue, { nullValue: 'NULL_VALUE' })) {\r\n            return false;\r\n        }\r\n        const other = doc.data.field(this.field);\r\n        return other !== null && !arrayValueContains(this.value.arrayValue, other);\r\n    }\r\n}\r\n/** A Filter that implements the array-contains-any operator. */\r\nclass ArrayContainsAnyFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"array-contains-any\" /* ARRAY_CONTAINS_ANY */, value);\r\n    }\r\n    matches(doc) {\r\n        const other = doc.data.field(this.field);\r\n        if (!isArray(other) || !other.arrayValue.values) {\r\n            return false;\r\n        }\r\n        return other.arrayValue.values.some(val => arrayValueContains(this.value.arrayValue, val));\r\n    }\r\n}\r\n/**\r\n * Represents a bound of a query.\r\n *\r\n * The bound is specified with the given components representing a position and\r\n * whether it's just before or just after the position (relative to whatever the\r\n * query order is).\r\n *\r\n * The position represents a logical index position for a query. It's a prefix\r\n * of values for the (potentially implicit) order by clauses of a query.\r\n *\r\n * Bound provides a function to determine whether a document comes before or\r\n * after a bound. This is influenced by whether the position is just before or\r\n * just after the provided values.\r\n */\r\nclass Bound {\r\n    constructor(position, inclusive) {\r\n        this.position = position;\r\n        this.inclusive = inclusive;\r\n    }\r\n}\r\n/**\r\n * An ordering on a field, in some Direction. Direction defaults to ASCENDING.\r\n */\r\nclass OrderBy {\r\n    constructor(field, dir = \"asc\" /* ASCENDING */) {\r\n        this.field = field;\r\n        this.dir = dir;\r\n    }\r\n}\r\nfunction canonifyOrderBy(orderBy) {\r\n    // TODO(b/29183165): Make this collision robust.\r\n    return orderBy.field.canonicalString() + orderBy.dir;\r\n}\r\nfunction stringifyOrderBy(orderBy) {\r\n    return `${orderBy.field.canonicalString()} (${orderBy.dir})`;\r\n}\r\nfunction orderByEquals(left, right) {\r\n    return left.dir === right.dir && left.field.isEqual(right.field);\r\n}\r\nfunction boundCompareToDocument(bound, orderBy, doc) {\r\n    let comparison = 0;\r\n    for (let i = 0; i < bound.position.length; i++) {\r\n        const orderByComponent = orderBy[i];\r\n        const component = bound.position[i];\r\n        if (orderByComponent.field.isKeyField()) {\r\n            comparison = DocumentKey.comparator(DocumentKey.fromName(component.referenceValue), doc.key);\r\n        }\r\n        else {\r\n            const docValue = doc.data.field(orderByComponent.field);\r\n            comparison = valueCompare(component, docValue);\r\n        }\r\n        if (orderByComponent.dir === \"desc\" /* DESCENDING */) {\r\n            comparison = comparison * -1;\r\n        }\r\n        if (comparison !== 0) {\r\n            break;\r\n        }\r\n    }\r\n    return comparison;\r\n}\r\n/**\r\n * Returns true if a document sorts after a bound using the provided sort\r\n * order.\r\n */\r\nfunction boundSortsAfterDocument(bound, orderBy, doc) {\r\n    const comparison = boundCompareToDocument(bound, orderBy, doc);\r\n    return bound.inclusive ? comparison >= 0 : comparison > 0;\r\n}\r\n/**\r\n * Returns true if a document sorts before a bound using the provided sort\r\n * order.\r\n */\r\nfunction boundSortsBeforeDocument(bound, orderBy, doc) {\r\n    const comparison = boundCompareToDocument(bound, orderBy, doc);\r\n    return bound.inclusive ? comparison <= 0 : comparison < 0;\r\n}\r\nfunction boundEquals(left, right) {\r\n    if (left === null) {\r\n        return right === null;\r\n    }\r\n    else if (right === null) {\r\n        return false;\r\n    }\r\n    if (left.inclusive !== right.inclusive ||\r\n        left.position.length !== right.position.length) {\r\n        return false;\r\n    }\r\n    for (let i = 0; i < left.position.length; i++) {\r\n        const leftPosition = left.position[i];\r\n        const rightPosition = right.position[i];\r\n        if (!valueEquals(leftPosition, rightPosition)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Query encapsulates all the query attributes we support in the SDK. It can\r\n * be run against the LocalStore, as well as be converted to a `Target` to\r\n * query the RemoteStore results.\r\n *\r\n * Visible for testing.\r\n */\r\nclass QueryImpl {\r\n    /**\r\n     * Initializes a Query with a path and optional additional query constraints.\r\n     * Path must currently be empty if this is a collection group query.\r\n     */\r\n    constructor(path, collectionGroup = null, explicitOrderBy = [], filters = [], limit = null, limitType = \"F\" /* First */, startAt = null, endAt = null) {\r\n        this.path = path;\r\n        this.collectionGroup = collectionGroup;\r\n        this.explicitOrderBy = explicitOrderBy;\r\n        this.filters = filters;\r\n        this.limit = limit;\r\n        this.limitType = limitType;\r\n        this.startAt = startAt;\r\n        this.endAt = endAt;\r\n        this.memoizedOrderBy = null;\r\n        // The corresponding `Target` of this `Query` instance.\r\n        this.memoizedTarget = null;\r\n        if (this.startAt) ;\r\n        if (this.endAt) ;\r\n    }\r\n}\r\n/** Creates a new Query instance with the options provided. */\r\nfunction newQuery(path, collectionGroup, explicitOrderBy, filters, limit, limitType, startAt, endAt) {\r\n    return new QueryImpl(path, collectionGroup, explicitOrderBy, filters, limit, limitType, startAt, endAt);\r\n}\r\n/** Creates a new Query for a query that matches all documents at `path` */\r\nfunction newQueryForPath(path) {\r\n    return new QueryImpl(path);\r\n}\r\n/**\r\n * Helper to convert a collection group query into a collection query at a\r\n * specific path. This is used when executing collection group queries, since\r\n * we have to split the query into a set of collection queries at multiple\r\n * paths.\r\n */\r\nfunction asCollectionQueryAtPath(query, path) {\r\n    return new QueryImpl(path, \r\n    /*collectionGroup=*/ null, query.explicitOrderBy.slice(), query.filters.slice(), query.limit, query.limitType, query.startAt, query.endAt);\r\n}\r\n/**\r\n * Returns true if this query does not specify any query constraints that\r\n * could remove results.\r\n */\r\nfunction queryMatchesAllDocuments(query) {\r\n    return (query.filters.length === 0 &&\r\n        query.limit === null &&\r\n        query.startAt == null &&\r\n        query.endAt == null &&\r\n        (query.explicitOrderBy.length === 0 ||\r\n            (query.explicitOrderBy.length === 1 &&\r\n                query.explicitOrderBy[0].field.isKeyField())));\r\n}\r\nfunction getFirstOrderByField(query) {\r\n    return query.explicitOrderBy.length > 0\r\n        ? query.explicitOrderBy[0].field\r\n        : null;\r\n}\r\nfunction getInequalityFilterField(query) {\r\n    for (const filter of query.filters) {\r\n        if (filter.isInequality()) {\r\n            return filter.field;\r\n        }\r\n    }\r\n    return null;\r\n}\r\n/**\r\n * Checks if any of the provided Operators are included in the query and\r\n * returns the first one that is, or null if none are.\r\n */\r\nfunction findFilterOperator(query, operators) {\r\n    for (const filter of query.filters) {\r\n        if (operators.indexOf(filter.op) >= 0) {\r\n            return filter.op;\r\n        }\r\n    }\r\n    return null;\r\n}\r\n/**\r\n * Creates a new Query for a collection group query that matches all documents\r\n * within the provided collection group.\r\n */\r\nfunction newQueryForCollectionGroup(collectionId) {\r\n    return new QueryImpl(ResourcePath.emptyPath(), collectionId);\r\n}\r\n/**\r\n * Returns whether the query matches a single document by path (rather than a\r\n * collection).\r\n */\r\nfunction isDocumentQuery$1(query) {\r\n    return (DocumentKey.isDocumentKey(query.path) &&\r\n        query.collectionGroup === null &&\r\n        query.filters.length === 0);\r\n}\r\n/**\r\n * Returns whether the query matches a collection group rather than a specific\r\n * collection.\r\n */\r\nfunction isCollectionGroupQuery(query) {\r\n    return query.collectionGroup !== null;\r\n}\r\n/**\r\n * Returns the implicit order by constraint that is used to execute the Query,\r\n * which can be different from the order by constraints the user provided (e.g.\r\n * the SDK and backend always orders by `__name__`).\r\n */\r\nfunction queryOrderBy(query) {\r\n    const queryImpl = debugCast(query);\r\n    if (queryImpl.memoizedOrderBy === null) {\r\n        queryImpl.memoizedOrderBy = [];\r\n        const inequalityField = getInequalityFilterField(queryImpl);\r\n        const firstOrderByField = getFirstOrderByField(queryImpl);\r\n        if (inequalityField !== null && firstOrderByField === null) {\r\n            // In order to implicitly add key ordering, we must also add the\r\n            // inequality filter field for it to be a valid query.\r\n            // Note that the default inequality field and key ordering is ascending.\r\n            if (!inequalityField.isKeyField()) {\r\n                queryImpl.memoizedOrderBy.push(new OrderBy(inequalityField));\r\n            }\r\n            queryImpl.memoizedOrderBy.push(new OrderBy(FieldPath$1.keyField(), \"asc\" /* ASCENDING */));\r\n        }\r\n        else {\r\n            let foundKeyOrdering = false;\r\n            for (const orderBy of queryImpl.explicitOrderBy) {\r\n                queryImpl.memoizedOrderBy.push(orderBy);\r\n                if (orderBy.field.isKeyField()) {\r\n                    foundKeyOrdering = true;\r\n                }\r\n            }\r\n            if (!foundKeyOrdering) {\r\n                // The order of the implicit key ordering always matches the last\r\n                // explicit order by\r\n                const lastDirection = queryImpl.explicitOrderBy.length > 0\r\n                    ? queryImpl.explicitOrderBy[queryImpl.explicitOrderBy.length - 1]\r\n                        .dir\r\n                    : \"asc\" /* ASCENDING */;\r\n                queryImpl.memoizedOrderBy.push(new OrderBy(FieldPath$1.keyField(), lastDirection));\r\n            }\r\n        }\r\n    }\r\n    return queryImpl.memoizedOrderBy;\r\n}\r\n/**\r\n * Converts this `Query` instance to it's corresponding `Target` representation.\r\n */\r\nfunction queryToTarget(query) {\r\n    const queryImpl = debugCast(query, QueryImpl);\r\n    if (!queryImpl.memoizedTarget) {\r\n        if (queryImpl.limitType === \"F\" /* First */) {\r\n            queryImpl.memoizedTarget = newTarget(queryImpl.path, queryImpl.collectionGroup, queryOrderBy(queryImpl), queryImpl.filters, queryImpl.limit, queryImpl.startAt, queryImpl.endAt);\r\n        }\r\n        else {\r\n            // Flip the orderBy directions since we want the last results\r\n            const orderBys = [];\r\n            for (const orderBy of queryOrderBy(queryImpl)) {\r\n                const dir = orderBy.dir === \"desc\" /* DESCENDING */\r\n                    ? \"asc\" /* ASCENDING */\r\n                    : \"desc\" /* DESCENDING */;\r\n                orderBys.push(new OrderBy(orderBy.field, dir));\r\n            }\r\n            // We need to swap the cursors to match the now-flipped query ordering.\r\n            const startAt = queryImpl.endAt\r\n                ? new Bound(queryImpl.endAt.position, queryImpl.endAt.inclusive)\r\n                : null;\r\n            const endAt = queryImpl.startAt\r\n                ? new Bound(queryImpl.startAt.position, queryImpl.startAt.inclusive)\r\n                : null;\r\n            // Now return as a LimitType.First query.\r\n            queryImpl.memoizedTarget = newTarget(queryImpl.path, queryImpl.collectionGroup, orderBys, queryImpl.filters, queryImpl.limit, startAt, endAt);\r\n        }\r\n    }\r\n    return queryImpl.memoizedTarget;\r\n}\r\nfunction queryWithAddedFilter(query, filter) {\r\n    const newFilters = query.filters.concat([filter]);\r\n    return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), newFilters, query.limit, query.limitType, query.startAt, query.endAt);\r\n}\r\nfunction queryWithAddedOrderBy(query, orderBy) {\r\n    // TODO(dimond): validate that orderBy does not list the same key twice.\r\n    const newOrderBy = query.explicitOrderBy.concat([orderBy]);\r\n    return new QueryImpl(query.path, query.collectionGroup, newOrderBy, query.filters.slice(), query.limit, query.limitType, query.startAt, query.endAt);\r\n}\r\nfunction queryWithLimit(query, limit, limitType) {\r\n    return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), query.filters.slice(), limit, limitType, query.startAt, query.endAt);\r\n}\r\nfunction queryWithStartAt(query, bound) {\r\n    return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), query.filters.slice(), query.limit, query.limitType, bound, query.endAt);\r\n}\r\nfunction queryWithEndAt(query, bound) {\r\n    return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), query.filters.slice(), query.limit, query.limitType, query.startAt, bound);\r\n}\r\nfunction queryEquals(left, right) {\r\n    return (targetEquals(queryToTarget(left), queryToTarget(right)) &&\r\n        left.limitType === right.limitType);\r\n}\r\n// TODO(b/29183165): This is used to get a unique string from a query to, for\r\n// example, use as a dictionary key, but the implementation is subject to\r\n// collisions. Make it collision-free.\r\nfunction canonifyQuery(query) {\r\n    return `${canonifyTarget(queryToTarget(query))}|lt:${query.limitType}`;\r\n}\r\nfunction stringifyQuery(query) {\r\n    return `Query(target=${stringifyTarget(queryToTarget(query))}; limitType=${query.limitType})`;\r\n}\r\n/** Returns whether `doc` matches the constraints of `query`. */\r\nfunction queryMatches(query, doc) {\r\n    return (doc.isFoundDocument() &&\r\n        queryMatchesPathAndCollectionGroup(query, doc) &&\r\n        queryMatchesOrderBy(query, doc) &&\r\n        queryMatchesFilters(query, doc) &&\r\n        queryMatchesBounds(query, doc));\r\n}\r\nfunction queryMatchesPathAndCollectionGroup(query, doc) {\r\n    const docPath = doc.key.path;\r\n    if (query.collectionGroup !== null) {\r\n        // NOTE: this.path is currently always empty since we don't expose Collection\r\n        // Group queries rooted at a document path yet.\r\n        return (doc.key.hasCollectionId(query.collectionGroup) &&\r\n            query.path.isPrefixOf(docPath));\r\n    }\r\n    else if (DocumentKey.isDocumentKey(query.path)) {\r\n        // exact match for document queries\r\n        return query.path.isEqual(docPath);\r\n    }\r\n    else {\r\n        // shallow ancestor queries by default\r\n        return query.path.isImmediateParentOf(docPath);\r\n    }\r\n}\r\n/**\r\n * A document must have a value for every ordering clause in order to show up\r\n * in the results.\r\n */\r\nfunction queryMatchesOrderBy(query, doc) {\r\n    for (const orderBy of query.explicitOrderBy) {\r\n        // order by key always matches\r\n        if (!orderBy.field.isKeyField() && doc.data.field(orderBy.field) === null) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\r\nfunction queryMatchesFilters(query, doc) {\r\n    for (const filter of query.filters) {\r\n        if (!filter.matches(doc)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\r\n/** Makes sure a document is within the bounds, if provided. */\r\nfunction queryMatchesBounds(query, doc) {\r\n    if (query.startAt &&\r\n        !boundSortsBeforeDocument(query.startAt, queryOrderBy(query), doc)) {\r\n        return false;\r\n    }\r\n    if (query.endAt &&\r\n        !boundSortsAfterDocument(query.endAt, queryOrderBy(query), doc)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\r\n/**\r\n * Returns the collection group that this query targets.\r\n *\r\n * PORTING NOTE: This is only used in the Web SDK to facilitate multi-tab\r\n * synchronization for query results.\r\n */\r\nfunction queryCollectionGroup(query) {\r\n    return (query.collectionGroup ||\r\n        (query.path.length % 2 === 1\r\n            ? query.path.lastSegment()\r\n            : query.path.get(query.path.length - 2)));\r\n}\r\n/**\r\n * Returns a new comparator function that can be used to compare two documents\r\n * based on the Query's ordering constraint.\r\n */\r\nfunction newQueryComparator(query) {\r\n    return (d1, d2) => {\r\n        let comparedOnKeyField = false;\r\n        for (const orderBy of queryOrderBy(query)) {\r\n            const comp = compareDocs(orderBy, d1, d2);\r\n            if (comp !== 0) {\r\n                return comp;\r\n            }\r\n            comparedOnKeyField = comparedOnKeyField || orderBy.field.isKeyField();\r\n        }\r\n        return 0;\r\n    };\r\n}\r\nfunction compareDocs(orderBy, d1, d2) {\r\n    const comparison = orderBy.field.isKeyField()\r\n        ? DocumentKey.comparator(d1.key, d2.key)\r\n        : compareDocumentsByField(orderBy.field, d1, d2);\r\n    switch (orderBy.dir) {\r\n        case \"asc\" /* ASCENDING */:\r\n            return comparison;\r\n        case \"desc\" /* DESCENDING */:\r\n            return -1 * comparison;\r\n        default:\r\n            return fail();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A map implementation that uses objects as keys. Objects must have an\r\n * associated equals function and must be immutable. Entries in the map are\r\n * stored together with the key being produced from the mapKeyFn. This map\r\n * automatically handles collisions of keys.\r\n */\r\nclass ObjectMap {\r\n    constructor(mapKeyFn, equalsFn) {\r\n        this.mapKeyFn = mapKeyFn;\r\n        this.equalsFn = equalsFn;\r\n        /**\r\n         * The inner map for a key/value pair. Due to the possibility of collisions we\r\n         * keep a list of entries that we do a linear search through to find an actual\r\n         * match. Note that collisions should be rare, so we still expect near\r\n         * constant time lookups in practice.\r\n         */\r\n        this.inner = {};\r\n        /** The number of entries stored in the map */\r\n        this.innerSize = 0;\r\n    }\r\n    /** Get a value for this key, or undefined if it does not exist. */\r\n    get(key) {\r\n        const id = this.mapKeyFn(key);\r\n        const matches = this.inner[id];\r\n        if (matches === undefined) {\r\n            return undefined;\r\n        }\r\n        for (const [otherKey, value] of matches) {\r\n            if (this.equalsFn(otherKey, key)) {\r\n                return value;\r\n            }\r\n        }\r\n        return undefined;\r\n    }\r\n    has(key) {\r\n        return this.get(key) !== undefined;\r\n    }\r\n    /** Put this key and value in the map. */\r\n    set(key, value) {\r\n        const id = this.mapKeyFn(key);\r\n        const matches = this.inner[id];\r\n        if (matches === undefined) {\r\n            this.inner[id] = [[key, value]];\r\n            this.innerSize++;\r\n            return;\r\n        }\r\n        for (let i = 0; i < matches.length; i++) {\r\n            if (this.equalsFn(matches[i][0], key)) {\r\n                // This is updating an existing entry and does not increase `innerSize`.\r\n                matches[i] = [key, value];\r\n                return;\r\n            }\r\n        }\r\n        matches.push([key, value]);\r\n        this.innerSize++;\r\n    }\r\n    /**\r\n     * Remove this key from the map. Returns a boolean if anything was deleted.\r\n     */\r\n    delete(key) {\r\n        const id = this.mapKeyFn(key);\r\n        const matches = this.inner[id];\r\n        if (matches === undefined) {\r\n            return false;\r\n        }\r\n        for (let i = 0; i < matches.length; i++) {\r\n            if (this.equalsFn(matches[i][0], key)) {\r\n                if (matches.length === 1) {\r\n                    delete this.inner[id];\r\n                }\r\n                else {\r\n                    matches.splice(i, 1);\r\n                }\r\n                this.innerSize--;\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n    forEach(fn) {\r\n        forEach(this.inner, (_, entries) => {\r\n            for (const [k, v] of entries) {\r\n                fn(k, v);\r\n            }\r\n        });\r\n    }\r\n    isEmpty() {\r\n        return isEmpty(this.inner);\r\n    }\r\n    size() {\r\n        return this.innerSize;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst EMPTY_MUTABLE_DOCUMENT_MAP = new SortedMap(DocumentKey.comparator);\r\nfunction mutableDocumentMap() {\r\n    return EMPTY_MUTABLE_DOCUMENT_MAP;\r\n}\r\nconst EMPTY_DOCUMENT_MAP = new SortedMap(DocumentKey.comparator);\r\nfunction documentMap(...docs) {\r\n    let map = EMPTY_DOCUMENT_MAP;\r\n    for (const doc of docs) {\r\n        map = map.insert(doc.key, doc);\r\n    }\r\n    return map;\r\n}\r\nfunction newOverlayedDocumentMap() {\r\n    return newDocumentKeyMap();\r\n}\r\nfunction convertOverlayedDocumentMapToDocumentMap(collection) {\r\n    let documents = EMPTY_DOCUMENT_MAP;\r\n    collection.forEach((k, v) => (documents = documents.insert(k, v.overlayedDocument)));\r\n    return documents;\r\n}\r\nfunction newOverlayMap() {\r\n    return newDocumentKeyMap();\r\n}\r\nfunction newMutationMap() {\r\n    return newDocumentKeyMap();\r\n}\r\nfunction newDocumentKeyMap() {\r\n    return new ObjectMap(key => key.toString(), (l, r) => l.isEqual(r));\r\n}\r\nconst EMPTY_DOCUMENT_VERSION_MAP = new SortedMap(DocumentKey.comparator);\r\nfunction documentVersionMap() {\r\n    return EMPTY_DOCUMENT_VERSION_MAP;\r\n}\r\nconst EMPTY_DOCUMENT_KEY_SET = new SortedSet(DocumentKey.comparator);\r\nfunction documentKeySet(...keys) {\r\n    let set = EMPTY_DOCUMENT_KEY_SET;\r\n    for (const key of keys) {\r\n        set = set.add(key);\r\n    }\r\n    return set;\r\n}\r\nconst EMPTY_TARGET_ID_SET = new SortedSet(primitiveComparator);\r\nfunction targetIdSet() {\r\n    return EMPTY_TARGET_ID_SET;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Returns an DoubleValue for `value` that is encoded based the serializer's\r\n * `useProto3Json` setting.\r\n */\r\nfunction toDouble(serializer, value) {\r\n    if (serializer.useProto3Json) {\r\n        if (isNaN(value)) {\r\n            return { doubleValue: 'NaN' };\r\n        }\r\n        else if (value === Infinity) {\r\n            return { doubleValue: 'Infinity' };\r\n        }\r\n        else if (value === -Infinity) {\r\n            return { doubleValue: '-Infinity' };\r\n        }\r\n    }\r\n    return { doubleValue: isNegativeZero(value) ? '-0' : value };\r\n}\r\n/**\r\n * Returns an IntegerValue for `value`.\r\n */\r\nfunction toInteger(value) {\r\n    return { integerValue: '' + value };\r\n}\r\n/**\r\n * Returns a value for a number that's appropriate to put into a proto.\r\n * The return value is an IntegerValue if it can safely represent the value,\r\n * otherwise a DoubleValue is returned.\r\n */\r\nfunction toNumber(serializer, value) {\r\n    return isSafeInteger(value) ? toInteger(value) : toDouble(serializer, value);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Used to represent a field transform on a mutation. */\r\nclass TransformOperation {\r\n    constructor() {\r\n        // Make sure that the structural type of `TransformOperation` is unique.\r\n        // See https://github.com/microsoft/TypeScript/issues/5451\r\n        this._ = undefined;\r\n    }\r\n}\r\n/**\r\n * Computes the local transform result against the provided `previousValue`,\r\n * optionally using the provided localWriteTime.\r\n */\r\nfunction applyTransformOperationToLocalView(transform, previousValue, localWriteTime) {\r\n    if (transform instanceof ServerTimestampTransform) {\r\n        return serverTimestamp$1(localWriteTime, previousValue);\r\n    }\r\n    else if (transform instanceof ArrayUnionTransformOperation) {\r\n        return applyArrayUnionTransformOperation(transform, previousValue);\r\n    }\r\n    else if (transform instanceof ArrayRemoveTransformOperation) {\r\n        return applyArrayRemoveTransformOperation(transform, previousValue);\r\n    }\r\n    else {\r\n        return applyNumericIncrementTransformOperationToLocalView(transform, previousValue);\r\n    }\r\n}\r\n/**\r\n * Computes a final transform result after the transform has been acknowledged\r\n * by the server, potentially using the server-provided transformResult.\r\n */\r\nfunction applyTransformOperationToRemoteDocument(transform, previousValue, transformResult) {\r\n    // The server just sends null as the transform result for array operations,\r\n    // so we have to calculate a result the same as we do for local\r\n    // applications.\r\n    if (transform instanceof ArrayUnionTransformOperation) {\r\n        return applyArrayUnionTransformOperation(transform, previousValue);\r\n    }\r\n    else if (transform instanceof ArrayRemoveTransformOperation) {\r\n        return applyArrayRemoveTransformOperation(transform, previousValue);\r\n    }\r\n    return transformResult;\r\n}\r\n/**\r\n * If this transform operation is not idempotent, returns the base value to\r\n * persist for this transform. If a base value is returned, the transform\r\n * operation is always applied to this base value, even if document has\r\n * already been updated.\r\n *\r\n * Base values provide consistent behavior for non-idempotent transforms and\r\n * allow us to return the same latency-compensated value even if the backend\r\n * has already applied the transform operation. The base value is null for\r\n * idempotent transforms, as they can be re-played even if the backend has\r\n * already applied them.\r\n *\r\n * @returns a base value to store along with the mutation, or null for\r\n * idempotent transforms.\r\n */\r\nfunction computeTransformOperationBaseValue(transform, previousValue) {\r\n    if (transform instanceof NumericIncrementTransformOperation) {\r\n        return isNumber(previousValue) ? previousValue : { integerValue: 0 };\r\n    }\r\n    return null;\r\n}\r\nfunction transformOperationEquals(left, right) {\r\n    if (left instanceof ArrayUnionTransformOperation &&\r\n        right instanceof ArrayUnionTransformOperation) {\r\n        return arrayEquals(left.elements, right.elements, valueEquals);\r\n    }\r\n    else if (left instanceof ArrayRemoveTransformOperation &&\r\n        right instanceof ArrayRemoveTransformOperation) {\r\n        return arrayEquals(left.elements, right.elements, valueEquals);\r\n    }\r\n    else if (left instanceof NumericIncrementTransformOperation &&\r\n        right instanceof NumericIncrementTransformOperation) {\r\n        return valueEquals(left.operand, right.operand);\r\n    }\r\n    return (left instanceof ServerTimestampTransform &&\r\n        right instanceof ServerTimestampTransform);\r\n}\r\n/** Transforms a value into a server-generated timestamp. */\r\nclass ServerTimestampTransform extends TransformOperation {\r\n}\r\n/** Transforms an array value via a union operation. */\r\nclass ArrayUnionTransformOperation extends TransformOperation {\r\n    constructor(elements) {\r\n        super();\r\n        this.elements = elements;\r\n    }\r\n}\r\nfunction applyArrayUnionTransformOperation(transform, previousValue) {\r\n    const values = coercedFieldValuesArray(previousValue);\r\n    for (const toUnion of transform.elements) {\r\n        if (!values.some(element => valueEquals(element, toUnion))) {\r\n            values.push(toUnion);\r\n        }\r\n    }\r\n    return { arrayValue: { values } };\r\n}\r\n/** Transforms an array value via a remove operation. */\r\nclass ArrayRemoveTransformOperation extends TransformOperation {\r\n    constructor(elements) {\r\n        super();\r\n        this.elements = elements;\r\n    }\r\n}\r\nfunction applyArrayRemoveTransformOperation(transform, previousValue) {\r\n    let values = coercedFieldValuesArray(previousValue);\r\n    for (const toRemove of transform.elements) {\r\n        values = values.filter(element => !valueEquals(element, toRemove));\r\n    }\r\n    return { arrayValue: { values } };\r\n}\r\n/**\r\n * Implements the backend semantics for locally computed NUMERIC_ADD (increment)\r\n * transforms. Converts all field values to integers or doubles, but unlike the\r\n * backend does not cap integer values at 2^63. Instead, JavaScript number\r\n * arithmetic is used and precision loss can occur for values greater than 2^53.\r\n */\r\nclass NumericIncrementTransformOperation extends TransformOperation {\r\n    constructor(serializer, operand) {\r\n        super();\r\n        this.serializer = serializer;\r\n        this.operand = operand;\r\n    }\r\n}\r\nfunction applyNumericIncrementTransformOperationToLocalView(transform, previousValue) {\r\n    // PORTING NOTE: Since JavaScript's integer arithmetic is limited to 53 bit\r\n    // precision and resolves overflows by reducing precision, we do not\r\n    // manually cap overflows at 2^63.\r\n    const baseValue = computeTransformOperationBaseValue(transform, previousValue);\r\n    const sum = asNumber(baseValue) + asNumber(transform.operand);\r\n    if (isInteger(baseValue) && isInteger(transform.operand)) {\r\n        return toInteger(sum);\r\n    }\r\n    else {\r\n        return toDouble(transform.serializer, sum);\r\n    }\r\n}\r\nfunction asNumber(value) {\r\n    return normalizeNumber(value.integerValue || value.doubleValue);\r\n}\r\nfunction coercedFieldValuesArray(value) {\r\n    return isArray(value) && value.arrayValue.values\r\n        ? value.arrayValue.values.slice()\r\n        : [];\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** A field path and the TransformOperation to perform upon it. */\r\nclass FieldTransform {\r\n    constructor(field, transform) {\r\n        this.field = field;\r\n        this.transform = transform;\r\n    }\r\n}\r\nfunction fieldTransformEquals(left, right) {\r\n    return (left.field.isEqual(right.field) &&\r\n        transformOperationEquals(left.transform, right.transform));\r\n}\r\nfunction fieldTransformsAreEqual(left, right) {\r\n    if (left === undefined && right === undefined) {\r\n        return true;\r\n    }\r\n    if (left && right) {\r\n        return arrayEquals(left, right, (l, r) => fieldTransformEquals(l, r));\r\n    }\r\n    return false;\r\n}\r\n/** The result of successfully applying a mutation to the backend. */\r\nclass MutationResult {\r\n    constructor(\r\n    /**\r\n     * The version at which the mutation was committed:\r\n     *\r\n     * - For most operations, this is the updateTime in the WriteResult.\r\n     * - For deletes, the commitTime of the WriteResponse (because deletes are\r\n     *   not stored and have no updateTime).\r\n     *\r\n     * Note that these versions can be different: No-op writes will not change\r\n     * the updateTime even though the commitTime advances.\r\n     */\r\n    version, \r\n    /**\r\n     * The resulting fields returned from the backend after a mutation\r\n     * containing field transforms has been committed. Contains one FieldValue\r\n     * for each FieldTransform that was in the mutation.\r\n     *\r\n     * Will be empty if the mutation did not contain any field transforms.\r\n     */\r\n    transformResults) {\r\n        this.version = version;\r\n        this.transformResults = transformResults;\r\n    }\r\n}\r\n/**\r\n * Encodes a precondition for a mutation. This follows the model that the\r\n * backend accepts with the special case of an explicit \"empty\" precondition\r\n * (meaning no precondition).\r\n */\r\nclass Precondition {\r\n    constructor(updateTime, exists) {\r\n        this.updateTime = updateTime;\r\n        this.exists = exists;\r\n    }\r\n    /** Creates a new empty Precondition. */\r\n    static none() {\r\n        return new Precondition();\r\n    }\r\n    /** Creates a new Precondition with an exists flag. */\r\n    static exists(exists) {\r\n        return new Precondition(undefined, exists);\r\n    }\r\n    /** Creates a new Precondition based on a version a document exists at. */\r\n    static updateTime(version) {\r\n        return new Precondition(version);\r\n    }\r\n    /** Returns whether this Precondition is empty. */\r\n    get isNone() {\r\n        return this.updateTime === undefined && this.exists === undefined;\r\n    }\r\n    isEqual(other) {\r\n        return (this.exists === other.exists &&\r\n            (this.updateTime\r\n                ? !!other.updateTime && this.updateTime.isEqual(other.updateTime)\r\n                : !other.updateTime));\r\n    }\r\n}\r\n/** Returns true if the preconditions is valid for the given document. */\r\nfunction preconditionIsValidForDocument(precondition, document) {\r\n    if (precondition.updateTime !== undefined) {\r\n        return (document.isFoundDocument() &&\r\n            document.version.isEqual(precondition.updateTime));\r\n    }\r\n    else if (precondition.exists !== undefined) {\r\n        return precondition.exists === document.isFoundDocument();\r\n    }\r\n    else {\r\n        return true;\r\n    }\r\n}\r\n/**\r\n * A mutation describes a self-contained change to a document. Mutations can\r\n * create, replace, delete, and update subsets of documents.\r\n *\r\n * Mutations not only act on the value of the document but also its version.\r\n *\r\n * For local mutations (mutations that haven't been committed yet), we preserve\r\n * the existing version for Set and Patch mutations. For Delete mutations, we\r\n * reset the version to 0.\r\n *\r\n * Here's the expected transition table.\r\n *\r\n * MUTATION           APPLIED TO            RESULTS IN\r\n *\r\n * SetMutation        Document(v3)          Document(v3)\r\n * SetMutation        NoDocument(v3)        Document(v0)\r\n * SetMutation        InvalidDocument(v0)   Document(v0)\r\n * PatchMutation      Document(v3)          Document(v3)\r\n * PatchMutation      NoDocument(v3)        NoDocument(v3)\r\n * PatchMutation      InvalidDocument(v0)   UnknownDocument(v3)\r\n * DeleteMutation     Document(v3)          NoDocument(v0)\r\n * DeleteMutation     NoDocument(v3)        NoDocument(v0)\r\n * DeleteMutation     InvalidDocument(v0)   NoDocument(v0)\r\n *\r\n * For acknowledged mutations, we use the updateTime of the WriteResponse as\r\n * the resulting version for Set and Patch mutations. As deletes have no\r\n * explicit update time, we use the commitTime of the WriteResponse for\r\n * Delete mutations.\r\n *\r\n * If a mutation is acknowledged by the backend but fails the precondition check\r\n * locally, we transition to an `UnknownDocument` and rely on Watch to send us\r\n * the updated version.\r\n *\r\n * Field transforms are used only with Patch and Set Mutations. We use the\r\n * `updateTransforms` message to store transforms, rather than the `transforms`s\r\n * messages.\r\n *\r\n * ## Subclassing Notes\r\n *\r\n * Every type of mutation needs to implement its own applyToRemoteDocument() and\r\n * applyToLocalView() to implement the actual behavior of applying the mutation\r\n * to some source document (see `setMutationApplyToRemoteDocument()` for an\r\n * example).\r\n */\r\nclass Mutation {\r\n}\r\n/**\r\n * A utility method to calculate a `Mutation` representing the overlay from the\r\n * final state of the document, and a `FieldMask` representing the fields that\r\n * are mutated by the local mutations.\r\n */\r\nfunction calculateOverlayMutation(doc, mask) {\r\n    if (!doc.hasLocalMutations || (mask && mask.fields.length === 0)) {\r\n        return null;\r\n    }\r\n    // mask is null when sets or deletes are applied to the current document.\r\n    if (mask === null) {\r\n        if (doc.isNoDocument()) {\r\n            return new DeleteMutation(doc.key, Precondition.none());\r\n        }\r\n        else {\r\n            return new SetMutation(doc.key, doc.data, Precondition.none());\r\n        }\r\n    }\r\n    else {\r\n        const docValue = doc.data;\r\n        const patchValue = ObjectValue.empty();\r\n        let maskSet = new SortedSet(FieldPath$1.comparator);\r\n        for (let path of mask.fields) {\r\n            if (!maskSet.has(path)) {\r\n                let value = docValue.field(path);\r\n                // If we are deleting a nested field, we take the immediate parent as\r\n                // the mask used to construct the resulting mutation.\r\n                // Justification: Nested fields can create parent fields implicitly. If\r\n                // only a leaf entry is deleted in later mutations, the parent field\r\n                // should still remain, but we may have lost this information.\r\n                // Consider mutation (foo.bar 1), then mutation (foo.bar delete()).\r\n                // This leaves the final result (foo, {}). Despite the fact that `doc`\r\n                // has the correct result, `foo` is not in `mask`, and the resulting\r\n                // mutation would miss `foo`.\r\n                if (value === null && path.length > 1) {\r\n                    path = path.popLast();\r\n                    value = docValue.field(path);\r\n                }\r\n                if (value === null) {\r\n                    patchValue.delete(path);\r\n                }\r\n                else {\r\n                    patchValue.set(path, value);\r\n                }\r\n                maskSet = maskSet.add(path);\r\n            }\r\n        }\r\n        return new PatchMutation(doc.key, patchValue, new FieldMask(maskSet.toArray()), Precondition.none());\r\n    }\r\n}\r\n/**\r\n * Applies this mutation to the given document for the purposes of computing a\r\n * new remote document. If the input document doesn't match the expected state\r\n * (e.g. it is invalid or outdated), the document type may transition to\r\n * unknown.\r\n *\r\n * @param mutation - The mutation to apply.\r\n * @param document - The document to mutate. The input document can be an\r\n *     invalid document if the client has no knowledge of the pre-mutation state\r\n *     of the document.\r\n * @param mutationResult - The result of applying the mutation from the backend.\r\n */\r\nfunction mutationApplyToRemoteDocument(mutation, document, mutationResult) {\r\n    if (mutation instanceof SetMutation) {\r\n        setMutationApplyToRemoteDocument(mutation, document, mutationResult);\r\n    }\r\n    else if (mutation instanceof PatchMutation) {\r\n        patchMutationApplyToRemoteDocument(mutation, document, mutationResult);\r\n    }\r\n    else {\r\n        deleteMutationApplyToRemoteDocument(mutation, document, mutationResult);\r\n    }\r\n}\r\n/**\r\n * Applies this mutation to the given document for the purposes of computing\r\n * the new local view of a document. If the input document doesn't match the\r\n * expected state, the document is not modified.\r\n *\r\n * @param mutation - The mutation to apply.\r\n * @param document - The document to mutate. The input document can be an\r\n *     invalid document if the client has no knowledge of the pre-mutation state\r\n *     of the document.\r\n * @param previousMask - The fields that have been updated before applying this mutation.\r\n * @param localWriteTime - A timestamp indicating the local write time of the\r\n *     batch this mutation is a part of.\r\n * @returns A `FieldMask` representing the fields that are changed by applying this mutation.\r\n */\r\nfunction mutationApplyToLocalView(mutation, document, previousMask, localWriteTime) {\r\n    if (mutation instanceof SetMutation) {\r\n        return setMutationApplyToLocalView(mutation, document, previousMask, localWriteTime);\r\n    }\r\n    else if (mutation instanceof PatchMutation) {\r\n        return patchMutationApplyToLocalView(mutation, document, previousMask, localWriteTime);\r\n    }\r\n    else {\r\n        return deleteMutationApplyToLocalView(mutation, document, previousMask);\r\n    }\r\n}\r\n/**\r\n * If this mutation is not idempotent, returns the base value to persist with\r\n * this mutation. If a base value is returned, the mutation is always applied\r\n * to this base value, even if document has already been updated.\r\n *\r\n * The base value is a sparse object that consists of only the document\r\n * fields for which this mutation contains a non-idempotent transformation\r\n * (e.g. a numeric increment). The provided value guarantees consistent\r\n * behavior for non-idempotent transforms and allow us to return the same\r\n * latency-compensated value even if the backend has already applied the\r\n * mutation. The base value is null for idempotent mutations, as they can be\r\n * re-played even if the backend has already applied them.\r\n *\r\n * @returns a base value to store along with the mutation, or null for\r\n * idempotent mutations.\r\n */\r\nfunction mutationExtractBaseValue(mutation, document) {\r\n    let baseObject = null;\r\n    for (const fieldTransform of mutation.fieldTransforms) {\r\n        const existingValue = document.data.field(fieldTransform.field);\r\n        const coercedValue = computeTransformOperationBaseValue(fieldTransform.transform, existingValue || null);\r\n        if (coercedValue != null) {\r\n            if (baseObject === null) {\r\n                baseObject = ObjectValue.empty();\r\n            }\r\n            baseObject.set(fieldTransform.field, coercedValue);\r\n        }\r\n    }\r\n    return baseObject ? baseObject : null;\r\n}\r\nfunction mutationEquals(left, right) {\r\n    if (left.type !== right.type) {\r\n        return false;\r\n    }\r\n    if (!left.key.isEqual(right.key)) {\r\n        return false;\r\n    }\r\n    if (!left.precondition.isEqual(right.precondition)) {\r\n        return false;\r\n    }\r\n    if (!fieldTransformsAreEqual(left.fieldTransforms, right.fieldTransforms)) {\r\n        return false;\r\n    }\r\n    if (left.type === 0 /* Set */) {\r\n        return left.value.isEqual(right.value);\r\n    }\r\n    if (left.type === 1 /* Patch */) {\r\n        return (left.data.isEqual(right.data) &&\r\n            left.fieldMask.isEqual(right.fieldMask));\r\n    }\r\n    return true;\r\n}\r\n/**\r\n * A mutation that creates or replaces the document at the given key with the\r\n * object value contents.\r\n */\r\nclass SetMutation extends Mutation {\r\n    constructor(key, value, precondition, fieldTransforms = []) {\r\n        super();\r\n        this.key = key;\r\n        this.value = value;\r\n        this.precondition = precondition;\r\n        this.fieldTransforms = fieldTransforms;\r\n        this.type = 0 /* Set */;\r\n    }\r\n    getFieldMask() {\r\n        return null;\r\n    }\r\n}\r\nfunction setMutationApplyToRemoteDocument(mutation, document, mutationResult) {\r\n    // Unlike setMutationApplyToLocalView, if we're applying a mutation to a\r\n    // remote document the server has accepted the mutation so the precondition\r\n    // must have held.\r\n    const newData = mutation.value.clone();\r\n    const transformResults = serverTransformResults(mutation.fieldTransforms, document, mutationResult.transformResults);\r\n    newData.setAll(transformResults);\r\n    document\r\n        .convertToFoundDocument(mutationResult.version, newData)\r\n        .setHasCommittedMutations();\r\n}\r\nfunction setMutationApplyToLocalView(mutation, document, previousMask, localWriteTime) {\r\n    if (!preconditionIsValidForDocument(mutation.precondition, document)) {\r\n        // The mutation failed to apply (e.g. a document ID created with add()\r\n        // caused a name collision).\r\n        return previousMask;\r\n    }\r\n    const newData = mutation.value.clone();\r\n    const transformResults = localTransformResults(mutation.fieldTransforms, localWriteTime, document);\r\n    newData.setAll(transformResults);\r\n    document\r\n        .convertToFoundDocument(document.version, newData)\r\n        .setHasLocalMutations();\r\n    return null; // SetMutation overwrites all fields.\r\n}\r\n/**\r\n * A mutation that modifies fields of the document at the given key with the\r\n * given values. The values are applied through a field mask:\r\n *\r\n *  * When a field is in both the mask and the values, the corresponding field\r\n *    is updated.\r\n *  * When a field is in neither the mask nor the values, the corresponding\r\n *    field is unmodified.\r\n *  * When a field is in the mask but not in the values, the corresponding field\r\n *    is deleted.\r\n *  * When a field is not in the mask but is in the values, the values map is\r\n *    ignored.\r\n */\r\nclass PatchMutation extends Mutation {\r\n    constructor(key, data, fieldMask, precondition, fieldTransforms = []) {\r\n        super();\r\n        this.key = key;\r\n        this.data = data;\r\n        this.fieldMask = fieldMask;\r\n        this.precondition = precondition;\r\n        this.fieldTransforms = fieldTransforms;\r\n        this.type = 1 /* Patch */;\r\n    }\r\n    getFieldMask() {\r\n        return this.fieldMask;\r\n    }\r\n}\r\nfunction patchMutationApplyToRemoteDocument(mutation, document, mutationResult) {\r\n    if (!preconditionIsValidForDocument(mutation.precondition, document)) {\r\n        // Since the mutation was not rejected, we know that the precondition\r\n        // matched on the backend. We therefore must not have the expected version\r\n        // of the document in our cache and convert to an UnknownDocument with a\r\n        // known updateTime.\r\n        document.convertToUnknownDocument(mutationResult.version);\r\n        return;\r\n    }\r\n    const transformResults = serverTransformResults(mutation.fieldTransforms, document, mutationResult.transformResults);\r\n    const newData = document.data;\r\n    newData.setAll(getPatch(mutation));\r\n    newData.setAll(transformResults);\r\n    document\r\n        .convertToFoundDocument(mutationResult.version, newData)\r\n        .setHasCommittedMutations();\r\n}\r\nfunction patchMutationApplyToLocalView(mutation, document, previousMask, localWriteTime) {\r\n    if (!preconditionIsValidForDocument(mutation.precondition, document)) {\r\n        return previousMask;\r\n    }\r\n    const transformResults = localTransformResults(mutation.fieldTransforms, localWriteTime, document);\r\n    const newData = document.data;\r\n    newData.setAll(getPatch(mutation));\r\n    newData.setAll(transformResults);\r\n    document\r\n        .convertToFoundDocument(document.version, newData)\r\n        .setHasLocalMutations();\r\n    if (previousMask === null) {\r\n        return null;\r\n    }\r\n    return previousMask\r\n        .unionWith(mutation.fieldMask.fields)\r\n        .unionWith(mutation.fieldTransforms.map(transform => transform.field));\r\n}\r\n/**\r\n * Returns a FieldPath/Value map with the content of the PatchMutation.\r\n */\r\nfunction getPatch(mutation) {\r\n    const result = new Map();\r\n    mutation.fieldMask.fields.forEach(fieldPath => {\r\n        if (!fieldPath.isEmpty()) {\r\n            const newValue = mutation.data.field(fieldPath);\r\n            result.set(fieldPath, newValue);\r\n        }\r\n    });\r\n    return result;\r\n}\r\n/**\r\n * Creates a list of \"transform results\" (a transform result is a field value\r\n * representing the result of applying a transform) for use after a mutation\r\n * containing transforms has been acknowledged by the server.\r\n *\r\n * @param fieldTransforms - The field transforms to apply the result to.\r\n * @param mutableDocument - The current state of the document after applying all\r\n * previous mutations.\r\n * @param serverTransformResults - The transform results received by the server.\r\n * @returns The transform results list.\r\n */\r\nfunction serverTransformResults(fieldTransforms, mutableDocument, serverTransformResults) {\r\n    const transformResults = new Map();\r\n    hardAssert(fieldTransforms.length === serverTransformResults.length);\r\n    for (let i = 0; i < serverTransformResults.length; i++) {\r\n        const fieldTransform = fieldTransforms[i];\r\n        const transform = fieldTransform.transform;\r\n        const previousValue = mutableDocument.data.field(fieldTransform.field);\r\n        transformResults.set(fieldTransform.field, applyTransformOperationToRemoteDocument(transform, previousValue, serverTransformResults[i]));\r\n    }\r\n    return transformResults;\r\n}\r\n/**\r\n * Creates a list of \"transform results\" (a transform result is a field value\r\n * representing the result of applying a transform) for use when applying a\r\n * transform locally.\r\n *\r\n * @param fieldTransforms - The field transforms to apply the result to.\r\n * @param localWriteTime - The local time of the mutation (used to\r\n *     generate ServerTimestampValues).\r\n * @param mutableDocument - The document to apply transforms on.\r\n * @returns The transform results list.\r\n */\r\nfunction localTransformResults(fieldTransforms, localWriteTime, mutableDocument) {\r\n    const transformResults = new Map();\r\n    for (const fieldTransform of fieldTransforms) {\r\n        const transform = fieldTransform.transform;\r\n        const previousValue = mutableDocument.data.field(fieldTransform.field);\r\n        transformResults.set(fieldTransform.field, applyTransformOperationToLocalView(transform, previousValue, localWriteTime));\r\n    }\r\n    return transformResults;\r\n}\r\n/** A mutation that deletes the document at the given key. */\r\nclass DeleteMutation extends Mutation {\r\n    constructor(key, precondition) {\r\n        super();\r\n        this.key = key;\r\n        this.precondition = precondition;\r\n        this.type = 2 /* Delete */;\r\n        this.fieldTransforms = [];\r\n    }\r\n    getFieldMask() {\r\n        return null;\r\n    }\r\n}\r\nfunction deleteMutationApplyToRemoteDocument(mutation, document, mutationResult) {\r\n    // Unlike applyToLocalView, if we're applying a mutation to a remote\r\n    // document the server has accepted the mutation so the precondition must\r\n    // have held.\r\n    document\r\n        .convertToNoDocument(mutationResult.version)\r\n        .setHasCommittedMutations();\r\n}\r\nfunction deleteMutationApplyToLocalView(mutation, document, previousMask) {\r\n    if (preconditionIsValidForDocument(mutation.precondition, document)) {\r\n        document.convertToNoDocument(document.version).setHasLocalMutations();\r\n        return null;\r\n    }\r\n    return previousMask;\r\n}\r\n/**\r\n * A mutation that verifies the existence of the document at the given key with\r\n * the provided precondition.\r\n *\r\n * The `verify` operation is only used in Transactions, and this class serves\r\n * primarily to facilitate serialization into protos.\r\n */\r\nclass VerifyMutation extends Mutation {\r\n    constructor(key, precondition) {\r\n        super();\r\n        this.key = key;\r\n        this.precondition = precondition;\r\n        this.type = 3 /* Verify */;\r\n        this.fieldTransforms = [];\r\n    }\r\n    getFieldMask() {\r\n        return null;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A batch of mutations that will be sent as one unit to the backend.\r\n */\r\nclass MutationBatch {\r\n    /**\r\n     * @param batchId - The unique ID of this mutation batch.\r\n     * @param localWriteTime - The original write time of this mutation.\r\n     * @param baseMutations - Mutations that are used to populate the base\r\n     * values when this mutation is applied locally. This can be used to locally\r\n     * overwrite values that are persisted in the remote document cache. Base\r\n     * mutations are never sent to the backend.\r\n     * @param mutations - The user-provided mutations in this mutation batch.\r\n     * User-provided mutations are applied both locally and remotely on the\r\n     * backend.\r\n     */\r\n    constructor(batchId, localWriteTime, baseMutations, mutations) {\r\n        this.batchId = batchId;\r\n        this.localWriteTime = localWriteTime;\r\n        this.baseMutations = baseMutations;\r\n        this.mutations = mutations;\r\n    }\r\n    /**\r\n     * Applies all the mutations in this MutationBatch to the specified document\r\n     * to compute the state of the remote document\r\n     *\r\n     * @param document - The document to apply mutations to.\r\n     * @param batchResult - The result of applying the MutationBatch to the\r\n     * backend.\r\n     */\r\n    applyToRemoteDocument(document, batchResult) {\r\n        const mutationResults = batchResult.mutationResults;\r\n        for (let i = 0; i < this.mutations.length; i++) {\r\n            const mutation = this.mutations[i];\r\n            if (mutation.key.isEqual(document.key)) {\r\n                const mutationResult = mutationResults[i];\r\n                mutationApplyToRemoteDocument(mutation, document, mutationResult);\r\n            }\r\n        }\r\n    }\r\n    /**\r\n     * Computes the local view of a document given all the mutations in this\r\n     * batch.\r\n     *\r\n     * @param document - The document to apply mutations to.\r\n     * @param mutatedFields - Fields that have been updated before applying this mutation batch.\r\n     * @returns A `FieldMask` representing all the fields that are mutated.\r\n     */\r\n    applyToLocalView(document, mutatedFields) {\r\n        // First, apply the base state. This allows us to apply non-idempotent\r\n        // transform against a consistent set of values.\r\n        for (const mutation of this.baseMutations) {\r\n            if (mutation.key.isEqual(document.key)) {\r\n                mutatedFields = mutationApplyToLocalView(mutation, document, mutatedFields, this.localWriteTime);\r\n            }\r\n        }\r\n        // Second, apply all user-provided mutations.\r\n        for (const mutation of this.mutations) {\r\n            if (mutation.key.isEqual(document.key)) {\r\n                mutatedFields = mutationApplyToLocalView(mutation, document, mutatedFields, this.localWriteTime);\r\n            }\r\n        }\r\n        return mutatedFields;\r\n    }\r\n    /**\r\n     * Computes the local view for all provided documents given the mutations in\r\n     * this batch. Returns a `DocumentKey` to `Mutation` map which can be used to\r\n     * replace all the mutation applications.\r\n     */\r\n    applyToLocalDocumentSet(documentMap, documentsWithoutRemoteVersion) {\r\n        // TODO(mrschmidt): This implementation is O(n^2). If we apply the mutations\r\n        // directly (as done in `applyToLocalView()`), we can reduce the complexity\r\n        // to O(n).\r\n        const overlays = newMutationMap();\r\n        this.mutations.forEach(m => {\r\n            const overlayedDocument = documentMap.get(m.key);\r\n            // TODO(mutabledocuments): This method should take a MutableDocumentMap\r\n            // and we should remove this cast.\r\n            const mutableDocument = overlayedDocument.overlayedDocument;\r\n            let mutatedFields = this.applyToLocalView(mutableDocument, overlayedDocument.mutatedFields);\r\n            // Set mutatedFields to null if the document is only from local mutations.\r\n            // This creates a Set or Delete mutation, instead of trying to create a\r\n            // patch mutation as the overlay.\r\n            mutatedFields = documentsWithoutRemoteVersion.has(m.key)\r\n                ? null\r\n                : mutatedFields;\r\n            const overlay = calculateOverlayMutation(mutableDocument, mutatedFields);\r\n            if (overlay !== null) {\r\n                overlays.set(m.key, overlay);\r\n            }\r\n            if (!mutableDocument.isValidDocument()) {\r\n                mutableDocument.convertToNoDocument(SnapshotVersion.min());\r\n            }\r\n        });\r\n        return overlays;\r\n    }\r\n    keys() {\r\n        return this.mutations.reduce((keys, m) => keys.add(m.key), documentKeySet());\r\n    }\r\n    isEqual(other) {\r\n        return (this.batchId === other.batchId &&\r\n            arrayEquals(this.mutations, other.mutations, (l, r) => mutationEquals(l, r)) &&\r\n            arrayEquals(this.baseMutations, other.baseMutations, (l, r) => mutationEquals(l, r)));\r\n    }\r\n}\r\n/** The result of applying a mutation batch to the backend. */\r\nclass MutationBatchResult {\r\n    constructor(batch, commitVersion, mutationResults, \r\n    /**\r\n     * A pre-computed mapping from each mutated document to the resulting\r\n     * version.\r\n     */\r\n    docVersions) {\r\n        this.batch = batch;\r\n        this.commitVersion = commitVersion;\r\n        this.mutationResults = mutationResults;\r\n        this.docVersions = docVersions;\r\n    }\r\n    /**\r\n     * Creates a new MutationBatchResult for the given batch and results. There\r\n     * must be one result for each mutation in the batch. This static factory\r\n     * caches a document=&gt;version mapping (docVersions).\r\n     */\r\n    static from(batch, commitVersion, results) {\r\n        hardAssert(batch.mutations.length === results.length);\r\n        let versionMap = documentVersionMap();\r\n        const mutations = batch.mutations;\r\n        for (let i = 0; i < mutations.length; i++) {\r\n            versionMap = versionMap.insert(mutations[i].key, results[i].version);\r\n        }\r\n        return new MutationBatchResult(batch, commitVersion, results, versionMap);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Representation of an overlay computed by Firestore.\r\n *\r\n * Holds information about a mutation and the largest batch id in Firestore when\r\n * the mutation was created.\r\n */\r\nclass Overlay {\r\n    constructor(largestBatchId, mutation) {\r\n        this.largestBatchId = largestBatchId;\r\n        this.mutation = mutation;\r\n    }\r\n    getKey() {\r\n        return this.mutation.key;\r\n    }\r\n    isEqual(other) {\r\n        return other !== null && this.mutation === other.mutation;\r\n    }\r\n    toString() {\r\n        return `Overlay{\n      largestBatchId: ${this.largestBatchId},\n      mutation: ${this.mutation.toString()}\n    }`;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass ExistenceFilter {\r\n    // TODO(b/33078163): just use simplest form of existence filter for now\r\n    constructor(count) {\r\n        this.count = count;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Error Codes describing the different ways GRPC can fail. These are copied\r\n * directly from GRPC's sources here:\r\n *\r\n * https://github.com/grpc/grpc/blob/bceec94ea4fc5f0085d81235d8e1c06798dc341a/include/grpc%2B%2B/impl/codegen/status_code_enum.h\r\n *\r\n * Important! The names of these identifiers matter because the string forms\r\n * are used for reverse lookups from the webchannel stream. Do NOT change the\r\n * names of these identifiers or change this into a const enum.\r\n */\r\nvar RpcCode;\r\n(function (RpcCode) {\r\n    RpcCode[RpcCode[\"OK\"] = 0] = \"OK\";\r\n    RpcCode[RpcCode[\"CANCELLED\"] = 1] = \"CANCELLED\";\r\n    RpcCode[RpcCode[\"UNKNOWN\"] = 2] = \"UNKNOWN\";\r\n    RpcCode[RpcCode[\"INVALID_ARGUMENT\"] = 3] = \"INVALID_ARGUMENT\";\r\n    RpcCode[RpcCode[\"DEADLINE_EXCEEDED\"] = 4] = \"DEADLINE_EXCEEDED\";\r\n    RpcCode[RpcCode[\"NOT_FOUND\"] = 5] = \"NOT_FOUND\";\r\n    RpcCode[RpcCode[\"ALREADY_EXISTS\"] = 6] = \"ALREADY_EXISTS\";\r\n    RpcCode[RpcCode[\"PERMISSION_DENIED\"] = 7] = \"PERMISSION_DENIED\";\r\n    RpcCode[RpcCode[\"UNAUTHENTICATED\"] = 16] = \"UNAUTHENTICATED\";\r\n    RpcCode[RpcCode[\"RESOURCE_EXHAUSTED\"] = 8] = \"RESOURCE_EXHAUSTED\";\r\n    RpcCode[RpcCode[\"FAILED_PRECONDITION\"] = 9] = \"FAILED_PRECONDITION\";\r\n    RpcCode[RpcCode[\"ABORTED\"] = 10] = \"ABORTED\";\r\n    RpcCode[RpcCode[\"OUT_OF_RANGE\"] = 11] = \"OUT_OF_RANGE\";\r\n    RpcCode[RpcCode[\"UNIMPLEMENTED\"] = 12] = \"UNIMPLEMENTED\";\r\n    RpcCode[RpcCode[\"INTERNAL\"] = 13] = \"INTERNAL\";\r\n    RpcCode[RpcCode[\"UNAVAILABLE\"] = 14] = \"UNAVAILABLE\";\r\n    RpcCode[RpcCode[\"DATA_LOSS\"] = 15] = \"DATA_LOSS\";\r\n})(RpcCode || (RpcCode = {}));\r\n/**\r\n * Determines whether an error code represents a permanent error when received\r\n * in response to a non-write operation.\r\n *\r\n * See isPermanentWriteError for classifying write errors.\r\n */\r\nfunction isPermanentError(code) {\r\n    switch (code) {\r\n        case Code.OK:\r\n            return fail();\r\n        case Code.CANCELLED:\r\n        case Code.UNKNOWN:\r\n        case Code.DEADLINE_EXCEEDED:\r\n        case Code.RESOURCE_EXHAUSTED:\r\n        case Code.INTERNAL:\r\n        case Code.UNAVAILABLE:\r\n        // Unauthenticated means something went wrong with our token and we need\r\n        // to retry with new credentials which will happen automatically.\r\n        case Code.UNAUTHENTICATED:\r\n            return false;\r\n        case Code.INVALID_ARGUMENT:\r\n        case Code.NOT_FOUND:\r\n        case Code.ALREADY_EXISTS:\r\n        case Code.PERMISSION_DENIED:\r\n        case Code.FAILED_PRECONDITION:\r\n        // Aborted might be retried in some scenarios, but that is dependant on\r\n        // the context and should handled individually by the calling code.\r\n        // See https://cloud.google.com/apis/design/errors.\r\n        case Code.ABORTED:\r\n        case Code.OUT_OF_RANGE:\r\n        case Code.UNIMPLEMENTED:\r\n        case Code.DATA_LOSS:\r\n            return true;\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\n/**\r\n * Determines whether an error code represents a permanent error when received\r\n * in response to a write operation.\r\n *\r\n * Write operations must be handled specially because as of b/119437764, ABORTED\r\n * errors on the write stream should be retried too (even though ABORTED errors\r\n * are not generally retryable).\r\n *\r\n * Note that during the initial handshake on the write stream an ABORTED error\r\n * signals that we should discard our stream token (i.e. it is permanent). This\r\n * means a handshake error should be classified with isPermanentError, above.\r\n */\r\nfunction isPermanentWriteError(code) {\r\n    return isPermanentError(code) && code !== Code.ABORTED;\r\n}\r\n/**\r\n * Maps an error Code from GRPC status code number, like 0, 1, or 14. These\r\n * are not the same as HTTP status codes.\r\n *\r\n * @returns The Code equivalent to the given GRPC status code. Fails if there\r\n *     is no match.\r\n */\r\nfunction mapCodeFromRpcCode(code) {\r\n    if (code === undefined) {\r\n        // This shouldn't normally happen, but in certain error cases (like trying\r\n        // to send invalid proto messages) we may get an error with no GRPC code.\r\n        logError('GRPC error has no .code');\r\n        return Code.UNKNOWN;\r\n    }\r\n    switch (code) {\r\n        case RpcCode.OK:\r\n            return Code.OK;\r\n        case RpcCode.CANCELLED:\r\n            return Code.CANCELLED;\r\n        case RpcCode.UNKNOWN:\r\n            return Code.UNKNOWN;\r\n        case RpcCode.DEADLINE_EXCEEDED:\r\n            return Code.DEADLINE_EXCEEDED;\r\n        case RpcCode.RESOURCE_EXHAUSTED:\r\n            return Code.RESOURCE_EXHAUSTED;\r\n        case RpcCode.INTERNAL:\r\n            return Code.INTERNAL;\r\n        case RpcCode.UNAVAILABLE:\r\n            return Code.UNAVAILABLE;\r\n        case RpcCode.UNAUTHENTICATED:\r\n            return Code.UNAUTHENTICATED;\r\n        case RpcCode.INVALID_ARGUMENT:\r\n            return Code.INVALID_ARGUMENT;\r\n        case RpcCode.NOT_FOUND:\r\n            return Code.NOT_FOUND;\r\n        case RpcCode.ALREADY_EXISTS:\r\n            return Code.ALREADY_EXISTS;\r\n        case RpcCode.PERMISSION_DENIED:\r\n            return Code.PERMISSION_DENIED;\r\n        case RpcCode.FAILED_PRECONDITION:\r\n            return Code.FAILED_PRECONDITION;\r\n        case RpcCode.ABORTED:\r\n            return Code.ABORTED;\r\n        case RpcCode.OUT_OF_RANGE:\r\n            return Code.OUT_OF_RANGE;\r\n        case RpcCode.UNIMPLEMENTED:\r\n            return Code.UNIMPLEMENTED;\r\n        case RpcCode.DATA_LOSS:\r\n            return Code.DATA_LOSS;\r\n        default:\r\n            return fail();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An event from the RemoteStore. It is split into targetChanges (changes to the\r\n * state or the set of documents in our watched targets) and documentUpdates\r\n * (changes to the actual documents).\r\n */\r\nclass RemoteEvent {\r\n    constructor(\r\n    /**\r\n     * The snapshot version this event brings us up to, or MIN if not set.\r\n     */\r\n    snapshotVersion, \r\n    /**\r\n     * A map from target to changes to the target. See TargetChange.\r\n     */\r\n    targetChanges, \r\n    /**\r\n     * A set of targets that is known to be inconsistent. Listens for these\r\n     * targets should be re-established without resume tokens.\r\n     */\r\n    targetMismatches, \r\n    /**\r\n     * A set of which documents have changed or been deleted, along with the\r\n     * doc's new values (if not deleted).\r\n     */\r\n    documentUpdates, \r\n    /**\r\n     * A set of which document updates are due only to limbo resolution targets.\r\n     */\r\n    resolvedLimboDocuments) {\r\n        this.snapshotVersion = snapshotVersion;\r\n        this.targetChanges = targetChanges;\r\n        this.targetMismatches = targetMismatches;\r\n        this.documentUpdates = documentUpdates;\r\n        this.resolvedLimboDocuments = resolvedLimboDocuments;\r\n    }\r\n    /**\r\n     * HACK: Views require RemoteEvents in order to determine whether the view is\r\n     * CURRENT, but secondary tabs don't receive remote events. So this method is\r\n     * used to create a synthesized RemoteEvent that can be used to apply a\r\n     * CURRENT status change to a View, for queries executed in a different tab.\r\n     */\r\n    // PORTING NOTE: Multi-tab only\r\n    static createSynthesizedRemoteEventForCurrentChange(targetId, current) {\r\n        const targetChanges = new Map();\r\n        targetChanges.set(targetId, TargetChange.createSynthesizedTargetChangeForCurrentChange(targetId, current));\r\n        return new RemoteEvent(SnapshotVersion.min(), targetChanges, targetIdSet(), mutableDocumentMap(), documentKeySet());\r\n    }\r\n}\r\n/**\r\n * A TargetChange specifies the set of changes for a specific target as part of\r\n * a RemoteEvent. These changes track which documents are added, modified or\r\n * removed, as well as the target's resume token and whether the target is\r\n * marked CURRENT.\r\n * The actual changes *to* documents are not part of the TargetChange since\r\n * documents may be part of multiple targets.\r\n */\r\nclass TargetChange {\r\n    constructor(\r\n    /**\r\n     * An opaque, server-assigned token that allows watching a query to be resumed\r\n     * after disconnecting without retransmitting all the data that matches the\r\n     * query. The resume token essentially identifies a point in time from which\r\n     * the server should resume sending results.\r\n     */\r\n    resumeToken, \r\n    /**\r\n     * The \"current\" (synced) status of this target. Note that \"current\"\r\n     * has special meaning in the RPC protocol that implies that a target is\r\n     * both up-to-date and consistent with the rest of the watch stream.\r\n     */\r\n    current, \r\n    /**\r\n     * The set of documents that were newly assigned to this target as part of\r\n     * this remote event.\r\n     */\r\n    addedDocuments, \r\n    /**\r\n     * The set of documents that were already assigned to this target but received\r\n     * an update during this remote event.\r\n     */\r\n    modifiedDocuments, \r\n    /**\r\n     * The set of documents that were removed from this target as part of this\r\n     * remote event.\r\n     */\r\n    removedDocuments) {\r\n        this.resumeToken = resumeToken;\r\n        this.current = current;\r\n        this.addedDocuments = addedDocuments;\r\n        this.modifiedDocuments = modifiedDocuments;\r\n        this.removedDocuments = removedDocuments;\r\n    }\r\n    /**\r\n     * This method is used to create a synthesized TargetChanges that can be used to\r\n     * apply a CURRENT status change to a View (for queries executed in a different\r\n     * tab) or for new queries (to raise snapshots with correct CURRENT status).\r\n     */\r\n    static createSynthesizedTargetChangeForCurrentChange(targetId, current) {\r\n        return new TargetChange(ByteString.EMPTY_BYTE_STRING, current, documentKeySet(), documentKeySet(), documentKeySet());\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Represents a changed document and a list of target ids to which this change\r\n * applies.\r\n *\r\n * If document has been deleted NoDocument will be provided.\r\n */\r\nclass DocumentWatchChange {\r\n    constructor(\r\n    /** The new document applies to all of these targets. */\r\n    updatedTargetIds, \r\n    /** The new document is removed from all of these targets. */\r\n    removedTargetIds, \r\n    /** The key of the document for this change. */\r\n    key, \r\n    /**\r\n     * The new document or NoDocument if it was deleted. Is null if the\r\n     * document went out of view without the server sending a new document.\r\n     */\r\n    newDoc) {\r\n        this.updatedTargetIds = updatedTargetIds;\r\n        this.removedTargetIds = removedTargetIds;\r\n        this.key = key;\r\n        this.newDoc = newDoc;\r\n    }\r\n}\r\nclass ExistenceFilterChange {\r\n    constructor(targetId, existenceFilter) {\r\n        this.targetId = targetId;\r\n        this.existenceFilter = existenceFilter;\r\n    }\r\n}\r\nclass WatchTargetChange {\r\n    constructor(\r\n    /** What kind of change occurred to the watch target. */\r\n    state, \r\n    /** The target IDs that were added/removed/set. */\r\n    targetIds, \r\n    /**\r\n     * An opaque, server-assigned token that allows watching a target to be\r\n     * resumed after disconnecting without retransmitting all the data that\r\n     * matches the target. The resume token essentially identifies a point in\r\n     * time from which the server should resume sending results.\r\n     */\r\n    resumeToken = ByteString.EMPTY_BYTE_STRING, \r\n    /** An RPC error indicating why the watch failed. */\r\n    cause = null) {\r\n        this.state = state;\r\n        this.targetIds = targetIds;\r\n        this.resumeToken = resumeToken;\r\n        this.cause = cause;\r\n    }\r\n}\r\n/** Tracks the internal state of a Watch target. */\r\nclass TargetState {\r\n    constructor() {\r\n        /**\r\n         * The number of pending responses (adds or removes) that we are waiting on.\r\n         * We only consider targets active that have no pending responses.\r\n         */\r\n        this.pendingResponses = 0;\r\n        /**\r\n         * Keeps track of the document changes since the last raised snapshot.\r\n         *\r\n         * These changes are continuously updated as we receive document updates and\r\n         * always reflect the current set of changes against the last issued snapshot.\r\n         */\r\n        this.documentChanges = snapshotChangesMap();\r\n        /** See public getters for explanations of these fields. */\r\n        this._resumeToken = ByteString.EMPTY_BYTE_STRING;\r\n        this._current = false;\r\n        /**\r\n         * Whether this target state should be included in the next snapshot. We\r\n         * initialize to true so that newly-added targets are included in the next\r\n         * RemoteEvent.\r\n         */\r\n        this._hasPendingChanges = true;\r\n    }\r\n    /**\r\n     * Whether this target has been marked 'current'.\r\n     *\r\n     * 'Current' has special meaning in the RPC protocol: It implies that the\r\n     * Watch backend has sent us all changes up to the point at which the target\r\n     * was added and that the target is consistent with the rest of the watch\r\n     * stream.\r\n     */\r\n    get current() {\r\n        return this._current;\r\n    }\r\n    /** The last resume token sent to us for this target. */\r\n    get resumeToken() {\r\n        return this._resumeToken;\r\n    }\r\n    /** Whether this target has pending target adds or target removes. */\r\n    get isPending() {\r\n        return this.pendingResponses !== 0;\r\n    }\r\n    /** Whether we have modified any state that should trigger a snapshot. */\r\n    get hasPendingChanges() {\r\n        return this._hasPendingChanges;\r\n    }\r\n    /**\r\n     * Applies the resume token to the TargetChange, but only when it has a new\r\n     * value. Empty resumeTokens are discarded.\r\n     */\r\n    updateResumeToken(resumeToken) {\r\n        if (resumeToken.approximateByteSize() > 0) {\r\n            this._hasPendingChanges = true;\r\n            this._resumeToken = resumeToken;\r\n        }\r\n    }\r\n    /**\r\n     * Creates a target change from the current set of changes.\r\n     *\r\n     * To reset the document changes after raising this snapshot, call\r\n     * `clearPendingChanges()`.\r\n     */\r\n    toTargetChange() {\r\n        let addedDocuments = documentKeySet();\r\n        let modifiedDocuments = documentKeySet();\r\n        let removedDocuments = documentKeySet();\r\n        this.documentChanges.forEach((key, changeType) => {\r\n            switch (changeType) {\r\n                case 0 /* Added */:\r\n                    addedDocuments = addedDocuments.add(key);\r\n                    break;\r\n                case 2 /* Modified */:\r\n                    modifiedDocuments = modifiedDocuments.add(key);\r\n                    break;\r\n                case 1 /* Removed */:\r\n                    removedDocuments = removedDocuments.add(key);\r\n                    break;\r\n                default:\r\n                    fail();\r\n            }\r\n        });\r\n        return new TargetChange(this._resumeToken, this._current, addedDocuments, modifiedDocuments, removedDocuments);\r\n    }\r\n    /**\r\n     * Resets the document changes and sets `hasPendingChanges` to false.\r\n     */\r\n    clearPendingChanges() {\r\n        this._hasPendingChanges = false;\r\n        this.documentChanges = snapshotChangesMap();\r\n    }\r\n    addDocumentChange(key, changeType) {\r\n        this._hasPendingChanges = true;\r\n        this.documentChanges = this.documentChanges.insert(key, changeType);\r\n    }\r\n    removeDocumentChange(key) {\r\n        this._hasPendingChanges = true;\r\n        this.documentChanges = this.documentChanges.remove(key);\r\n    }\r\n    recordPendingTargetRequest() {\r\n        this.pendingResponses += 1;\r\n    }\r\n    recordTargetResponse() {\r\n        this.pendingResponses -= 1;\r\n    }\r\n    markCurrent() {\r\n        this._hasPendingChanges = true;\r\n        this._current = true;\r\n    }\r\n}\r\nconst LOG_TAG$g = 'WatchChangeAggregator';\r\n/**\r\n * A helper class to accumulate watch changes into a RemoteEvent.\r\n */\r\nclass WatchChangeAggregator {\r\n    constructor(metadataProvider) {\r\n        this.metadataProvider = metadataProvider;\r\n        /** The internal state of all tracked targets. */\r\n        this.targetStates = new Map();\r\n        /** Keeps track of the documents to update since the last raised snapshot. */\r\n        this.pendingDocumentUpdates = mutableDocumentMap();\r\n        /** A mapping of document keys to their set of target IDs. */\r\n        this.pendingDocumentTargetMapping = documentTargetMap();\r\n        /**\r\n         * A list of targets with existence filter mismatches. These targets are\r\n         * known to be inconsistent and their listens needs to be re-established by\r\n         * RemoteStore.\r\n         */\r\n        this.pendingTargetResets = new SortedSet(primitiveComparator);\r\n    }\r\n    /**\r\n     * Processes and adds the DocumentWatchChange to the current set of changes.\r\n     */\r\n    handleDocumentChange(docChange) {\r\n        for (const targetId of docChange.updatedTargetIds) {\r\n            if (docChange.newDoc && docChange.newDoc.isFoundDocument()) {\r\n                this.addDocumentToTarget(targetId, docChange.newDoc);\r\n            }\r\n            else {\r\n                this.removeDocumentFromTarget(targetId, docChange.key, docChange.newDoc);\r\n            }\r\n        }\r\n        for (const targetId of docChange.removedTargetIds) {\r\n            this.removeDocumentFromTarget(targetId, docChange.key, docChange.newDoc);\r\n        }\r\n    }\r\n    /** Processes and adds the WatchTargetChange to the current set of changes. */\r\n    handleTargetChange(targetChange) {\r\n        this.forEachTarget(targetChange, targetId => {\r\n            const targetState = this.ensureTargetState(targetId);\r\n            switch (targetChange.state) {\r\n                case 0 /* NoChange */:\r\n                    if (this.isActiveTarget(targetId)) {\r\n                        targetState.updateResumeToken(targetChange.resumeToken);\r\n                    }\r\n                    break;\r\n                case 1 /* Added */:\r\n                    // We need to decrement the number of pending acks needed from watch\r\n                    // for this targetId.\r\n                    targetState.recordTargetResponse();\r\n                    if (!targetState.isPending) {\r\n                        // We have a freshly added target, so we need to reset any state\r\n                        // that we had previously. This can happen e.g. when remove and add\r\n                        // back a target for existence filter mismatches.\r\n                        targetState.clearPendingChanges();\r\n                    }\r\n                    targetState.updateResumeToken(targetChange.resumeToken);\r\n                    break;\r\n                case 2 /* Removed */:\r\n                    // We need to keep track of removed targets to we can post-filter and\r\n                    // remove any target changes.\r\n                    // We need to decrement the number of pending acks needed from watch\r\n                    // for this targetId.\r\n                    targetState.recordTargetResponse();\r\n                    if (!targetState.isPending) {\r\n                        this.removeTarget(targetId);\r\n                    }\r\n                    break;\r\n                case 3 /* Current */:\r\n                    if (this.isActiveTarget(targetId)) {\r\n                        targetState.markCurrent();\r\n                        targetState.updateResumeToken(targetChange.resumeToken);\r\n                    }\r\n                    break;\r\n                case 4 /* Reset */:\r\n                    if (this.isActiveTarget(targetId)) {\r\n                        // Reset the target and synthesizes removes for all existing\r\n                        // documents. The backend will re-add any documents that still\r\n                        // match the target before it sends the next global snapshot.\r\n                        this.resetTarget(targetId);\r\n                        targetState.updateResumeToken(targetChange.resumeToken);\r\n                    }\r\n                    break;\r\n                default:\r\n                    fail();\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * Iterates over all targetIds that the watch change applies to: either the\r\n     * targetIds explicitly listed in the change or the targetIds of all currently\r\n     * active targets.\r\n     */\r\n    forEachTarget(targetChange, fn) {\r\n        if (targetChange.targetIds.length > 0) {\r\n            targetChange.targetIds.forEach(fn);\r\n        }\r\n        else {\r\n            this.targetStates.forEach((_, targetId) => {\r\n                if (this.isActiveTarget(targetId)) {\r\n                    fn(targetId);\r\n                }\r\n            });\r\n        }\r\n    }\r\n    /**\r\n     * Handles existence filters and synthesizes deletes for filter mismatches.\r\n     * Targets that are invalidated by filter mismatches are added to\r\n     * `pendingTargetResets`.\r\n     */\r\n    handleExistenceFilter(watchChange) {\r\n        const targetId = watchChange.targetId;\r\n        const expectedCount = watchChange.existenceFilter.count;\r\n        const targetData = this.targetDataForActiveTarget(targetId);\r\n        if (targetData) {\r\n            const target = targetData.target;\r\n            if (targetIsDocumentTarget(target)) {\r\n                if (expectedCount === 0) {\r\n                    // The existence filter told us the document does not exist. We deduce\r\n                    // that this document does not exist and apply a deleted document to\r\n                    // our updates. Without applying this deleted document there might be\r\n                    // another query that will raise this document as part of a snapshot\r\n                    // until it is resolved, essentially exposing inconsistency between\r\n                    // queries.\r\n                    const key = new DocumentKey(target.path);\r\n                    this.removeDocumentFromTarget(targetId, key, MutableDocument.newNoDocument(key, SnapshotVersion.min()));\r\n                }\r\n                else {\r\n                    hardAssert(expectedCount === 1);\r\n                }\r\n            }\r\n            else {\r\n                const currentSize = this.getCurrentDocumentCountForTarget(targetId);\r\n                if (currentSize !== expectedCount) {\r\n                    // Existence filter mismatch: We reset the mapping and raise a new\r\n                    // snapshot with `isFromCache:true`.\r\n                    this.resetTarget(targetId);\r\n                    this.pendingTargetResets = this.pendingTargetResets.add(targetId);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    /**\r\n     * Converts the currently accumulated state into a remote event at the\r\n     * provided snapshot version. Resets the accumulated changes before returning.\r\n     */\r\n    createRemoteEvent(snapshotVersion) {\r\n        const targetChanges = new Map();\r\n        this.targetStates.forEach((targetState, targetId) => {\r\n            const targetData = this.targetDataForActiveTarget(targetId);\r\n            if (targetData) {\r\n                if (targetState.current && targetIsDocumentTarget(targetData.target)) {\r\n                    // Document queries for document that don't exist can produce an empty\r\n                    // result set. To update our local cache, we synthesize a document\r\n                    // delete if we have not previously received the document. This\r\n                    // resolves the limbo state of the document, removing it from\r\n                    // limboDocumentRefs.\r\n                    //\r\n                    // TODO(dimond): Ideally we would have an explicit lookup target\r\n                    // instead resulting in an explicit delete message and we could\r\n                    // remove this special logic.\r\n                    const key = new DocumentKey(targetData.target.path);\r\n                    if (this.pendingDocumentUpdates.get(key) === null &&\r\n                        !this.targetContainsDocument(targetId, key)) {\r\n                        this.removeDocumentFromTarget(targetId, key, MutableDocument.newNoDocument(key, snapshotVersion));\r\n                    }\r\n                }\r\n                if (targetState.hasPendingChanges) {\r\n                    targetChanges.set(targetId, targetState.toTargetChange());\r\n                    targetState.clearPendingChanges();\r\n                }\r\n            }\r\n        });\r\n        let resolvedLimboDocuments = documentKeySet();\r\n        // We extract the set of limbo-only document updates as the GC logic\r\n        // special-cases documents that do not appear in the target cache.\r\n        //\r\n        // TODO(gsoltis): Expand on this comment once GC is available in the JS\r\n        // client.\r\n        this.pendingDocumentTargetMapping.forEach((key, targets) => {\r\n            let isOnlyLimboTarget = true;\r\n            targets.forEachWhile(targetId => {\r\n                const targetData = this.targetDataForActiveTarget(targetId);\r\n                if (targetData &&\r\n                    targetData.purpose !== 2 /* LimboResolution */) {\r\n                    isOnlyLimboTarget = false;\r\n                    return false;\r\n                }\r\n                return true;\r\n            });\r\n            if (isOnlyLimboTarget) {\r\n                resolvedLimboDocuments = resolvedLimboDocuments.add(key);\r\n            }\r\n        });\r\n        this.pendingDocumentUpdates.forEach((_, doc) => doc.setReadTime(snapshotVersion));\r\n        const remoteEvent = new RemoteEvent(snapshotVersion, targetChanges, this.pendingTargetResets, this.pendingDocumentUpdates, resolvedLimboDocuments);\r\n        this.pendingDocumentUpdates = mutableDocumentMap();\r\n        this.pendingDocumentTargetMapping = documentTargetMap();\r\n        this.pendingTargetResets = new SortedSet(primitiveComparator);\r\n        return remoteEvent;\r\n    }\r\n    /**\r\n     * Adds the provided document to the internal list of document updates and\r\n     * its document key to the given target's mapping.\r\n     */\r\n    // Visible for testing.\r\n    addDocumentToTarget(targetId, document) {\r\n        if (!this.isActiveTarget(targetId)) {\r\n            return;\r\n        }\r\n        const changeType = this.targetContainsDocument(targetId, document.key)\r\n            ? 2 /* Modified */\r\n            : 0 /* Added */;\r\n        const targetState = this.ensureTargetState(targetId);\r\n        targetState.addDocumentChange(document.key, changeType);\r\n        this.pendingDocumentUpdates = this.pendingDocumentUpdates.insert(document.key, document);\r\n        this.pendingDocumentTargetMapping =\r\n            this.pendingDocumentTargetMapping.insert(document.key, this.ensureDocumentTargetMapping(document.key).add(targetId));\r\n    }\r\n    /**\r\n     * Removes the provided document from the target mapping. If the\r\n     * document no longer matches the target, but the document's state is still\r\n     * known (e.g. we know that the document was deleted or we received the change\r\n     * that caused the filter mismatch), the new document can be provided\r\n     * to update the remote document cache.\r\n     */\r\n    // Visible for testing.\r\n    removeDocumentFromTarget(targetId, key, updatedDocument) {\r\n        if (!this.isActiveTarget(targetId)) {\r\n            return;\r\n        }\r\n        const targetState = this.ensureTargetState(targetId);\r\n        if (this.targetContainsDocument(targetId, key)) {\r\n            targetState.addDocumentChange(key, 1 /* Removed */);\r\n        }\r\n        else {\r\n            // The document may have entered and left the target before we raised a\r\n            // snapshot, so we can just ignore the change.\r\n            targetState.removeDocumentChange(key);\r\n        }\r\n        this.pendingDocumentTargetMapping =\r\n            this.pendingDocumentTargetMapping.insert(key, this.ensureDocumentTargetMapping(key).delete(targetId));\r\n        if (updatedDocument) {\r\n            this.pendingDocumentUpdates = this.pendingDocumentUpdates.insert(key, updatedDocument);\r\n        }\r\n    }\r\n    removeTarget(targetId) {\r\n        this.targetStates.delete(targetId);\r\n    }\r\n    /**\r\n     * Returns the current count of documents in the target. This includes both\r\n     * the number of documents that the LocalStore considers to be part of the\r\n     * target as well as any accumulated changes.\r\n     */\r\n    getCurrentDocumentCountForTarget(targetId) {\r\n        const targetState = this.ensureTargetState(targetId);\r\n        const targetChange = targetState.toTargetChange();\r\n        return (this.metadataProvider.getRemoteKeysForTarget(targetId).size +\r\n            targetChange.addedDocuments.size -\r\n            targetChange.removedDocuments.size);\r\n    }\r\n    /**\r\n     * Increment the number of acks needed from watch before we can consider the\r\n     * server to be 'in-sync' with the client's active targets.\r\n     */\r\n    recordPendingTargetRequest(targetId) {\r\n        // For each request we get we need to record we need a response for it.\r\n        const targetState = this.ensureTargetState(targetId);\r\n        targetState.recordPendingTargetRequest();\r\n    }\r\n    ensureTargetState(targetId) {\r\n        let result = this.targetStates.get(targetId);\r\n        if (!result) {\r\n            result = new TargetState();\r\n            this.targetStates.set(targetId, result);\r\n        }\r\n        return result;\r\n    }\r\n    ensureDocumentTargetMapping(key) {\r\n        let targetMapping = this.pendingDocumentTargetMapping.get(key);\r\n        if (!targetMapping) {\r\n            targetMapping = new SortedSet(primitiveComparator);\r\n            this.pendingDocumentTargetMapping =\r\n                this.pendingDocumentTargetMapping.insert(key, targetMapping);\r\n        }\r\n        return targetMapping;\r\n    }\r\n    /**\r\n     * Verifies that the user is still interested in this target (by calling\r\n     * `getTargetDataForTarget()`) and that we are not waiting for pending ADDs\r\n     * from watch.\r\n     */\r\n    isActiveTarget(targetId) {\r\n        const targetActive = this.targetDataForActiveTarget(targetId) !== null;\r\n        if (!targetActive) {\r\n            logDebug(LOG_TAG$g, 'Detected inactive target', targetId);\r\n        }\r\n        return targetActive;\r\n    }\r\n    /**\r\n     * Returns the TargetData for an active target (i.e. a target that the user\r\n     * is still interested in that has no outstanding target change requests).\r\n     */\r\n    targetDataForActiveTarget(targetId) {\r\n        const targetState = this.targetStates.get(targetId);\r\n        return targetState && targetState.isPending\r\n            ? null\r\n            : this.metadataProvider.getTargetDataForTarget(targetId);\r\n    }\r\n    /**\r\n     * Resets the state of a Watch target to its initial state (e.g. sets\r\n     * 'current' to false, clears the resume token and removes its target mapping\r\n     * from all documents).\r\n     */\r\n    resetTarget(targetId) {\r\n        this.targetStates.set(targetId, new TargetState());\r\n        // Trigger removal for any documents currently mapped to this target.\r\n        // These removals will be part of the initial snapshot if Watch does not\r\n        // resend these documents.\r\n        const existingKeys = this.metadataProvider.getRemoteKeysForTarget(targetId);\r\n        existingKeys.forEach(key => {\r\n            this.removeDocumentFromTarget(targetId, key, /*updatedDocument=*/ null);\r\n        });\r\n    }\r\n    /**\r\n     * Returns whether the LocalStore considers the document to be part of the\r\n     * specified target.\r\n     */\r\n    targetContainsDocument(targetId, key) {\r\n        const existingKeys = this.metadataProvider.getRemoteKeysForTarget(targetId);\r\n        return existingKeys.has(key);\r\n    }\r\n}\r\nfunction documentTargetMap() {\r\n    return new SortedMap(DocumentKey.comparator);\r\n}\r\nfunction snapshotChangesMap() {\r\n    return new SortedMap(DocumentKey.comparator);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst DIRECTIONS = (() => {\r\n    const dirs = {};\r\n    dirs[\"asc\" /* ASCENDING */] = 'ASCENDING';\r\n    dirs[\"desc\" /* DESCENDING */] = 'DESCENDING';\r\n    return dirs;\r\n})();\r\nconst OPERATORS = (() => {\r\n    const ops = {};\r\n    ops[\"<\" /* LESS_THAN */] = 'LESS_THAN';\r\n    ops[\"<=\" /* LESS_THAN_OR_EQUAL */] = 'LESS_THAN_OR_EQUAL';\r\n    ops[\">\" /* GREATER_THAN */] = 'GREATER_THAN';\r\n    ops[\">=\" /* GREATER_THAN_OR_EQUAL */] = 'GREATER_THAN_OR_EQUAL';\r\n    ops[\"==\" /* EQUAL */] = 'EQUAL';\r\n    ops[\"!=\" /* NOT_EQUAL */] = 'NOT_EQUAL';\r\n    ops[\"array-contains\" /* ARRAY_CONTAINS */] = 'ARRAY_CONTAINS';\r\n    ops[\"in\" /* IN */] = 'IN';\r\n    ops[\"not-in\" /* NOT_IN */] = 'NOT_IN';\r\n    ops[\"array-contains-any\" /* ARRAY_CONTAINS_ANY */] = 'ARRAY_CONTAINS_ANY';\r\n    return ops;\r\n})();\r\nfunction assertPresent(value, description) {\r\n}\r\n/**\r\n * This class generates JsonObject values for the Datastore API suitable for\r\n * sending to either GRPC stub methods or via the JSON/HTTP REST API.\r\n *\r\n * The serializer supports both Protobuf.js and Proto3 JSON formats. By\r\n * setting `useProto3Json` to true, the serializer will use the Proto3 JSON\r\n * format.\r\n *\r\n * For a description of the Proto3 JSON format check\r\n * https://developers.google.com/protocol-buffers/docs/proto3#json\r\n *\r\n * TODO(klimt): We can remove the databaseId argument if we keep the full\r\n * resource name in documents.\r\n */\r\nclass JsonProtoSerializer {\r\n    constructor(databaseId, useProto3Json) {\r\n        this.databaseId = databaseId;\r\n        this.useProto3Json = useProto3Json;\r\n    }\r\n}\r\nfunction fromRpcStatus(status) {\r\n    const code = status.code === undefined ? Code.UNKNOWN : mapCodeFromRpcCode(status.code);\r\n    return new FirestoreError(code, status.message || '');\r\n}\r\n/**\r\n * Returns a value for a number (or null) that's appropriate to put into\r\n * a google.protobuf.Int32Value proto.\r\n * DO NOT USE THIS FOR ANYTHING ELSE.\r\n * This method cheats. It's typed as returning \"number\" because that's what\r\n * our generated proto interfaces say Int32Value must be. But GRPC actually\r\n * expects a { value: <number> } struct.\r\n */\r\nfunction toInt32Proto(serializer, val) {\r\n    if (serializer.useProto3Json || isNullOrUndefined(val)) {\r\n        return val;\r\n    }\r\n    else {\r\n        return { value: val };\r\n    }\r\n}\r\n/**\r\n * Returns a number (or null) from a google.protobuf.Int32Value proto.\r\n */\r\nfunction fromInt32Proto(val) {\r\n    let result;\r\n    if (typeof val === 'object') {\r\n        result = val.value;\r\n    }\r\n    else {\r\n        result = val;\r\n    }\r\n    return isNullOrUndefined(result) ? null : result;\r\n}\r\n/**\r\n * Returns a value for a Date that's appropriate to put into a proto.\r\n */\r\nfunction toTimestamp(serializer, timestamp) {\r\n    if (serializer.useProto3Json) {\r\n        // Serialize to ISO-8601 date format, but with full nano resolution.\r\n        // Since JS Date has only millis, let's only use it for the seconds and\r\n        // then manually add the fractions to the end.\r\n        const jsDateStr = new Date(timestamp.seconds * 1000).toISOString();\r\n        // Remove .xxx frac part and Z in the end.\r\n        const strUntilSeconds = jsDateStr.replace(/\\.\\d*/, '').replace('Z', '');\r\n        // Pad the fraction out to 9 digits (nanos).\r\n        const nanoStr = ('000000000' + timestamp.nanoseconds).slice(-9);\r\n        return `${strUntilSeconds}.${nanoStr}Z`;\r\n    }\r\n    else {\r\n        return {\r\n            seconds: '' + timestamp.seconds,\r\n            nanos: timestamp.nanoseconds\r\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n        };\r\n    }\r\n}\r\nfunction fromTimestamp(date) {\r\n    const timestamp = normalizeTimestamp(date);\r\n    return new Timestamp(timestamp.seconds, timestamp.nanos);\r\n}\r\n/**\r\n * Returns a value for bytes that's appropriate to put in a proto.\r\n *\r\n * Visible for testing.\r\n */\r\nfunction toBytes(serializer, bytes) {\r\n    if (serializer.useProto3Json) {\r\n        return bytes.toBase64();\r\n    }\r\n    else {\r\n        return bytes.toUint8Array();\r\n    }\r\n}\r\n/**\r\n * Returns a ByteString based on the proto string value.\r\n */\r\nfunction fromBytes(serializer, value) {\r\n    if (serializer.useProto3Json) {\r\n        hardAssert(value === undefined || typeof value === 'string');\r\n        return ByteString.fromBase64String(value ? value : '');\r\n    }\r\n    else {\r\n        hardAssert(value === undefined || value instanceof Uint8Array);\r\n        return ByteString.fromUint8Array(value ? value : new Uint8Array());\r\n    }\r\n}\r\nfunction toVersion(serializer, version) {\r\n    return toTimestamp(serializer, version.toTimestamp());\r\n}\r\nfunction fromVersion(version) {\r\n    hardAssert(!!version);\r\n    return SnapshotVersion.fromTimestamp(fromTimestamp(version));\r\n}\r\nfunction toResourceName(databaseId, path) {\r\n    return fullyQualifiedPrefixPath(databaseId)\r\n        .child('documents')\r\n        .child(path)\r\n        .canonicalString();\r\n}\r\nfunction fromResourceName(name) {\r\n    const resource = ResourcePath.fromString(name);\r\n    hardAssert(isValidResourceName(resource));\r\n    return resource;\r\n}\r\nfunction toName(serializer, key) {\r\n    return toResourceName(serializer.databaseId, key.path);\r\n}\r\nfunction fromName(serializer, name) {\r\n    const resource = fromResourceName(name);\r\n    if (resource.get(1) !== serializer.databaseId.projectId) {\r\n        throw new FirestoreError(Code.INVALID_ARGUMENT, 'Tried to deserialize key from different project: ' +\r\n            resource.get(1) +\r\n            ' vs ' +\r\n            serializer.databaseId.projectId);\r\n    }\r\n    if (resource.get(3) !== serializer.databaseId.database) {\r\n        throw new FirestoreError(Code.INVALID_ARGUMENT, 'Tried to deserialize key from different database: ' +\r\n            resource.get(3) +\r\n            ' vs ' +\r\n            serializer.databaseId.database);\r\n    }\r\n    return new DocumentKey(extractLocalPathFromResourceName(resource));\r\n}\r\nfunction toQueryPath(serializer, path) {\r\n    return toResourceName(serializer.databaseId, path);\r\n}\r\nfunction fromQueryPath(name) {\r\n    const resourceName = fromResourceName(name);\r\n    // In v1beta1 queries for collections at the root did not have a trailing\r\n    // \"/documents\". In v1 all resource paths contain \"/documents\". Preserve the\r\n    // ability to read the v1beta1 form for compatibility with queries persisted\r\n    // in the local target cache.\r\n    if (resourceName.length === 4) {\r\n        return ResourcePath.emptyPath();\r\n    }\r\n    return extractLocalPathFromResourceName(resourceName);\r\n}\r\nfunction getEncodedDatabaseId(serializer) {\r\n    const path = new ResourcePath([\r\n        'projects',\r\n        serializer.databaseId.projectId,\r\n        'databases',\r\n        serializer.databaseId.database\r\n    ]);\r\n    return path.canonicalString();\r\n}\r\nfunction fullyQualifiedPrefixPath(databaseId) {\r\n    return new ResourcePath([\r\n        'projects',\r\n        databaseId.projectId,\r\n        'databases',\r\n        databaseId.database\r\n    ]);\r\n}\r\nfunction extractLocalPathFromResourceName(resourceName) {\r\n    hardAssert(resourceName.length > 4 && resourceName.get(4) === 'documents');\r\n    return resourceName.popFirst(5);\r\n}\r\n/** Creates a Document proto from key and fields (but no create/update time) */\r\nfunction toMutationDocument(serializer, key, fields) {\r\n    return {\r\n        name: toName(serializer, key),\r\n        fields: fields.value.mapValue.fields\r\n    };\r\n}\r\nfunction toDocument(serializer, document) {\r\n    return {\r\n        name: toName(serializer, document.key),\r\n        fields: document.data.value.mapValue.fields,\r\n        updateTime: toTimestamp(serializer, document.version.toTimestamp())\r\n    };\r\n}\r\nfunction fromDocument(serializer, document, hasCommittedMutations) {\r\n    const key = fromName(serializer, document.name);\r\n    const version = fromVersion(document.updateTime);\r\n    const data = new ObjectValue({ mapValue: { fields: document.fields } });\r\n    const result = MutableDocument.newFoundDocument(key, version, data);\r\n    if (hasCommittedMutations) {\r\n        result.setHasCommittedMutations();\r\n    }\r\n    return hasCommittedMutations ? result.setHasCommittedMutations() : result;\r\n}\r\nfunction fromFound(serializer, doc) {\r\n    hardAssert(!!doc.found);\r\n    assertPresent(doc.found.name);\r\n    assertPresent(doc.found.updateTime);\r\n    const key = fromName(serializer, doc.found.name);\r\n    const version = fromVersion(doc.found.updateTime);\r\n    const data = new ObjectValue({ mapValue: { fields: doc.found.fields } });\r\n    return MutableDocument.newFoundDocument(key, version, data);\r\n}\r\nfunction fromMissing(serializer, result) {\r\n    hardAssert(!!result.missing);\r\n    hardAssert(!!result.readTime);\r\n    const key = fromName(serializer, result.missing);\r\n    const version = fromVersion(result.readTime);\r\n    return MutableDocument.newNoDocument(key, version);\r\n}\r\nfunction fromBatchGetDocumentsResponse(serializer, result) {\r\n    if ('found' in result) {\r\n        return fromFound(serializer, result);\r\n    }\r\n    else if ('missing' in result) {\r\n        return fromMissing(serializer, result);\r\n    }\r\n    return fail();\r\n}\r\nfunction fromWatchChange(serializer, change) {\r\n    let watchChange;\r\n    if ('targetChange' in change) {\r\n        assertPresent(change.targetChange);\r\n        // proto3 default value is unset in JSON (undefined), so use 'NO_CHANGE'\r\n        // if unset\r\n        const state = fromWatchTargetChangeState(change.targetChange.targetChangeType || 'NO_CHANGE');\r\n        const targetIds = change.targetChange.targetIds || [];\r\n        const resumeToken = fromBytes(serializer, change.targetChange.resumeToken);\r\n        const causeProto = change.targetChange.cause;\r\n        const cause = causeProto && fromRpcStatus(causeProto);\r\n        watchChange = new WatchTargetChange(state, targetIds, resumeToken, cause || null);\r\n    }\r\n    else if ('documentChange' in change) {\r\n        assertPresent(change.documentChange);\r\n        const entityChange = change.documentChange;\r\n        assertPresent(entityChange.document);\r\n        assertPresent(entityChange.document.name);\r\n        assertPresent(entityChange.document.updateTime);\r\n        const key = fromName(serializer, entityChange.document.name);\r\n        const version = fromVersion(entityChange.document.updateTime);\r\n        const data = new ObjectValue({\r\n            mapValue: { fields: entityChange.document.fields }\r\n        });\r\n        const doc = MutableDocument.newFoundDocument(key, version, data);\r\n        const updatedTargetIds = entityChange.targetIds || [];\r\n        const removedTargetIds = entityChange.removedTargetIds || [];\r\n        watchChange = new DocumentWatchChange(updatedTargetIds, removedTargetIds, doc.key, doc);\r\n    }\r\n    else if ('documentDelete' in change) {\r\n        assertPresent(change.documentDelete);\r\n        const docDelete = change.documentDelete;\r\n        assertPresent(docDelete.document);\r\n        const key = fromName(serializer, docDelete.document);\r\n        const version = docDelete.readTime\r\n            ? fromVersion(docDelete.readTime)\r\n            : SnapshotVersion.min();\r\n        const doc = MutableDocument.newNoDocument(key, version);\r\n        const removedTargetIds = docDelete.removedTargetIds || [];\r\n        watchChange = new DocumentWatchChange([], removedTargetIds, doc.key, doc);\r\n    }\r\n    else if ('documentRemove' in change) {\r\n        assertPresent(change.documentRemove);\r\n        const docRemove = change.documentRemove;\r\n        assertPresent(docRemove.document);\r\n        const key = fromName(serializer, docRemove.document);\r\n        const removedTargetIds = docRemove.removedTargetIds || [];\r\n        watchChange = new DocumentWatchChange([], removedTargetIds, key, null);\r\n    }\r\n    else if ('filter' in change) {\r\n        // TODO(dimond): implement existence filter parsing with strategy.\r\n        assertPresent(change.filter);\r\n        const filter = change.filter;\r\n        assertPresent(filter.targetId);\r\n        const count = filter.count || 0;\r\n        const existenceFilter = new ExistenceFilter(count);\r\n        const targetId = filter.targetId;\r\n        watchChange = new ExistenceFilterChange(targetId, existenceFilter);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n    return watchChange;\r\n}\r\nfunction fromWatchTargetChangeState(state) {\r\n    if (state === 'NO_CHANGE') {\r\n        return 0 /* NoChange */;\r\n    }\r\n    else if (state === 'ADD') {\r\n        return 1 /* Added */;\r\n    }\r\n    else if (state === 'REMOVE') {\r\n        return 2 /* Removed */;\r\n    }\r\n    else if (state === 'CURRENT') {\r\n        return 3 /* Current */;\r\n    }\r\n    else if (state === 'RESET') {\r\n        return 4 /* Reset */;\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction versionFromListenResponse(change) {\r\n    // We have only reached a consistent snapshot for the entire stream if there\r\n    // is a read_time set and it applies to all targets (i.e. the list of\r\n    // targets is empty). The backend is guaranteed to send such responses.\r\n    if (!('targetChange' in change)) {\r\n        return SnapshotVersion.min();\r\n    }\r\n    const targetChange = change.targetChange;\r\n    if (targetChange.targetIds && targetChange.targetIds.length) {\r\n        return SnapshotVersion.min();\r\n    }\r\n    if (!targetChange.readTime) {\r\n        return SnapshotVersion.min();\r\n    }\r\n    return fromVersion(targetChange.readTime);\r\n}\r\nfunction toMutation(serializer, mutation) {\r\n    let result;\r\n    if (mutation instanceof SetMutation) {\r\n        result = {\r\n            update: toMutationDocument(serializer, mutation.key, mutation.value)\r\n        };\r\n    }\r\n    else if (mutation instanceof DeleteMutation) {\r\n        result = { delete: toName(serializer, mutation.key) };\r\n    }\r\n    else if (mutation instanceof PatchMutation) {\r\n        result = {\r\n            update: toMutationDocument(serializer, mutation.key, mutation.data),\r\n            updateMask: toDocumentMask(mutation.fieldMask)\r\n        };\r\n    }\r\n    else if (mutation instanceof VerifyMutation) {\r\n        result = {\r\n            verify: toName(serializer, mutation.key)\r\n        };\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n    if (mutation.fieldTransforms.length > 0) {\r\n        result.updateTransforms = mutation.fieldTransforms.map(transform => toFieldTransform(serializer, transform));\r\n    }\r\n    if (!mutation.precondition.isNone) {\r\n        result.currentDocument = toPrecondition(serializer, mutation.precondition);\r\n    }\r\n    return result;\r\n}\r\nfunction fromMutation(serializer, proto) {\r\n    const precondition = proto.currentDocument\r\n        ? fromPrecondition(proto.currentDocument)\r\n        : Precondition.none();\r\n    const fieldTransforms = proto.updateTransforms\r\n        ? proto.updateTransforms.map(transform => fromFieldTransform(serializer, transform))\r\n        : [];\r\n    if (proto.update) {\r\n        assertPresent(proto.update.name);\r\n        const key = fromName(serializer, proto.update.name);\r\n        const value = new ObjectValue({\r\n            mapValue: { fields: proto.update.fields }\r\n        });\r\n        if (proto.updateMask) {\r\n            const fieldMask = fromDocumentMask(proto.updateMask);\r\n            return new PatchMutation(key, value, fieldMask, precondition, fieldTransforms);\r\n        }\r\n        else {\r\n            return new SetMutation(key, value, precondition, fieldTransforms);\r\n        }\r\n    }\r\n    else if (proto.delete) {\r\n        const key = fromName(serializer, proto.delete);\r\n        return new DeleteMutation(key, precondition);\r\n    }\r\n    else if (proto.verify) {\r\n        const key = fromName(serializer, proto.verify);\r\n        return new VerifyMutation(key, precondition);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction toPrecondition(serializer, precondition) {\r\n    if (precondition.updateTime !== undefined) {\r\n        return {\r\n            updateTime: toVersion(serializer, precondition.updateTime)\r\n        };\r\n    }\r\n    else if (precondition.exists !== undefined) {\r\n        return { exists: precondition.exists };\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction fromPrecondition(precondition) {\r\n    if (precondition.updateTime !== undefined) {\r\n        return Precondition.updateTime(fromVersion(precondition.updateTime));\r\n    }\r\n    else if (precondition.exists !== undefined) {\r\n        return Precondition.exists(precondition.exists);\r\n    }\r\n    else {\r\n        return Precondition.none();\r\n    }\r\n}\r\nfunction fromWriteResult(proto, commitTime) {\r\n    // NOTE: Deletes don't have an updateTime.\r\n    let version = proto.updateTime\r\n        ? fromVersion(proto.updateTime)\r\n        : fromVersion(commitTime);\r\n    if (version.isEqual(SnapshotVersion.min())) {\r\n        // The Firestore Emulator currently returns an update time of 0 for\r\n        // deletes of non-existing documents (rather than null). This breaks the\r\n        // test \"get deleted doc while offline with source=cache\" as NoDocuments\r\n        // with version 0 are filtered by IndexedDb's RemoteDocumentCache.\r\n        // TODO(#2149): Remove this when Emulator is fixed\r\n        version = fromVersion(commitTime);\r\n    }\r\n    return new MutationResult(version, proto.transformResults || []);\r\n}\r\nfunction fromWriteResults(protos, commitTime) {\r\n    if (protos && protos.length > 0) {\r\n        hardAssert(commitTime !== undefined);\r\n        return protos.map(proto => fromWriteResult(proto, commitTime));\r\n    }\r\n    else {\r\n        return [];\r\n    }\r\n}\r\nfunction toFieldTransform(serializer, fieldTransform) {\r\n    const transform = fieldTransform.transform;\r\n    if (transform instanceof ServerTimestampTransform) {\r\n        return {\r\n            fieldPath: fieldTransform.field.canonicalString(),\r\n            setToServerValue: 'REQUEST_TIME'\r\n        };\r\n    }\r\n    else if (transform instanceof ArrayUnionTransformOperation) {\r\n        return {\r\n            fieldPath: fieldTransform.field.canonicalString(),\r\n            appendMissingElements: {\r\n                values: transform.elements\r\n            }\r\n        };\r\n    }\r\n    else if (transform instanceof ArrayRemoveTransformOperation) {\r\n        return {\r\n            fieldPath: fieldTransform.field.canonicalString(),\r\n            removeAllFromArray: {\r\n                values: transform.elements\r\n            }\r\n        };\r\n    }\r\n    else if (transform instanceof NumericIncrementTransformOperation) {\r\n        return {\r\n            fieldPath: fieldTransform.field.canonicalString(),\r\n            increment: transform.operand\r\n        };\r\n    }\r\n    else {\r\n        throw fail();\r\n    }\r\n}\r\nfunction fromFieldTransform(serializer, proto) {\r\n    let transform = null;\r\n    if ('setToServerValue' in proto) {\r\n        hardAssert(proto.setToServerValue === 'REQUEST_TIME');\r\n        transform = new ServerTimestampTransform();\r\n    }\r\n    else if ('appendMissingElements' in proto) {\r\n        const values = proto.appendMissingElements.values || [];\r\n        transform = new ArrayUnionTransformOperation(values);\r\n    }\r\n    else if ('removeAllFromArray' in proto) {\r\n        const values = proto.removeAllFromArray.values || [];\r\n        transform = new ArrayRemoveTransformOperation(values);\r\n    }\r\n    else if ('increment' in proto) {\r\n        transform = new NumericIncrementTransformOperation(serializer, proto.increment);\r\n    }\r\n    else {\r\n        fail();\r\n    }\r\n    const fieldPath = FieldPath$1.fromServerFormat(proto.fieldPath);\r\n    return new FieldTransform(fieldPath, transform);\r\n}\r\nfunction toDocumentsTarget(serializer, target) {\r\n    return { documents: [toQueryPath(serializer, target.path)] };\r\n}\r\nfunction fromDocumentsTarget(documentsTarget) {\r\n    const count = documentsTarget.documents.length;\r\n    hardAssert(count === 1);\r\n    const name = documentsTarget.documents[0];\r\n    return queryToTarget(newQueryForPath(fromQueryPath(name)));\r\n}\r\nfunction toQueryTarget(serializer, target) {\r\n    // Dissect the path into parent, collectionId, and optional key filter.\r\n    const result = { structuredQuery: {} };\r\n    const path = target.path;\r\n    if (target.collectionGroup !== null) {\r\n        result.parent = toQueryPath(serializer, path);\r\n        result.structuredQuery.from = [\r\n            {\r\n                collectionId: target.collectionGroup,\r\n                allDescendants: true\r\n            }\r\n        ];\r\n    }\r\n    else {\r\n        result.parent = toQueryPath(serializer, path.popLast());\r\n        result.structuredQuery.from = [{ collectionId: path.lastSegment() }];\r\n    }\r\n    const where = toFilter(target.filters);\r\n    if (where) {\r\n        result.structuredQuery.where = where;\r\n    }\r\n    const orderBy = toOrder(target.orderBy);\r\n    if (orderBy) {\r\n        result.structuredQuery.orderBy = orderBy;\r\n    }\r\n    const limit = toInt32Proto(serializer, target.limit);\r\n    if (limit !== null) {\r\n        result.structuredQuery.limit = limit;\r\n    }\r\n    if (target.startAt) {\r\n        result.structuredQuery.startAt = toStartAtCursor(target.startAt);\r\n    }\r\n    if (target.endAt) {\r\n        result.structuredQuery.endAt = toEndAtCursor(target.endAt);\r\n    }\r\n    return result;\r\n}\r\nfunction convertQueryTargetToQuery(target) {\r\n    let path = fromQueryPath(target.parent);\r\n    const query = target.structuredQuery;\r\n    const fromCount = query.from ? query.from.length : 0;\r\n    let collectionGroup = null;\r\n    if (fromCount > 0) {\r\n        hardAssert(fromCount === 1);\r\n        const from = query.from[0];\r\n        if (from.allDescendants) {\r\n            collectionGroup = from.collectionId;\r\n        }\r\n        else {\r\n            path = path.child(from.collectionId);\r\n        }\r\n    }\r\n    let filterBy = [];\r\n    if (query.where) {\r\n        filterBy = fromFilter(query.where);\r\n    }\r\n    let orderBy = [];\r\n    if (query.orderBy) {\r\n        orderBy = fromOrder(query.orderBy);\r\n    }\r\n    let limit = null;\r\n    if (query.limit) {\r\n        limit = fromInt32Proto(query.limit);\r\n    }\r\n    let startAt = null;\r\n    if (query.startAt) {\r\n        startAt = fromStartAtCursor(query.startAt);\r\n    }\r\n    let endAt = null;\r\n    if (query.endAt) {\r\n        endAt = fromEndAtCursor(query.endAt);\r\n    }\r\n    return newQuery(path, collectionGroup, orderBy, filterBy, limit, \"F\" /* First */, startAt, endAt);\r\n}\r\nfunction fromQueryTarget(target) {\r\n    return queryToTarget(convertQueryTargetToQuery(target));\r\n}\r\nfunction toListenRequestLabels(serializer, targetData) {\r\n    const value = toLabel(serializer, targetData.purpose);\r\n    if (value == null) {\r\n        return null;\r\n    }\r\n    else {\r\n        return {\r\n            'goog-listen-tags': value\r\n        };\r\n    }\r\n}\r\nfunction toLabel(serializer, purpose) {\r\n    switch (purpose) {\r\n        case 0 /* Listen */:\r\n            return null;\r\n        case 1 /* ExistenceFilterMismatch */:\r\n            return 'existence-filter-mismatch';\r\n        case 2 /* LimboResolution */:\r\n            return 'limbo-document';\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\nfunction toTarget(serializer, targetData) {\r\n    let result;\r\n    const target = targetData.target;\r\n    if (targetIsDocumentTarget(target)) {\r\n        result = { documents: toDocumentsTarget(serializer, target) };\r\n    }\r\n    else {\r\n        result = { query: toQueryTarget(serializer, target) };\r\n    }\r\n    result.targetId = targetData.targetId;\r\n    if (targetData.resumeToken.approximateByteSize() > 0) {\r\n        result.resumeToken = toBytes(serializer, targetData.resumeToken);\r\n    }\r\n    else if (targetData.snapshotVersion.compareTo(SnapshotVersion.min()) > 0) {\r\n        // TODO(wuandy): Consider removing above check because it is most likely true.\r\n        // Right now, many tests depend on this behaviour though (leaving min() out\r\n        // of serialization).\r\n        result.readTime = toTimestamp(serializer, targetData.snapshotVersion.toTimestamp());\r\n    }\r\n    return result;\r\n}\r\nfunction toFilter(filters) {\r\n    if (filters.length === 0) {\r\n        return;\r\n    }\r\n    const protos = filters.map(filter => {\r\n        return toUnaryOrFieldFilter(filter);\r\n    });\r\n    if (protos.length === 1) {\r\n        return protos[0];\r\n    }\r\n    return { compositeFilter: { op: 'AND', filters: protos } };\r\n}\r\nfunction fromFilter(filter) {\r\n    if (!filter) {\r\n        return [];\r\n    }\r\n    else if (filter.unaryFilter !== undefined) {\r\n        return [fromUnaryFilter(filter)];\r\n    }\r\n    else if (filter.fieldFilter !== undefined) {\r\n        return [fromFieldFilter(filter)];\r\n    }\r\n    else if (filter.compositeFilter !== undefined) {\r\n        return filter.compositeFilter\r\n            .filters.map(f => fromFilter(f))\r\n            .reduce((accum, current) => accum.concat(current));\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction toOrder(orderBys) {\r\n    if (orderBys.length === 0) {\r\n        return;\r\n    }\r\n    return orderBys.map(order => toPropertyOrder(order));\r\n}\r\nfunction fromOrder(orderBys) {\r\n    return orderBys.map(order => fromPropertyOrder(order));\r\n}\r\nfunction toStartAtCursor(cursor) {\r\n    return {\r\n        before: cursor.inclusive,\r\n        values: cursor.position\r\n    };\r\n}\r\nfunction toEndAtCursor(cursor) {\r\n    return {\r\n        before: !cursor.inclusive,\r\n        values: cursor.position\r\n    };\r\n}\r\nfunction fromStartAtCursor(cursor) {\r\n    const inclusive = !!cursor.before;\r\n    const position = cursor.values || [];\r\n    return new Bound(position, inclusive);\r\n}\r\nfunction fromEndAtCursor(cursor) {\r\n    const inclusive = !cursor.before;\r\n    const position = cursor.values || [];\r\n    return new Bound(position, inclusive);\r\n}\r\n// visible for testing\r\nfunction toDirection(dir) {\r\n    return DIRECTIONS[dir];\r\n}\r\n// visible for testing\r\nfunction fromDirection(dir) {\r\n    switch (dir) {\r\n        case 'ASCENDING':\r\n            return \"asc\" /* ASCENDING */;\r\n        case 'DESCENDING':\r\n            return \"desc\" /* DESCENDING */;\r\n        default:\r\n            return undefined;\r\n    }\r\n}\r\n// visible for testing\r\nfunction toOperatorName(op) {\r\n    return OPERATORS[op];\r\n}\r\nfunction fromOperatorName(op) {\r\n    switch (op) {\r\n        case 'EQUAL':\r\n            return \"==\" /* EQUAL */;\r\n        case 'NOT_EQUAL':\r\n            return \"!=\" /* NOT_EQUAL */;\r\n        case 'GREATER_THAN':\r\n            return \">\" /* GREATER_THAN */;\r\n        case 'GREATER_THAN_OR_EQUAL':\r\n            return \">=\" /* GREATER_THAN_OR_EQUAL */;\r\n        case 'LESS_THAN':\r\n            return \"<\" /* LESS_THAN */;\r\n        case 'LESS_THAN_OR_EQUAL':\r\n            return \"<=\" /* LESS_THAN_OR_EQUAL */;\r\n        case 'ARRAY_CONTAINS':\r\n            return \"array-contains\" /* ARRAY_CONTAINS */;\r\n        case 'IN':\r\n            return \"in\" /* IN */;\r\n        case 'NOT_IN':\r\n            return \"not-in\" /* NOT_IN */;\r\n        case 'ARRAY_CONTAINS_ANY':\r\n            return \"array-contains-any\" /* ARRAY_CONTAINS_ANY */;\r\n        case 'OPERATOR_UNSPECIFIED':\r\n            return fail();\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\nfunction toFieldPathReference(path) {\r\n    return { fieldPath: path.canonicalString() };\r\n}\r\nfunction fromFieldPathReference(fieldReference) {\r\n    return FieldPath$1.fromServerFormat(fieldReference.fieldPath);\r\n}\r\n// visible for testing\r\nfunction toPropertyOrder(orderBy) {\r\n    return {\r\n        field: toFieldPathReference(orderBy.field),\r\n        direction: toDirection(orderBy.dir)\r\n    };\r\n}\r\nfunction fromPropertyOrder(orderBy) {\r\n    return new OrderBy(fromFieldPathReference(orderBy.field), fromDirection(orderBy.direction));\r\n}\r\nfunction fromFieldFilter(filter) {\r\n    return FieldFilter.create(fromFieldPathReference(filter.fieldFilter.field), fromOperatorName(filter.fieldFilter.op), filter.fieldFilter.value);\r\n}\r\n// visible for testing\r\nfunction toUnaryOrFieldFilter(filter) {\r\n    if (filter.op === \"==\" /* EQUAL */) {\r\n        if (isNanValue(filter.value)) {\r\n            return {\r\n                unaryFilter: {\r\n                    field: toFieldPathReference(filter.field),\r\n                    op: 'IS_NAN'\r\n                }\r\n            };\r\n        }\r\n        else if (isNullValue(filter.value)) {\r\n            return {\r\n                unaryFilter: {\r\n                    field: toFieldPathReference(filter.field),\r\n                    op: 'IS_NULL'\r\n                }\r\n            };\r\n        }\r\n    }\r\n    else if (filter.op === \"!=\" /* NOT_EQUAL */) {\r\n        if (isNanValue(filter.value)) {\r\n            return {\r\n                unaryFilter: {\r\n                    field: toFieldPathReference(filter.field),\r\n                    op: 'IS_NOT_NAN'\r\n                }\r\n            };\r\n        }\r\n        else if (isNullValue(filter.value)) {\r\n            return {\r\n                unaryFilter: {\r\n                    field: toFieldPathReference(filter.field),\r\n                    op: 'IS_NOT_NULL'\r\n                }\r\n            };\r\n        }\r\n    }\r\n    return {\r\n        fieldFilter: {\r\n            field: toFieldPathReference(filter.field),\r\n            op: toOperatorName(filter.op),\r\n            value: filter.value\r\n        }\r\n    };\r\n}\r\nfunction fromUnaryFilter(filter) {\r\n    switch (filter.unaryFilter.op) {\r\n        case 'IS_NAN':\r\n            const nanField = fromFieldPathReference(filter.unaryFilter.field);\r\n            return FieldFilter.create(nanField, \"==\" /* EQUAL */, {\r\n                doubleValue: NaN\r\n            });\r\n        case 'IS_NULL':\r\n            const nullField = fromFieldPathReference(filter.unaryFilter.field);\r\n            return FieldFilter.create(nullField, \"==\" /* EQUAL */, {\r\n                nullValue: 'NULL_VALUE'\r\n            });\r\n        case 'IS_NOT_NAN':\r\n            const notNanField = fromFieldPathReference(filter.unaryFilter.field);\r\n            return FieldFilter.create(notNanField, \"!=\" /* NOT_EQUAL */, {\r\n                doubleValue: NaN\r\n            });\r\n        case 'IS_NOT_NULL':\r\n            const notNullField = fromFieldPathReference(filter.unaryFilter.field);\r\n            return FieldFilter.create(notNullField, \"!=\" /* NOT_EQUAL */, {\r\n                nullValue: 'NULL_VALUE'\r\n            });\r\n        case 'OPERATOR_UNSPECIFIED':\r\n            return fail();\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\nfunction toDocumentMask(fieldMask) {\r\n    const canonicalFields = [];\r\n    fieldMask.fields.forEach(field => canonicalFields.push(field.canonicalString()));\r\n    return {\r\n        fieldPaths: canonicalFields\r\n    };\r\n}\r\nfunction fromDocumentMask(proto) {\r\n    const paths = proto.fieldPaths || [];\r\n    return new FieldMask(paths.map(path => FieldPath$1.fromServerFormat(path)));\r\n}\r\nfunction isValidResourceName(path) {\r\n    // Resource names have at least 4 components (project ID, database ID)\r\n    return (path.length >= 4 &&\r\n        path.get(0) === 'projects' &&\r\n        path.get(2) === 'databases');\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An immutable set of metadata that the local store tracks for each target.\r\n */\r\nclass TargetData {\r\n    constructor(\r\n    /** The target being listened to. */\r\n    target, \r\n    /**\r\n     * The target ID to which the target corresponds; Assigned by the\r\n     * LocalStore for user listens and by the SyncEngine for limbo watches.\r\n     */\r\n    targetId, \r\n    /** The purpose of the target. */\r\n    purpose, \r\n    /**\r\n     * The sequence number of the last transaction during which this target data\r\n     * was modified.\r\n     */\r\n    sequenceNumber, \r\n    /** The latest snapshot version seen for this target. */\r\n    snapshotVersion = SnapshotVersion.min(), \r\n    /**\r\n     * The maximum snapshot version at which the associated view\r\n     * contained no limbo documents.\r\n     */\r\n    lastLimboFreeSnapshotVersion = SnapshotVersion.min(), \r\n    /**\r\n     * An opaque, server-assigned token that allows watching a target to be\r\n     * resumed after disconnecting without retransmitting all the data that\r\n     * matches the target. The resume token essentially identifies a point in\r\n     * time from which the server should resume sending results.\r\n     */\r\n    resumeToken = ByteString.EMPTY_BYTE_STRING) {\r\n        this.target = target;\r\n        this.targetId = targetId;\r\n        this.purpose = purpose;\r\n        this.sequenceNumber = sequenceNumber;\r\n        this.snapshotVersion = snapshotVersion;\r\n        this.lastLimboFreeSnapshotVersion = lastLimboFreeSnapshotVersion;\r\n        this.resumeToken = resumeToken;\r\n    }\r\n    /** Creates a new target data instance with an updated sequence number. */\r\n    withSequenceNumber(sequenceNumber) {\r\n        return new TargetData(this.target, this.targetId, this.purpose, sequenceNumber, this.snapshotVersion, this.lastLimboFreeSnapshotVersion, this.resumeToken);\r\n    }\r\n    /**\r\n     * Creates a new target data instance with an updated resume token and\r\n     * snapshot version.\r\n     */\r\n    withResumeToken(resumeToken, snapshotVersion) {\r\n        return new TargetData(this.target, this.targetId, this.purpose, this.sequenceNumber, snapshotVersion, this.lastLimboFreeSnapshotVersion, resumeToken);\r\n    }\r\n    /**\r\n     * Creates a new target data instance with an updated last limbo free\r\n     * snapshot version number.\r\n     */\r\n    withLastLimboFreeSnapshotVersion(lastLimboFreeSnapshotVersion) {\r\n        return new TargetData(this.target, this.targetId, this.purpose, this.sequenceNumber, this.snapshotVersion, lastLimboFreeSnapshotVersion, this.resumeToken);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Serializer for values stored in the LocalStore. */\r\nclass LocalSerializer {\r\n    constructor(remoteSerializer) {\r\n        this.remoteSerializer = remoteSerializer;\r\n    }\r\n}\r\n/** Decodes a remote document from storage locally to a Document. */\r\nfunction fromDbRemoteDocument(localSerializer, remoteDoc) {\r\n    let doc;\r\n    if (remoteDoc.document) {\r\n        doc = fromDocument(localSerializer.remoteSerializer, remoteDoc.document, !!remoteDoc.hasCommittedMutations);\r\n    }\r\n    else if (remoteDoc.noDocument) {\r\n        const key = DocumentKey.fromSegments(remoteDoc.noDocument.path);\r\n        const version = fromDbTimestamp(remoteDoc.noDocument.readTime);\r\n        doc = MutableDocument.newNoDocument(key, version);\r\n        if (remoteDoc.hasCommittedMutations) {\r\n            doc.setHasCommittedMutations();\r\n        }\r\n    }\r\n    else if (remoteDoc.unknownDocument) {\r\n        const key = DocumentKey.fromSegments(remoteDoc.unknownDocument.path);\r\n        const version = fromDbTimestamp(remoteDoc.unknownDocument.version);\r\n        doc = MutableDocument.newUnknownDocument(key, version);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n    if (remoteDoc.readTime) {\r\n        doc.setReadTime(fromDbTimestampKey(remoteDoc.readTime));\r\n    }\r\n    return doc;\r\n}\r\n/** Encodes a document for storage locally. */\r\nfunction toDbRemoteDocument(localSerializer, document) {\r\n    const key = document.key;\r\n    const remoteDoc = {\r\n        prefixPath: key.getCollectionPath().popLast().toArray(),\r\n        collectionGroup: key.collectionGroup,\r\n        documentId: key.path.lastSegment(),\r\n        readTime: toDbTimestampKey(document.readTime),\r\n        hasCommittedMutations: document.hasCommittedMutations\r\n    };\r\n    if (document.isFoundDocument()) {\r\n        remoteDoc.document = toDocument(localSerializer.remoteSerializer, document);\r\n    }\r\n    else if (document.isNoDocument()) {\r\n        remoteDoc.noDocument = {\r\n            path: key.path.toArray(),\r\n            readTime: toDbTimestamp(document.version)\r\n        };\r\n    }\r\n    else if (document.isUnknownDocument()) {\r\n        remoteDoc.unknownDocument = {\r\n            path: key.path.toArray(),\r\n            version: toDbTimestamp(document.version)\r\n        };\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n    return remoteDoc;\r\n}\r\nfunction toDbTimestampKey(snapshotVersion) {\r\n    const timestamp = snapshotVersion.toTimestamp();\r\n    return [timestamp.seconds, timestamp.nanoseconds];\r\n}\r\nfunction fromDbTimestampKey(dbTimestampKey) {\r\n    const timestamp = new Timestamp(dbTimestampKey[0], dbTimestampKey[1]);\r\n    return SnapshotVersion.fromTimestamp(timestamp);\r\n}\r\nfunction toDbTimestamp(snapshotVersion) {\r\n    const timestamp = snapshotVersion.toTimestamp();\r\n    return { seconds: timestamp.seconds, nanoseconds: timestamp.nanoseconds };\r\n}\r\nfunction fromDbTimestamp(dbTimestamp) {\r\n    const timestamp = new Timestamp(dbTimestamp.seconds, dbTimestamp.nanoseconds);\r\n    return SnapshotVersion.fromTimestamp(timestamp);\r\n}\r\n/** Encodes a batch of mutations into a DbMutationBatch for local storage. */\r\nfunction toDbMutationBatch(localSerializer, userId, batch) {\r\n    const serializedBaseMutations = batch.baseMutations.map(m => toMutation(localSerializer.remoteSerializer, m));\r\n    const serializedMutations = batch.mutations.map(m => toMutation(localSerializer.remoteSerializer, m));\r\n    return {\r\n        userId,\r\n        batchId: batch.batchId,\r\n        localWriteTimeMs: batch.localWriteTime.toMillis(),\r\n        baseMutations: serializedBaseMutations,\r\n        mutations: serializedMutations\r\n    };\r\n}\r\n/** Decodes a DbMutationBatch into a MutationBatch */\r\nfunction fromDbMutationBatch(localSerializer, dbBatch) {\r\n    const baseMutations = (dbBatch.baseMutations || []).map(m => fromMutation(localSerializer.remoteSerializer, m));\r\n    // Squash old transform mutations into existing patch or set mutations.\r\n    // The replacement of representing `transforms` with `update_transforms`\r\n    // on the SDK means that old `transform` mutations stored in IndexedDB need\r\n    // to be updated to `update_transforms`.\r\n    // TODO(b/174608374): Remove this code once we perform a schema migration.\r\n    for (let i = 0; i < dbBatch.mutations.length - 1; ++i) {\r\n        const currentMutation = dbBatch.mutations[i];\r\n        const hasTransform = i + 1 < dbBatch.mutations.length &&\r\n            dbBatch.mutations[i + 1].transform !== undefined;\r\n        if (hasTransform) {\r\n            const transformMutation = dbBatch.mutations[i + 1];\r\n            currentMutation.updateTransforms =\r\n                transformMutation.transform.fieldTransforms;\r\n            dbBatch.mutations.splice(i + 1, 1);\r\n            ++i;\r\n        }\r\n    }\r\n    const mutations = dbBatch.mutations.map(m => fromMutation(localSerializer.remoteSerializer, m));\r\n    const timestamp = Timestamp.fromMillis(dbBatch.localWriteTimeMs);\r\n    return new MutationBatch(dbBatch.batchId, timestamp, baseMutations, mutations);\r\n}\r\n/** Decodes a DbTarget into TargetData */\r\nfunction fromDbTarget(dbTarget) {\r\n    const version = fromDbTimestamp(dbTarget.readTime);\r\n    const lastLimboFreeSnapshotVersion = dbTarget.lastLimboFreeSnapshotVersion !== undefined\r\n        ? fromDbTimestamp(dbTarget.lastLimboFreeSnapshotVersion)\r\n        : SnapshotVersion.min();\r\n    let target;\r\n    if (isDocumentQuery(dbTarget.query)) {\r\n        target = fromDocumentsTarget(dbTarget.query);\r\n    }\r\n    else {\r\n        target = fromQueryTarget(dbTarget.query);\r\n    }\r\n    return new TargetData(target, dbTarget.targetId, 0 /* Listen */, dbTarget.lastListenSequenceNumber, version, lastLimboFreeSnapshotVersion, ByteString.fromBase64String(dbTarget.resumeToken));\r\n}\r\n/** Encodes TargetData into a DbTarget for storage locally. */\r\nfunction toDbTarget(localSerializer, targetData) {\r\n    const dbTimestamp = toDbTimestamp(targetData.snapshotVersion);\r\n    const dbLastLimboFreeTimestamp = toDbTimestamp(targetData.lastLimboFreeSnapshotVersion);\r\n    let queryProto;\r\n    if (targetIsDocumentTarget(targetData.target)) {\r\n        queryProto = toDocumentsTarget(localSerializer.remoteSerializer, targetData.target);\r\n    }\r\n    else {\r\n        queryProto = toQueryTarget(localSerializer.remoteSerializer, targetData.target);\r\n    }\r\n    // We can't store the resumeToken as a ByteString in IndexedDb, so we\r\n    // convert it to a base64 string for storage.\r\n    const resumeToken = targetData.resumeToken.toBase64();\r\n    // lastListenSequenceNumber is always 0 until we do real GC.\r\n    return {\r\n        targetId: targetData.targetId,\r\n        canonicalId: canonifyTarget(targetData.target),\r\n        readTime: dbTimestamp,\r\n        resumeToken,\r\n        lastListenSequenceNumber: targetData.sequenceNumber,\r\n        lastLimboFreeSnapshotVersion: dbLastLimboFreeTimestamp,\r\n        query: queryProto\r\n    };\r\n}\r\n/**\r\n * A helper function for figuring out what kind of query has been stored.\r\n */\r\nfunction isDocumentQuery(dbQuery) {\r\n    return dbQuery.documents !== undefined;\r\n}\r\n/** Encodes a DbBundle to a BundleMetadata object. */\r\nfunction fromDbBundle(dbBundle) {\r\n    return {\r\n        id: dbBundle.bundleId,\r\n        createTime: fromDbTimestamp(dbBundle.createTime),\r\n        version: dbBundle.version\r\n    };\r\n}\r\n/** Encodes a BundleMetadata to a DbBundle. */\r\nfunction toDbBundle(metadata) {\r\n    return {\r\n        bundleId: metadata.id,\r\n        createTime: toDbTimestamp(fromVersion(metadata.createTime)),\r\n        version: metadata.version\r\n    };\r\n}\r\n/** Encodes a DbNamedQuery to a NamedQuery. */\r\nfunction fromDbNamedQuery(dbNamedQuery) {\r\n    return {\r\n        name: dbNamedQuery.name,\r\n        query: fromBundledQuery(dbNamedQuery.bundledQuery),\r\n        readTime: fromDbTimestamp(dbNamedQuery.readTime)\r\n    };\r\n}\r\n/** Encodes a NamedQuery from a bundle proto to a DbNamedQuery. */\r\nfunction toDbNamedQuery(query) {\r\n    return {\r\n        name: query.name,\r\n        readTime: toDbTimestamp(fromVersion(query.readTime)),\r\n        bundledQuery: query.bundledQuery\r\n    };\r\n}\r\n/**\r\n * Encodes a `BundledQuery` from bundle proto to a Query object.\r\n *\r\n * This reconstructs the original query used to build the bundle being loaded,\r\n * including features exists only in SDKs (for example: limit-to-last).\r\n */\r\nfunction fromBundledQuery(bundledQuery) {\r\n    const query = convertQueryTargetToQuery({\r\n        parent: bundledQuery.parent,\r\n        structuredQuery: bundledQuery.structuredQuery\r\n    });\r\n    if (bundledQuery.limitType === 'LAST') {\r\n        return queryWithLimit(query, query.limit, \"L\" /* Last */);\r\n    }\r\n    return query;\r\n}\r\n/** Encodes a NamedQuery proto object to a NamedQuery model object. */\r\nfunction fromProtoNamedQuery(namedQuery) {\r\n    return {\r\n        name: namedQuery.name,\r\n        query: fromBundledQuery(namedQuery.bundledQuery),\r\n        readTime: fromVersion(namedQuery.readTime)\r\n    };\r\n}\r\n/** Decodes a BundleMetadata proto into a BundleMetadata object. */\r\nfunction fromBundleMetadata(metadata) {\r\n    return {\r\n        id: metadata.id,\r\n        version: metadata.version,\r\n        createTime: fromVersion(metadata.createTime)\r\n    };\r\n}\r\n/** Encodes a DbDocumentOverlay object to an Overlay model object. */\r\nfunction fromDbDocumentOverlay(localSerializer, dbDocumentOverlay) {\r\n    return new Overlay(dbDocumentOverlay.largestBatchId, fromMutation(localSerializer.remoteSerializer, dbDocumentOverlay.overlayMutation));\r\n}\r\n/** Decodes an Overlay model object into a DbDocumentOverlay object. */\r\nfunction toDbDocumentOverlay(localSerializer, userId, overlay) {\r\n    const [_, collectionPath, documentId] = toDbDocumentOverlayKey(userId, overlay.mutation.key);\r\n    return {\r\n        userId,\r\n        collectionPath,\r\n        documentId,\r\n        collectionGroup: overlay.mutation.key.getCollectionGroup(),\r\n        largestBatchId: overlay.largestBatchId,\r\n        overlayMutation: toMutation(localSerializer.remoteSerializer, overlay.mutation)\r\n    };\r\n}\r\n/**\r\n * Returns the DbDocumentOverlayKey corresponding to the given user and\r\n * document key.\r\n */\r\nfunction toDbDocumentOverlayKey(userId, docKey) {\r\n    const docId = docKey.path.lastSegment();\r\n    const collectionPath = encodeResourcePath(docKey.path.popLast());\r\n    return [userId, collectionPath, docId];\r\n}\r\nfunction toDbIndexConfiguration(index) {\r\n    return {\r\n        indexId: index.indexId,\r\n        collectionGroup: index.collectionGroup,\r\n        fields: index.fields.map(s => [s.fieldPath.canonicalString(), s.kind])\r\n    };\r\n}\r\nfunction fromDbIndexConfiguration(index, state) {\r\n    const decodedState = state\r\n        ? new IndexState(state.sequenceNumber, new IndexOffset(fromDbTimestamp(state.readTime), new DocumentKey(decodeResourcePath(state.documentKey)), state.largestBatchId))\r\n        : IndexState.empty();\r\n    const decodedSegments = index.fields.map(([fieldPath, kind]) => new IndexSegment(FieldPath$1.fromServerFormat(fieldPath), kind));\r\n    return new FieldIndex(index.indexId, index.collectionGroup, decodedSegments, decodedState);\r\n}\r\nfunction toDbIndexState(indexId, user, sequenceNumber, offset) {\r\n    return {\r\n        indexId,\r\n        uid: user.uid || '',\r\n        sequenceNumber,\r\n        readTime: toDbTimestamp(offset.readTime),\r\n        documentKey: encodeResourcePath(offset.documentKey.path),\r\n        largestBatchId: offset.largestBatchId\r\n    };\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass IndexedDbBundleCache {\r\n    getBundleMetadata(transaction, bundleId) {\r\n        return bundlesStore(transaction)\r\n            .get(bundleId)\r\n            .next(bundle => {\r\n            if (bundle) {\r\n                return fromDbBundle(bundle);\r\n            }\r\n            return undefined;\r\n        });\r\n    }\r\n    saveBundleMetadata(transaction, bundleMetadata) {\r\n        return bundlesStore(transaction).put(toDbBundle(bundleMetadata));\r\n    }\r\n    getNamedQuery(transaction, queryName) {\r\n        return namedQueriesStore(transaction)\r\n            .get(queryName)\r\n            .next(query => {\r\n            if (query) {\r\n                return fromDbNamedQuery(query);\r\n            }\r\n            return undefined;\r\n        });\r\n    }\r\n    saveNamedQuery(transaction, query) {\r\n        return namedQueriesStore(transaction).put(toDbNamedQuery(query));\r\n    }\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the bundles object store.\r\n */\r\nfunction bundlesStore(txn) {\r\n    return getStore(txn, DbBundleStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the namedQueries object store.\r\n */\r\nfunction namedQueriesStore(txn) {\r\n    return getStore(txn, DbNamedQueryStore);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Implementation of DocumentOverlayCache using IndexedDb.\r\n */\r\nclass IndexedDbDocumentOverlayCache {\r\n    /**\r\n     * @param serializer - The document serializer.\r\n     * @param userId - The userId for which we are accessing overlays.\r\n     */\r\n    constructor(serializer, userId) {\r\n        this.serializer = serializer;\r\n        this.userId = userId;\r\n    }\r\n    static forUser(serializer, user) {\r\n        const userId = user.uid || '';\r\n        return new IndexedDbDocumentOverlayCache(serializer, userId);\r\n    }\r\n    getOverlay(transaction, key) {\r\n        return documentOverlayStore(transaction)\r\n            .get(toDbDocumentOverlayKey(this.userId, key))\r\n            .next(dbOverlay => {\r\n            if (dbOverlay) {\r\n                return fromDbDocumentOverlay(this.serializer, dbOverlay);\r\n            }\r\n            return null;\r\n        });\r\n    }\r\n    getOverlays(transaction, keys) {\r\n        const result = newOverlayMap();\r\n        return PersistencePromise.forEach(keys, (key) => {\r\n            return this.getOverlay(transaction, key).next(overlay => {\r\n                if (overlay !== null) {\r\n                    result.set(key, overlay);\r\n                }\r\n            });\r\n        }).next(() => result);\r\n    }\r\n    saveOverlays(transaction, largestBatchId, overlays) {\r\n        const promises = [];\r\n        overlays.forEach((_, mutation) => {\r\n            const overlay = new Overlay(largestBatchId, mutation);\r\n            promises.push(this.saveOverlay(transaction, overlay));\r\n        });\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    removeOverlaysForBatchId(transaction, documentKeys, batchId) {\r\n        const collectionPaths = new Set();\r\n        // Get the set of unique collection paths.\r\n        documentKeys.forEach(key => collectionPaths.add(encodeResourcePath(key.getCollectionPath())));\r\n        const promises = [];\r\n        collectionPaths.forEach(collectionPath => {\r\n            const range = IDBKeyRange.bound([this.userId, collectionPath, batchId], [this.userId, collectionPath, batchId + 1], \r\n            /*lowerOpen=*/ false, \r\n            /*upperOpen=*/ true);\r\n            promises.push(documentOverlayStore(transaction).deleteAll(DbDocumentOverlayCollectionPathOverlayIndex, range));\r\n        });\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    getOverlaysForCollection(transaction, collection, sinceBatchId) {\r\n        const result = newOverlayMap();\r\n        const collectionPath = encodeResourcePath(collection);\r\n        // We want batch IDs larger than `sinceBatchId`, and so the lower bound\r\n        // is not inclusive.\r\n        const range = IDBKeyRange.bound([this.userId, collectionPath, sinceBatchId], [this.userId, collectionPath, Number.POSITIVE_INFINITY], \r\n        /*lowerOpen=*/ true);\r\n        return documentOverlayStore(transaction)\r\n            .loadAll(DbDocumentOverlayCollectionPathOverlayIndex, range)\r\n            .next(dbOverlays => {\r\n            for (const dbOverlay of dbOverlays) {\r\n                const overlay = fromDbDocumentOverlay(this.serializer, dbOverlay);\r\n                result.set(overlay.getKey(), overlay);\r\n            }\r\n            return result;\r\n        });\r\n    }\r\n    getOverlaysForCollectionGroup(transaction, collectionGroup, sinceBatchId, count) {\r\n        const result = newOverlayMap();\r\n        let currentBatchId = undefined;\r\n        // We want batch IDs larger than `sinceBatchId`, and so the lower bound\r\n        // is not inclusive.\r\n        const range = IDBKeyRange.bound([this.userId, collectionGroup, sinceBatchId], [this.userId, collectionGroup, Number.POSITIVE_INFINITY], \r\n        /*lowerOpen=*/ true);\r\n        return documentOverlayStore(transaction)\r\n            .iterate({\r\n            index: DbDocumentOverlayCollectionGroupOverlayIndex,\r\n            range\r\n        }, (_, dbOverlay, control) => {\r\n            // We do not want to return partial batch overlays, even if the size\r\n            // of the result set exceeds the given `count` argument. Therefore, we\r\n            // continue to aggregate results even after the result size exceeds\r\n            // `count` if there are more overlays from the `currentBatchId`.\r\n            const overlay = fromDbDocumentOverlay(this.serializer, dbOverlay);\r\n            if (result.size() < count ||\r\n                overlay.largestBatchId === currentBatchId) {\r\n                result.set(overlay.getKey(), overlay);\r\n                currentBatchId = overlay.largestBatchId;\r\n            }\r\n            else {\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => result);\r\n    }\r\n    saveOverlay(transaction, overlay) {\r\n        return documentOverlayStore(transaction).put(toDbDocumentOverlay(this.serializer, this.userId, overlay));\r\n    }\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the document overlay object store.\r\n */\r\nfunction documentOverlayStore(txn) {\r\n    return getStore(txn, DbDocumentOverlayStore);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// Note: This code is copied from the backend. Code that is not used by\r\n// Firestore was removed.\r\nconst INDEX_TYPE_NULL = 5;\r\nconst INDEX_TYPE_BOOLEAN = 10;\r\nconst INDEX_TYPE_NAN = 13;\r\nconst INDEX_TYPE_NUMBER = 15;\r\nconst INDEX_TYPE_TIMESTAMP = 20;\r\nconst INDEX_TYPE_STRING = 25;\r\nconst INDEX_TYPE_BLOB = 30;\r\nconst INDEX_TYPE_REFERENCE = 37;\r\nconst INDEX_TYPE_GEOPOINT = 45;\r\nconst INDEX_TYPE_ARRAY = 50;\r\nconst INDEX_TYPE_MAP = 55;\r\nconst INDEX_TYPE_REFERENCE_SEGMENT = 60;\r\n// A terminator that indicates that a truncatable value was not truncated.\r\n// This must be smaller than all other type labels.\r\nconst NOT_TRUNCATED = 2;\r\n/** Firestore index value writer.  */\r\nclass FirestoreIndexValueWriter {\r\n    constructor() { }\r\n    // The write methods below short-circuit writing terminators for values\r\n    // containing a (terminating) truncated value.\r\n    //\r\n    // As an example, consider the resulting encoding for:\r\n    //\r\n    // [\"bar\", [2, \"foo\"]] -> (STRING, \"bar\", TERM, ARRAY, NUMBER, 2, STRING, \"foo\", TERM, TERM, TERM)\r\n    // [\"bar\", [2, truncated(\"foo\")]] -> (STRING, \"bar\", TERM, ARRAY, NUMBER, 2, STRING, \"foo\", TRUNC)\r\n    // [\"bar\", truncated([\"foo\"])] -> (STRING, \"bar\", TERM, ARRAY. STRING, \"foo\", TERM, TRUNC)\r\n    /** Writes an index value.  */\r\n    writeIndexValue(value, encoder) {\r\n        this.writeIndexValueAux(value, encoder);\r\n        // Write separator to split index values\r\n        // (see go/firestore-storage-format#encodings).\r\n        encoder.writeInfinity();\r\n    }\r\n    writeIndexValueAux(indexValue, encoder) {\r\n        if ('nullValue' in indexValue) {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_NULL);\r\n        }\r\n        else if ('booleanValue' in indexValue) {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_BOOLEAN);\r\n            encoder.writeNumber(indexValue.booleanValue ? 1 : 0);\r\n        }\r\n        else if ('integerValue' in indexValue) {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_NUMBER);\r\n            encoder.writeNumber(normalizeNumber(indexValue.integerValue));\r\n        }\r\n        else if ('doubleValue' in indexValue) {\r\n            const n = normalizeNumber(indexValue.doubleValue);\r\n            if (isNaN(n)) {\r\n                this.writeValueTypeLabel(encoder, INDEX_TYPE_NAN);\r\n            }\r\n            else {\r\n                this.writeValueTypeLabel(encoder, INDEX_TYPE_NUMBER);\r\n                if (isNegativeZero(n)) {\r\n                    // -0.0, 0 and 0.0 are all considered the same\r\n                    encoder.writeNumber(0.0);\r\n                }\r\n                else {\r\n                    encoder.writeNumber(n);\r\n                }\r\n            }\r\n        }\r\n        else if ('timestampValue' in indexValue) {\r\n            const timestamp = indexValue.timestampValue;\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_TIMESTAMP);\r\n            if (typeof timestamp === 'string') {\r\n                encoder.writeString(timestamp);\r\n            }\r\n            else {\r\n                encoder.writeString(`${timestamp.seconds || ''}`);\r\n                encoder.writeNumber(timestamp.nanos || 0);\r\n            }\r\n        }\r\n        else if ('stringValue' in indexValue) {\r\n            this.writeIndexString(indexValue.stringValue, encoder);\r\n            this.writeTruncationMarker(encoder);\r\n        }\r\n        else if ('bytesValue' in indexValue) {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_BLOB);\r\n            encoder.writeBytes(normalizeByteString(indexValue.bytesValue));\r\n            this.writeTruncationMarker(encoder);\r\n        }\r\n        else if ('referenceValue' in indexValue) {\r\n            this.writeIndexEntityRef(indexValue.referenceValue, encoder);\r\n        }\r\n        else if ('geoPointValue' in indexValue) {\r\n            const geoPoint = indexValue.geoPointValue;\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_GEOPOINT);\r\n            encoder.writeNumber(geoPoint.latitude || 0);\r\n            encoder.writeNumber(geoPoint.longitude || 0);\r\n        }\r\n        else if ('mapValue' in indexValue) {\r\n            if (isMaxValue(indexValue)) {\r\n                this.writeValueTypeLabel(encoder, Number.MAX_SAFE_INTEGER);\r\n            }\r\n            else {\r\n                this.writeIndexMap(indexValue.mapValue, encoder);\r\n                this.writeTruncationMarker(encoder);\r\n            }\r\n        }\r\n        else if ('arrayValue' in indexValue) {\r\n            this.writeIndexArray(indexValue.arrayValue, encoder);\r\n            this.writeTruncationMarker(encoder);\r\n        }\r\n        else {\r\n            fail();\r\n        }\r\n    }\r\n    writeIndexString(stringIndexValue, encoder) {\r\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_STRING);\r\n        this.writeUnlabeledIndexString(stringIndexValue, encoder);\r\n    }\r\n    writeUnlabeledIndexString(stringIndexValue, encoder) {\r\n        encoder.writeString(stringIndexValue);\r\n    }\r\n    writeIndexMap(mapIndexValue, encoder) {\r\n        const map = mapIndexValue.fields || {};\r\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_MAP);\r\n        for (const key of Object.keys(map)) {\r\n            this.writeIndexString(key, encoder);\r\n            this.writeIndexValueAux(map[key], encoder);\r\n        }\r\n    }\r\n    writeIndexArray(arrayIndexValue, encoder) {\r\n        const values = arrayIndexValue.values || [];\r\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_ARRAY);\r\n        for (const element of values) {\r\n            this.writeIndexValueAux(element, encoder);\r\n        }\r\n    }\r\n    writeIndexEntityRef(referenceValue, encoder) {\r\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_REFERENCE);\r\n        const path = DocumentKey.fromName(referenceValue).path;\r\n        path.forEach(segment => {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_REFERENCE_SEGMENT);\r\n            this.writeUnlabeledIndexString(segment, encoder);\r\n        });\r\n    }\r\n    writeValueTypeLabel(encoder, typeOrder) {\r\n        encoder.writeNumber(typeOrder);\r\n    }\r\n    writeTruncationMarker(encoder) {\r\n        // While the SDK does not implement truncation, the truncation marker is\r\n        // used to terminate all variable length values (which are strings, bytes,\r\n        // references, arrays and maps).\r\n        encoder.writeNumber(NOT_TRUNCATED);\r\n    }\r\n}\r\nFirestoreIndexValueWriter.INSTANCE = new FirestoreIndexValueWriter();\n\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law | agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES | CONDITIONS OF ANY KIND, either express | implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** These constants are taken from the backend. */\r\nconst MIN_SURROGATE = '\\uD800';\r\nconst MAX_SURROGATE = '\\uDBFF';\r\nconst ESCAPE1 = 0x00;\r\nconst NULL_BYTE = 0xff; // Combined with ESCAPE1\r\nconst SEPARATOR = 0x01; // Combined with ESCAPE1\r\nconst ESCAPE2 = 0xff;\r\nconst INFINITY = 0xff; // Combined with ESCAPE2\r\nconst FF_BYTE = 0x00; // Combined with ESCAPE2\r\nconst LONG_SIZE = 64;\r\nconst BYTE_SIZE = 8;\r\n/**\r\n * The default size of the buffer. This is arbitrary, but likely larger than\r\n * most index values so that less copies of the underlying buffer will be made.\r\n * For large values, a single copy will made to double the buffer length.\r\n */\r\nconst DEFAULT_BUFFER_SIZE = 1024;\r\n/** Converts a JavaScript number to a byte array (using big endian encoding). */\r\nfunction doubleToLongBits(value) {\r\n    const dv = new DataView(new ArrayBuffer(8));\r\n    dv.setFloat64(0, value, /* littleEndian= */ false);\r\n    return new Uint8Array(dv.buffer);\r\n}\r\n/**\r\n * Counts the number of zeros in a byte.\r\n *\r\n * Visible for testing.\r\n */\r\nfunction numberOfLeadingZerosInByte(x) {\r\n    if (x === 0) {\r\n        return 8;\r\n    }\r\n    let zeros = 0;\r\n    if (x >> 4 === 0) {\r\n        // Test if the first four bits are zero.\r\n        zeros += 4;\r\n        x = x << 4;\r\n    }\r\n    if (x >> 6 === 0) {\r\n        // Test if the first two (or next two) bits are zero.\r\n        zeros += 2;\r\n        x = x << 2;\r\n    }\r\n    if (x >> 7 === 0) {\r\n        // Test if the remaining bit is zero.\r\n        zeros += 1;\r\n    }\r\n    return zeros;\r\n}\r\n/** Counts the number of leading zeros in the given byte array. */\r\nfunction numberOfLeadingZeros(bytes) {\r\n    let leadingZeros = 0;\r\n    for (let i = 0; i < 8; ++i) {\r\n        const zeros = numberOfLeadingZerosInByte(bytes[i] & 0xff);\r\n        leadingZeros += zeros;\r\n        if (zeros !== 8) {\r\n            break;\r\n        }\r\n    }\r\n    return leadingZeros;\r\n}\r\n/**\r\n * Returns the number of bytes required to store \"value\". Leading zero bytes\r\n * are skipped.\r\n */\r\nfunction unsignedNumLength(value) {\r\n    // This is just the number of bytes for the unsigned representation of the number.\r\n    const numBits = LONG_SIZE - numberOfLeadingZeros(value);\r\n    return Math.ceil(numBits / BYTE_SIZE);\r\n}\r\n/**\r\n * OrderedCodeWriter is a minimal-allocation implementation of the writing\r\n * behavior defined by the backend.\r\n *\r\n * The code is ported from its Java counterpart.\r\n */\r\nclass OrderedCodeWriter {\r\n    constructor() {\r\n        this.buffer = new Uint8Array(DEFAULT_BUFFER_SIZE);\r\n        this.position = 0;\r\n    }\r\n    writeBytesAscending(value) {\r\n        const it = value[Symbol.iterator]();\r\n        let byte = it.next();\r\n        while (!byte.done) {\r\n            this.writeByteAscending(byte.value);\r\n            byte = it.next();\r\n        }\r\n        this.writeSeparatorAscending();\r\n    }\r\n    writeBytesDescending(value) {\r\n        const it = value[Symbol.iterator]();\r\n        let byte = it.next();\r\n        while (!byte.done) {\r\n            this.writeByteDescending(byte.value);\r\n            byte = it.next();\r\n        }\r\n        this.writeSeparatorDescending();\r\n    }\r\n    /** Writes utf8 bytes into this byte sequence, ascending. */\r\n    writeUtf8Ascending(sequence) {\r\n        for (const c of sequence) {\r\n            const charCode = c.charCodeAt(0);\r\n            if (charCode < 0x80) {\r\n                this.writeByteAscending(charCode);\r\n            }\r\n            else if (charCode < 0x800) {\r\n                this.writeByteAscending((0x0f << 6) | (charCode >>> 6));\r\n                this.writeByteAscending(0x80 | (0x3f & charCode));\r\n            }\r\n            else if (c < MIN_SURROGATE || MAX_SURROGATE < c) {\r\n                this.writeByteAscending((0x0f << 5) | (charCode >>> 12));\r\n                this.writeByteAscending(0x80 | (0x3f & (charCode >>> 6)));\r\n                this.writeByteAscending(0x80 | (0x3f & charCode));\r\n            }\r\n            else {\r\n                const codePoint = c.codePointAt(0);\r\n                this.writeByteAscending((0x0f << 4) | (codePoint >>> 18));\r\n                this.writeByteAscending(0x80 | (0x3f & (codePoint >>> 12)));\r\n                this.writeByteAscending(0x80 | (0x3f & (codePoint >>> 6)));\r\n                this.writeByteAscending(0x80 | (0x3f & codePoint));\r\n            }\r\n        }\r\n        this.writeSeparatorAscending();\r\n    }\r\n    /** Writes utf8 bytes into this byte sequence, descending */\r\n    writeUtf8Descending(sequence) {\r\n        for (const c of sequence) {\r\n            const charCode = c.charCodeAt(0);\r\n            if (charCode < 0x80) {\r\n                this.writeByteDescending(charCode);\r\n            }\r\n            else if (charCode < 0x800) {\r\n                this.writeByteDescending((0x0f << 6) | (charCode >>> 6));\r\n                this.writeByteDescending(0x80 | (0x3f & charCode));\r\n            }\r\n            else if (c < MIN_SURROGATE || MAX_SURROGATE < c) {\r\n                this.writeByteDescending((0x0f << 5) | (charCode >>> 12));\r\n                this.writeByteDescending(0x80 | (0x3f & (charCode >>> 6)));\r\n                this.writeByteDescending(0x80 | (0x3f & charCode));\r\n            }\r\n            else {\r\n                const codePoint = c.codePointAt(0);\r\n                this.writeByteDescending((0x0f << 4) | (codePoint >>> 18));\r\n                this.writeByteDescending(0x80 | (0x3f & (codePoint >>> 12)));\r\n                this.writeByteDescending(0x80 | (0x3f & (codePoint >>> 6)));\r\n                this.writeByteDescending(0x80 | (0x3f & codePoint));\r\n            }\r\n        }\r\n        this.writeSeparatorDescending();\r\n    }\r\n    writeNumberAscending(val) {\r\n        // Values are encoded with a single byte length prefix, followed by the\r\n        // actual value in big-endian format with leading 0 bytes dropped.\r\n        const value = this.toOrderedBits(val);\r\n        const len = unsignedNumLength(value);\r\n        this.ensureAvailable(1 + len);\r\n        this.buffer[this.position++] = len & 0xff; // Write the length\r\n        for (let i = value.length - len; i < value.length; ++i) {\r\n            this.buffer[this.position++] = value[i] & 0xff;\r\n        }\r\n    }\r\n    writeNumberDescending(val) {\r\n        // Values are encoded with a single byte length prefix, followed by the\r\n        // inverted value in big-endian format with leading 0 bytes dropped.\r\n        const value = this.toOrderedBits(val);\r\n        const len = unsignedNumLength(value);\r\n        this.ensureAvailable(1 + len);\r\n        this.buffer[this.position++] = ~(len & 0xff); // Write the length\r\n        for (let i = value.length - len; i < value.length; ++i) {\r\n            this.buffer[this.position++] = ~(value[i] & 0xff);\r\n        }\r\n    }\r\n    /**\r\n     * Writes the \"infinity\" byte sequence that sorts after all other byte\r\n     * sequences written in ascending order.\r\n     */\r\n    writeInfinityAscending() {\r\n        this.writeEscapedByteAscending(ESCAPE2);\r\n        this.writeEscapedByteAscending(INFINITY);\r\n    }\r\n    /**\r\n     * Writes the \"infinity\" byte sequence that sorts before all other byte\r\n     * sequences written in descending order.\r\n     */\r\n    writeInfinityDescending() {\r\n        this.writeEscapedByteDescending(ESCAPE2);\r\n        this.writeEscapedByteDescending(INFINITY);\r\n    }\r\n    /**\r\n     * Resets the buffer such that it is the same as when it was newly\r\n     * constructed.\r\n     */\r\n    reset() {\r\n        this.position = 0;\r\n    }\r\n    seed(encodedBytes) {\r\n        this.ensureAvailable(encodedBytes.length);\r\n        this.buffer.set(encodedBytes, this.position);\r\n        this.position += encodedBytes.length;\r\n    }\r\n    /** Makes a copy of the encoded bytes in this buffer.  */\r\n    encodedBytes() {\r\n        return this.buffer.slice(0, this.position);\r\n    }\r\n    /**\r\n     * Encodes `val` into an encoding so that the order matches the IEEE 754\r\n     * floating-point comparison results with the following exceptions:\r\n     *   -0.0 < 0.0\r\n     *   all non-NaN < NaN\r\n     *   NaN = NaN\r\n     */\r\n    toOrderedBits(val) {\r\n        const value = doubleToLongBits(val);\r\n        // Check if the first bit is set. We use a bit mask since value[0] is\r\n        // encoded as a number from 0 to 255.\r\n        const isNegative = (value[0] & 0x80) !== 0;\r\n        // Revert the two complement to get natural ordering\r\n        value[0] ^= isNegative ? 0xff : 0x80;\r\n        for (let i = 1; i < value.length; ++i) {\r\n            value[i] ^= isNegative ? 0xff : 0x00;\r\n        }\r\n        return value;\r\n    }\r\n    /** Writes a single byte ascending to the buffer. */\r\n    writeByteAscending(b) {\r\n        const masked = b & 0xff;\r\n        if (masked === ESCAPE1) {\r\n            this.writeEscapedByteAscending(ESCAPE1);\r\n            this.writeEscapedByteAscending(NULL_BYTE);\r\n        }\r\n        else if (masked === ESCAPE2) {\r\n            this.writeEscapedByteAscending(ESCAPE2);\r\n            this.writeEscapedByteAscending(FF_BYTE);\r\n        }\r\n        else {\r\n            this.writeEscapedByteAscending(masked);\r\n        }\r\n    }\r\n    /** Writes a single byte descending to the buffer.  */\r\n    writeByteDescending(b) {\r\n        const masked = b & 0xff;\r\n        if (masked === ESCAPE1) {\r\n            this.writeEscapedByteDescending(ESCAPE1);\r\n            this.writeEscapedByteDescending(NULL_BYTE);\r\n        }\r\n        else if (masked === ESCAPE2) {\r\n            this.writeEscapedByteDescending(ESCAPE2);\r\n            this.writeEscapedByteDescending(FF_BYTE);\r\n        }\r\n        else {\r\n            this.writeEscapedByteDescending(b);\r\n        }\r\n    }\r\n    writeSeparatorAscending() {\r\n        this.writeEscapedByteAscending(ESCAPE1);\r\n        this.writeEscapedByteAscending(SEPARATOR);\r\n    }\r\n    writeSeparatorDescending() {\r\n        this.writeEscapedByteDescending(ESCAPE1);\r\n        this.writeEscapedByteDescending(SEPARATOR);\r\n    }\r\n    writeEscapedByteAscending(b) {\r\n        this.ensureAvailable(1);\r\n        this.buffer[this.position++] = b;\r\n    }\r\n    writeEscapedByteDescending(b) {\r\n        this.ensureAvailable(1);\r\n        this.buffer[this.position++] = ~b;\r\n    }\r\n    ensureAvailable(bytes) {\r\n        const minCapacity = bytes + this.position;\r\n        if (minCapacity <= this.buffer.length) {\r\n            return;\r\n        }\r\n        // Try doubling.\r\n        let newLength = this.buffer.length * 2;\r\n        // Still not big enough? Just allocate the right size.\r\n        if (newLength < minCapacity) {\r\n            newLength = minCapacity;\r\n        }\r\n        // Create the new buffer.\r\n        const newBuffer = new Uint8Array(newLength);\r\n        newBuffer.set(this.buffer); // copy old data\r\n        this.buffer = newBuffer;\r\n    }\r\n}\n\nclass AscendingIndexByteEncoder {\r\n    constructor(orderedCode) {\r\n        this.orderedCode = orderedCode;\r\n    }\r\n    writeBytes(value) {\r\n        this.orderedCode.writeBytesAscending(value);\r\n    }\r\n    writeString(value) {\r\n        this.orderedCode.writeUtf8Ascending(value);\r\n    }\r\n    writeNumber(value) {\r\n        this.orderedCode.writeNumberAscending(value);\r\n    }\r\n    writeInfinity() {\r\n        this.orderedCode.writeInfinityAscending();\r\n    }\r\n}\r\nclass DescendingIndexByteEncoder {\r\n    constructor(orderedCode) {\r\n        this.orderedCode = orderedCode;\r\n    }\r\n    writeBytes(value) {\r\n        this.orderedCode.writeBytesDescending(value);\r\n    }\r\n    writeString(value) {\r\n        this.orderedCode.writeUtf8Descending(value);\r\n    }\r\n    writeNumber(value) {\r\n        this.orderedCode.writeNumberDescending(value);\r\n    }\r\n    writeInfinity() {\r\n        this.orderedCode.writeInfinityDescending();\r\n    }\r\n}\r\n/**\r\n * Implements `DirectionalIndexByteEncoder` using `OrderedCodeWriter` for the\r\n * actual encoding.\r\n */\r\nclass IndexByteEncoder {\r\n    constructor() {\r\n        this.orderedCode = new OrderedCodeWriter();\r\n        this.ascending = new AscendingIndexByteEncoder(this.orderedCode);\r\n        this.descending = new DescendingIndexByteEncoder(this.orderedCode);\r\n    }\r\n    seed(encodedBytes) {\r\n        this.orderedCode.seed(encodedBytes);\r\n    }\r\n    forKind(kind) {\r\n        return kind === 0 /* ASCENDING */ ? this.ascending : this.descending;\r\n    }\r\n    encodedBytes() {\r\n        return this.orderedCode.encodedBytes();\r\n    }\r\n    reset() {\r\n        this.orderedCode.reset();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Represents an index entry saved by the SDK in persisted storage. */\r\nclass IndexEntry {\r\n    constructor(indexId, documentKey, arrayValue, directionalValue) {\r\n        this.indexId = indexId;\r\n        this.documentKey = documentKey;\r\n        this.arrayValue = arrayValue;\r\n        this.directionalValue = directionalValue;\r\n    }\r\n    /**\r\n     * Returns an IndexEntry entry that sorts immediately after the current\r\n     * directional value.\r\n     */\r\n    successor() {\r\n        const currentLength = this.directionalValue.length;\r\n        const newLength = currentLength === 0 || this.directionalValue[currentLength - 1] === 255\r\n            ? currentLength + 1\r\n            : currentLength;\r\n        const successor = new Uint8Array(newLength);\r\n        successor.set(this.directionalValue, 0);\r\n        if (newLength !== currentLength) {\r\n            successor.set([0], this.directionalValue.length);\r\n        }\r\n        else {\r\n            ++successor[successor.length - 1];\r\n        }\r\n        return new IndexEntry(this.indexId, this.documentKey, this.arrayValue, successor);\r\n    }\r\n}\r\nfunction indexEntryComparator(left, right) {\r\n    let cmp = left.indexId - right.indexId;\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    cmp = compareByteArrays(left.arrayValue, right.arrayValue);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    cmp = compareByteArrays(left.directionalValue, right.directionalValue);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    return DocumentKey.comparator(left.documentKey, right.documentKey);\r\n}\r\nfunction compareByteArrays(left, right) {\r\n    for (let i = 0; i < left.length && i < right.length; ++i) {\r\n        const compare = left[i] - right[i];\r\n        if (compare !== 0) {\r\n            return compare;\r\n        }\r\n    }\r\n    return left.length - right.length;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A light query planner for Firestore.\r\n *\r\n * This class matches a `FieldIndex` against a Firestore Query `Target`. It\r\n * determines whether a given index can be used to serve the specified target.\r\n *\r\n * The following table showcases some possible index configurations:\r\n *\r\n * Query                                               | Index\r\n * -----------------------------------------------------------------------------\r\n * where('a', '==', 'a').where('b', '==', 'b')         | a ASC, b DESC\r\n * where('a', '==', 'a').where('b', '==', 'b')         | a ASC\r\n * where('a', '==', 'a').where('b', '==', 'b')         | b DESC\r\n * where('a', '>=', 'a').orderBy('a')                  | a ASC\r\n * where('a', '>=', 'a').orderBy('a', 'desc')          | a DESC\r\n * where('a', '>=', 'a').orderBy('a').orderBy('b')     | a ASC, b ASC\r\n * where('a', '>=', 'a').orderBy('a').orderBy('b')     | a ASC\r\n * where('a', 'array-contains', 'a').orderBy('b')      | a CONTAINS, b ASCENDING\r\n * where('a', 'array-contains', 'a').orderBy('b')      | a CONTAINS\r\n */\r\nclass TargetIndexMatcher {\r\n    constructor(target) {\r\n        this.collectionId =\r\n            target.collectionGroup != null\r\n                ? target.collectionGroup\r\n                : target.path.lastSegment();\r\n        this.orderBys = target.orderBy;\r\n        this.equalityFilters = [];\r\n        for (const filter of target.filters) {\r\n            const fieldFilter = filter;\r\n            if (fieldFilter.isInequality()) {\r\n                this.inequalityFilter = fieldFilter;\r\n            }\r\n            else {\r\n                this.equalityFilters.push(fieldFilter);\r\n            }\r\n        }\r\n    }\r\n    /**\r\n     * Returns whether the index can be used to serve the TargetIndexMatcher's\r\n     * target.\r\n     *\r\n     * An index is considered capable of serving the target when:\r\n     * - The target uses all index segments for its filters and orderBy clauses.\r\n     *   The target can have additional filter and orderBy clauses, but not\r\n     *   fewer.\r\n     * - If an ArrayContains/ArrayContainsAnyfilter is used, the index must also\r\n     *   have a corresponding `CONTAINS` segment.\r\n     * - All directional index segments can be mapped to the target as a series of\r\n     *   equality filters, a single inequality filter and a series of orderBy\r\n     *   clauses.\r\n     * - The segments that represent the equality filters may appear out of order.\r\n     * - The optional segment for the inequality filter must appear after all\r\n     *   equality segments.\r\n     * - The segments that represent that orderBy clause of the target must appear\r\n     *   in order after all equality and inequality segments. Single orderBy\r\n     *   clauses cannot be skipped, but a continuous orderBy suffix may be\r\n     *   omitted.\r\n     */\r\n    servedByIndex(index) {\r\n        // If there is an array element, find a matching filter.\r\n        const arraySegment = fieldIndexGetArraySegment(index);\r\n        if (arraySegment !== undefined &&\r\n            !this.hasMatchingEqualityFilter(arraySegment)) {\r\n            return false;\r\n        }\r\n        const segments = fieldIndexGetDirectionalSegments(index);\r\n        let segmentIndex = 0;\r\n        let orderBysIndex = 0;\r\n        // Process all equalities first. Equalities can appear out of order.\r\n        for (; segmentIndex < segments.length; ++segmentIndex) {\r\n            // We attempt to greedily match all segments to equality filters. If a\r\n            // filter matches an index segment, we can mark the segment as used.\r\n            // Since it is not possible to use the same field path in both an equality\r\n            // and inequality/oderBy clause, we do not have to consider the possibility\r\n            // that a matching equality segment should instead be used to map to an\r\n            // inequality filter or orderBy clause.\r\n            if (!this.hasMatchingEqualityFilter(segments[segmentIndex])) {\r\n                // If we cannot find a matching filter, we need to verify whether the\r\n                // remaining segments map to the target's inequality and its orderBy\r\n                // clauses.\r\n                break;\r\n            }\r\n        }\r\n        // If we already have processed all segments, all segments are used to serve\r\n        // the equality filters and we do not need to map any segments to the\r\n        // target's inequality and orderBy clauses.\r\n        if (segmentIndex === segments.length) {\r\n            return true;\r\n        }\r\n        // If there is an inequality filter, the next segment must match both the\r\n        // filter and the first orderBy clause.\r\n        if (this.inequalityFilter !== undefined) {\r\n            const segment = segments[segmentIndex];\r\n            if (!this.matchesFilter(this.inequalityFilter, segment) ||\r\n                !this.matchesOrderBy(this.orderBys[orderBysIndex++], segment)) {\r\n                return false;\r\n            }\r\n            ++segmentIndex;\r\n        }\r\n        // All remaining segments need to represent the prefix of the target's\r\n        // orderBy.\r\n        for (; segmentIndex < segments.length; ++segmentIndex) {\r\n            const segment = segments[segmentIndex];\r\n            if (orderBysIndex >= this.orderBys.length ||\r\n                !this.matchesOrderBy(this.orderBys[orderBysIndex++], segment)) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    hasMatchingEqualityFilter(segment) {\r\n        for (const filter of this.equalityFilters) {\r\n            if (this.matchesFilter(filter, segment)) {\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n    matchesFilter(filter, segment) {\r\n        if (filter === undefined || !filter.field.isEqual(segment.fieldPath)) {\r\n            return false;\r\n        }\r\n        const isArrayOperator = filter.op === \"array-contains\" /* ARRAY_CONTAINS */ ||\r\n            filter.op === \"array-contains-any\" /* ARRAY_CONTAINS_ANY */;\r\n        return (segment.kind === 2 /* CONTAINS */) === isArrayOperator;\r\n    }\r\n    matchesOrderBy(orderBy, segment) {\r\n        if (!orderBy.field.isEqual(segment.fieldPath)) {\r\n            return false;\r\n        }\r\n        return ((segment.kind === 0 /* ASCENDING */ &&\r\n            orderBy.dir === \"asc\" /* ASCENDING */) ||\r\n            (segment.kind === 1 /* DESCENDING */ &&\r\n                orderBy.dir === \"desc\" /* DESCENDING */));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An in-memory implementation of IndexManager.\r\n */\r\nclass MemoryIndexManager {\r\n    constructor() {\r\n        this.collectionParentIndex = new MemoryCollectionParentIndex();\r\n    }\r\n    addToCollectionParentIndex(transaction, collectionPath) {\r\n        this.collectionParentIndex.add(collectionPath);\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getCollectionParents(transaction, collectionId) {\r\n        return PersistencePromise.resolve(this.collectionParentIndex.getEntries(collectionId));\r\n    }\r\n    addFieldIndex(transaction, index) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve();\r\n    }\r\n    deleteFieldIndex(transaction, index) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getDocumentsMatchingTarget(transaction, target) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve(null);\r\n    }\r\n    getIndexType(transaction, target) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve(0 /* NONE */);\r\n    }\r\n    getFieldIndexes(transaction, collectionGroup) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve([]);\r\n    }\r\n    getNextCollectionGroupToUpdate(transaction) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve(null);\r\n    }\r\n    getMinOffset(transaction, target) {\r\n        return PersistencePromise.resolve(IndexOffset.min());\r\n    }\r\n    getMinOffsetFromCollectionGroup(transaction, collectionGroup) {\r\n        return PersistencePromise.resolve(IndexOffset.min());\r\n    }\r\n    updateCollectionGroup(transaction, collectionGroup, offset) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve();\r\n    }\r\n    updateIndexEntries(transaction, documents) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve();\r\n    }\r\n}\r\n/**\r\n * Internal implementation of the collection-parent index exposed by MemoryIndexManager.\r\n * Also used for in-memory caching by IndexedDbIndexManager and initial index population\r\n * in indexeddb_schema.ts\r\n */\r\nclass MemoryCollectionParentIndex {\r\n    constructor() {\r\n        this.index = {};\r\n    }\r\n    // Returns false if the entry already existed.\r\n    add(collectionPath) {\r\n        const collectionId = collectionPath.lastSegment();\r\n        const parentPath = collectionPath.popLast();\r\n        const existingParents = this.index[collectionId] ||\r\n            new SortedSet(ResourcePath.comparator);\r\n        const added = !existingParents.has(parentPath);\r\n        this.index[collectionId] = existingParents.add(parentPath);\r\n        return added;\r\n    }\r\n    has(collectionPath) {\r\n        const collectionId = collectionPath.lastSegment();\r\n        const parentPath = collectionPath.popLast();\r\n        const existingParents = this.index[collectionId];\r\n        return existingParents && existingParents.has(parentPath);\r\n    }\r\n    getEntries(collectionId) {\r\n        const parentPaths = this.index[collectionId] ||\r\n            new SortedSet(ResourcePath.comparator);\r\n        return parentPaths.toArray();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst LOG_TAG$f = 'IndexedDbIndexManager';\r\nconst EMPTY_VALUE = new Uint8Array(0);\r\n/**\r\n * A persisted implementation of IndexManager.\r\n *\r\n * PORTING NOTE: Unlike iOS and Android, the Web SDK does not memoize index\r\n * data as it supports multi-tab access.\r\n */\r\nclass IndexedDbIndexManager {\r\n    constructor(user, databaseId) {\r\n        this.user = user;\r\n        this.databaseId = databaseId;\r\n        /**\r\n         * An in-memory copy of the index entries we've already written since the SDK\r\n         * launched. Used to avoid re-writing the same entry repeatedly.\r\n         *\r\n         * This is *NOT* a complete cache of what's in persistence and so can never be\r\n         * used to satisfy reads.\r\n         */\r\n        this.collectionParentsCache = new MemoryCollectionParentIndex();\r\n        /**\r\n         * Maps from a target to its equivalent list of sub-targets. Each sub-target\r\n         * contains only one term from the target's disjunctive normal form (DNF).\r\n         */\r\n        this.targetToDnfSubTargets = new ObjectMap(t => canonifyTarget(t), (l, r) => targetEquals(l, r));\r\n        this.uid = user.uid || '';\r\n    }\r\n    /**\r\n     * Adds a new entry to the collection parent index.\r\n     *\r\n     * Repeated calls for the same collectionPath should be avoided within a\r\n     * transaction as IndexedDbIndexManager only caches writes once a transaction\r\n     * has been committed.\r\n     */\r\n    addToCollectionParentIndex(transaction, collectionPath) {\r\n        if (!this.collectionParentsCache.has(collectionPath)) {\r\n            const collectionId = collectionPath.lastSegment();\r\n            const parentPath = collectionPath.popLast();\r\n            transaction.addOnCommittedListener(() => {\r\n                // Add the collection to the in memory cache only if the transaction was\r\n                // successfully committed.\r\n                this.collectionParentsCache.add(collectionPath);\r\n            });\r\n            const collectionParent = {\r\n                collectionId,\r\n                parent: encodeResourcePath(parentPath)\r\n            };\r\n            return collectionParentsStore(transaction).put(collectionParent);\r\n        }\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getCollectionParents(transaction, collectionId) {\r\n        const parentPaths = [];\r\n        const range = IDBKeyRange.bound([collectionId, ''], [immediateSuccessor(collectionId), ''], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true);\r\n        return collectionParentsStore(transaction)\r\n            .loadAll(range)\r\n            .next(entries => {\r\n            for (const entry of entries) {\r\n                // This collectionId guard shouldn't be necessary (and isn't as long\r\n                // as we're running in a real browser), but there's a bug in\r\n                // indexeddbshim that breaks our range in our tests running in node:\r\n                // https://github.com/axemclion/IndexedDBShim/issues/334\r\n                if (entry.collectionId !== collectionId) {\r\n                    break;\r\n                }\r\n                parentPaths.push(decodeResourcePath(entry.parent));\r\n            }\r\n            return parentPaths;\r\n        });\r\n    }\r\n    addFieldIndex(transaction, index) {\r\n        // TODO(indexing): Verify that the auto-incrementing index ID works in\r\n        // Safari & Firefox.\r\n        const indexes = indexConfigurationStore(transaction);\r\n        const dbIndex = toDbIndexConfiguration(index);\r\n        delete dbIndex.indexId; // `indexId` is auto-populated by IndexedDb\r\n        const result = indexes.add(dbIndex);\r\n        if (index.indexState) {\r\n            const states = indexStateStore(transaction);\r\n            return result.next(indexId => {\r\n                states.put(toDbIndexState(indexId, this.user, index.indexState.sequenceNumber, index.indexState.offset));\r\n            });\r\n        }\r\n        else {\r\n            return result.next();\r\n        }\r\n    }\r\n    deleteFieldIndex(transaction, index) {\r\n        const indexes = indexConfigurationStore(transaction);\r\n        const states = indexStateStore(transaction);\r\n        const entries = indexEntriesStore(transaction);\r\n        return indexes\r\n            .delete(index.indexId)\r\n            .next(() => states.delete(IDBKeyRange.bound([index.indexId], [index.indexId + 1], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true)))\r\n            .next(() => entries.delete(IDBKeyRange.bound([index.indexId], [index.indexId + 1], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true)));\r\n    }\r\n    getDocumentsMatchingTarget(transaction, target) {\r\n        const indexEntries = indexEntriesStore(transaction);\r\n        let canServeTarget = true;\r\n        const indexes = new Map();\r\n        return PersistencePromise.forEach(this.getSubTargets(target), (subTarget) => {\r\n            return this.getFieldIndex(transaction, subTarget).next(index => {\r\n                canServeTarget && (canServeTarget = !!index);\r\n                indexes.set(subTarget, index);\r\n            });\r\n        }).next(() => {\r\n            if (!canServeTarget) {\r\n                return PersistencePromise.resolve(null);\r\n            }\r\n            else {\r\n                let existingKeys = documentKeySet();\r\n                const result = [];\r\n                return PersistencePromise.forEach(indexes, (index, subTarget) => {\r\n                    logDebug(LOG_TAG$f, `Using index ${fieldIndexToString(index)} to execute ${canonifyTarget(target)}`);\r\n                    const arrayValues = targetGetArrayValues(subTarget, index);\r\n                    const notInValues = targetGetNotInValues(subTarget, index);\r\n                    const lowerBound = targetGetLowerBound(subTarget, index);\r\n                    const upperBound = targetGetUpperBound(subTarget, index);\r\n                    const lowerBoundEncoded = this.encodeBound(index, subTarget, lowerBound);\r\n                    const upperBoundEncoded = this.encodeBound(index, subTarget, upperBound);\r\n                    const notInEncoded = this.encodeValues(index, subTarget, notInValues);\r\n                    const indexRanges = this.generateIndexRanges(index.indexId, arrayValues, lowerBoundEncoded, lowerBound.inclusive, upperBoundEncoded, upperBound.inclusive, notInEncoded);\r\n                    return PersistencePromise.forEach(indexRanges, (indexRange) => {\r\n                        return indexEntries\r\n                            .loadFirst(indexRange, target.limit)\r\n                            .next(entries => {\r\n                            entries.forEach(entry => {\r\n                                const documentKey = DocumentKey.fromSegments(entry.documentKey);\r\n                                if (!existingKeys.has(documentKey)) {\r\n                                    existingKeys = existingKeys.add(documentKey);\r\n                                    result.push(documentKey);\r\n                                }\r\n                            });\r\n                        });\r\n                    });\r\n                }).next(() => result);\r\n            }\r\n        });\r\n    }\r\n    getSubTargets(target) {\r\n        let subTargets = this.targetToDnfSubTargets.get(target);\r\n        if (subTargets) {\r\n            return subTargets;\r\n        }\r\n        // TODO(orquery): Implement DNF transform\r\n        subTargets = [target];\r\n        this.targetToDnfSubTargets.set(target, subTargets);\r\n        return subTargets;\r\n    }\r\n    /**\r\n     * Constructs a key range query on `DbIndexEntryStore` that unions all\r\n     * bounds.\r\n     */\r\n    generateIndexRanges(indexId, arrayValues, lowerBounds, lowerBoundInclusive, upperBounds, upperBoundInclusive, notInValues) {\r\n        // The number of total index scans we union together. This is similar to a\r\n        // distributed normal form, but adapted for array values. We create a single\r\n        // index range per value in an ARRAY_CONTAINS or ARRAY_CONTAINS_ANY filter\r\n        // combined with the values from the query bounds.\r\n        const totalScans = (arrayValues != null ? arrayValues.length : 1) *\r\n            Math.max(lowerBounds.length, upperBounds.length);\r\n        const scansPerArrayElement = totalScans / (arrayValues != null ? arrayValues.length : 1);\r\n        const indexRanges = [];\r\n        for (let i = 0; i < totalScans; ++i) {\r\n            const arrayValue = arrayValues\r\n                ? this.encodeSingleElement(arrayValues[i / scansPerArrayElement])\r\n                : EMPTY_VALUE;\r\n            const lowerBound = this.generateLowerBound(indexId, arrayValue, lowerBounds[i % scansPerArrayElement], lowerBoundInclusive);\r\n            const upperBound = this.generateUpperBound(indexId, arrayValue, upperBounds[i % scansPerArrayElement], upperBoundInclusive);\r\n            const notInBound = notInValues.map(notIn => this.generateLowerBound(indexId, arrayValue, notIn, \r\n            /* inclusive= */ true));\r\n            indexRanges.push(...this.createRange(lowerBound, upperBound, notInBound));\r\n        }\r\n        return indexRanges;\r\n    }\r\n    /** Generates the lower bound for `arrayValue` and `directionalValue`. */\r\n    generateLowerBound(indexId, arrayValue, directionalValue, inclusive) {\r\n        const entry = new IndexEntry(indexId, DocumentKey.empty(), arrayValue, directionalValue);\r\n        return inclusive ? entry : entry.successor();\r\n    }\r\n    /** Generates the upper bound for `arrayValue` and `directionalValue`. */\r\n    generateUpperBound(indexId, arrayValue, directionalValue, inclusive) {\r\n        const entry = new IndexEntry(indexId, DocumentKey.empty(), arrayValue, directionalValue);\r\n        return inclusive ? entry.successor() : entry;\r\n    }\r\n    getFieldIndex(transaction, target) {\r\n        const targetIndexMatcher = new TargetIndexMatcher(target);\r\n        const collectionGroup = target.collectionGroup != null\r\n            ? target.collectionGroup\r\n            : target.path.lastSegment();\r\n        return this.getFieldIndexes(transaction, collectionGroup).next(indexes => {\r\n            // Return the index with the most number of segments.\r\n            let index = null;\r\n            for (const candidate of indexes) {\r\n                const matches = targetIndexMatcher.servedByIndex(candidate);\r\n                if (matches &&\r\n                    (!index || candidate.fields.length > index.fields.length)) {\r\n                    index = candidate;\r\n                }\r\n            }\r\n            return index;\r\n        });\r\n    }\r\n    getIndexType(transaction, target) {\r\n        let indexType = 2 /* FULL */;\r\n        return PersistencePromise.forEach(this.getSubTargets(target), (target) => {\r\n            return this.getFieldIndex(transaction, target).next(index => {\r\n                if (!index) {\r\n                    indexType = 0 /* NONE */;\r\n                }\r\n                else if (indexType !== 0 /* NONE */ &&\r\n                    index.fields.length < targetGetSegmentCount(target)) {\r\n                    indexType = 1 /* PARTIAL */;\r\n                }\r\n            });\r\n        }).next(() => indexType);\r\n    }\r\n    /**\r\n     * Returns the byte encoded form of the directional values in the field index.\r\n     * Returns `null` if the document does not have all fields specified in the\r\n     * index.\r\n     */\r\n    encodeDirectionalElements(fieldIndex, document) {\r\n        const encoder = new IndexByteEncoder();\r\n        for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n            const field = document.data.field(segment.fieldPath);\r\n            if (field == null) {\r\n                return null;\r\n            }\r\n            const directionalEncoder = encoder.forKind(segment.kind);\r\n            FirestoreIndexValueWriter.INSTANCE.writeIndexValue(field, directionalEncoder);\r\n        }\r\n        return encoder.encodedBytes();\r\n    }\r\n    /** Encodes a single value to the ascending index format. */\r\n    encodeSingleElement(value) {\r\n        const encoder = new IndexByteEncoder();\r\n        FirestoreIndexValueWriter.INSTANCE.writeIndexValue(value, encoder.forKind(0 /* ASCENDING */));\r\n        return encoder.encodedBytes();\r\n    }\r\n    /**\r\n     * Returns an encoded form of the document key that sorts based on the key\r\n     * ordering of the field index.\r\n     */\r\n    encodeDirectionalKey(fieldIndex, documentKey) {\r\n        const encoder = new IndexByteEncoder();\r\n        FirestoreIndexValueWriter.INSTANCE.writeIndexValue(refValue(this.databaseId, documentKey), encoder.forKind(fieldIndexGetKeyOrder(fieldIndex)));\r\n        return encoder.encodedBytes();\r\n    }\r\n    /**\r\n     * Encodes the given field values according to the specification in `target`.\r\n     * For IN queries, a list of possible values is returned.\r\n     */\r\n    encodeValues(fieldIndex, target, bound) {\r\n        if (bound === null) {\r\n            return [];\r\n        }\r\n        let encoders = [];\r\n        encoders.push(new IndexByteEncoder());\r\n        let boundIdx = 0;\r\n        for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n            const value = bound[boundIdx++];\r\n            for (const encoder of encoders) {\r\n                if (this.isInFilter(target, segment.fieldPath) && isArray(value)) {\r\n                    encoders = this.expandIndexValues(encoders, segment, value);\r\n                }\r\n                else {\r\n                    const directionalEncoder = encoder.forKind(segment.kind);\r\n                    FirestoreIndexValueWriter.INSTANCE.writeIndexValue(value, directionalEncoder);\r\n                }\r\n            }\r\n        }\r\n        return this.getEncodedBytes(encoders);\r\n    }\r\n    /**\r\n     * Encodes the given bounds according to the specification in `target`. For IN\r\n     * queries, a list of possible values is returned.\r\n     */\r\n    encodeBound(fieldIndex, target, bound) {\r\n        return this.encodeValues(fieldIndex, target, bound.position);\r\n    }\r\n    /** Returns the byte representation for the provided encoders. */\r\n    getEncodedBytes(encoders) {\r\n        const result = [];\r\n        for (let i = 0; i < encoders.length; ++i) {\r\n            result[i] = encoders[i].encodedBytes();\r\n        }\r\n        return result;\r\n    }\r\n    /**\r\n     * Creates a separate encoder for each element of an array.\r\n     *\r\n     * The method appends each value to all existing encoders (e.g. filter(\"a\",\r\n     * \"==\", \"a1\").filter(\"b\", \"in\", [\"b1\", \"b2\"]) becomes [\"a1,b1\", \"a1,b2\"]). A\r\n     * list of new encoders is returned.\r\n     */\r\n    expandIndexValues(encoders, segment, value) {\r\n        const prefixes = [...encoders];\r\n        const results = [];\r\n        for (const arrayElement of value.arrayValue.values || []) {\r\n            for (const prefix of prefixes) {\r\n                const clonedEncoder = new IndexByteEncoder();\r\n                clonedEncoder.seed(prefix.encodedBytes());\r\n                FirestoreIndexValueWriter.INSTANCE.writeIndexValue(arrayElement, clonedEncoder.forKind(segment.kind));\r\n                results.push(clonedEncoder);\r\n            }\r\n        }\r\n        return results;\r\n    }\r\n    isInFilter(target, fieldPath) {\r\n        return !!target.filters.find(f => f instanceof FieldFilter &&\r\n            f.field.isEqual(fieldPath) &&\r\n            (f.op === \"in\" /* IN */ || f.op === \"not-in\" /* NOT_IN */));\r\n    }\r\n    getFieldIndexes(transaction, collectionGroup) {\r\n        const indexes = indexConfigurationStore(transaction);\r\n        const states = indexStateStore(transaction);\r\n        return (collectionGroup\r\n            ? indexes.loadAll(DbIndexConfigurationCollectionGroupIndex, IDBKeyRange.bound(collectionGroup, collectionGroup))\r\n            : indexes.loadAll()).next(indexConfigs => {\r\n            const result = [];\r\n            return PersistencePromise.forEach(indexConfigs, (indexConfig) => {\r\n                return states\r\n                    .get([indexConfig.indexId, this.uid])\r\n                    .next(indexState => {\r\n                    result.push(fromDbIndexConfiguration(indexConfig, indexState));\r\n                });\r\n            }).next(() => result);\r\n        });\r\n    }\r\n    getNextCollectionGroupToUpdate(transaction) {\r\n        return this.getFieldIndexes(transaction).next(indexes => {\r\n            if (indexes.length === 0) {\r\n                return null;\r\n            }\r\n            indexes.sort((l, r) => {\r\n                const cmp = l.indexState.sequenceNumber - r.indexState.sequenceNumber;\r\n                return cmp !== 0\r\n                    ? cmp\r\n                    : primitiveComparator(l.collectionGroup, r.collectionGroup);\r\n            });\r\n            return indexes[0].collectionGroup;\r\n        });\r\n    }\r\n    updateCollectionGroup(transaction, collectionGroup, offset) {\r\n        const indexes = indexConfigurationStore(transaction);\r\n        const states = indexStateStore(transaction);\r\n        return this.getNextSequenceNumber(transaction).next(nextSequenceNumber => indexes\r\n            .loadAll(DbIndexConfigurationCollectionGroupIndex, IDBKeyRange.bound(collectionGroup, collectionGroup))\r\n            .next(configs => PersistencePromise.forEach(configs, (config) => states.put(toDbIndexState(config.indexId, this.user, nextSequenceNumber, offset)))));\r\n    }\r\n    updateIndexEntries(transaction, documents) {\r\n        // Porting Note: `getFieldIndexes()` on Web does not cache index lookups as\r\n        // it could be used across different IndexedDB transactions. As any cached\r\n        // data might be invalidated by other multi-tab clients, we can only trust\r\n        // data within a single IndexedDB transaction. We therefore add a cache\r\n        // here.\r\n        const memoizedIndexes = new Map();\r\n        return PersistencePromise.forEach(documents, (key, doc) => {\r\n            const memoizedCollectionIndexes = memoizedIndexes.get(key.collectionGroup);\r\n            const fieldIndexes = memoizedCollectionIndexes\r\n                ? PersistencePromise.resolve(memoizedCollectionIndexes)\r\n                : this.getFieldIndexes(transaction, key.collectionGroup);\r\n            return fieldIndexes.next(fieldIndexes => {\r\n                memoizedIndexes.set(key.collectionGroup, fieldIndexes);\r\n                return PersistencePromise.forEach(fieldIndexes, (fieldIndex) => {\r\n                    return this.getExistingIndexEntries(transaction, key, fieldIndex).next(existingEntries => {\r\n                        const newEntries = this.computeIndexEntries(doc, fieldIndex);\r\n                        if (!existingEntries.isEqual(newEntries)) {\r\n                            return this.updateEntries(transaction, doc, fieldIndex, existingEntries, newEntries);\r\n                        }\r\n                        return PersistencePromise.resolve();\r\n                    });\r\n                });\r\n            });\r\n        });\r\n    }\r\n    addIndexEntry(transaction, document, fieldIndex, indexEntry) {\r\n        const indexEntries = indexEntriesStore(transaction);\r\n        return indexEntries.put({\r\n            indexId: indexEntry.indexId,\r\n            uid: this.uid,\r\n            arrayValue: indexEntry.arrayValue,\r\n            directionalValue: indexEntry.directionalValue,\r\n            orderedDocumentKey: this.encodeDirectionalKey(fieldIndex, document.key),\r\n            documentKey: document.key.path.toArray()\r\n        });\r\n    }\r\n    deleteIndexEntry(transaction, document, fieldIndex, indexEntry) {\r\n        const indexEntries = indexEntriesStore(transaction);\r\n        return indexEntries.delete([\r\n            indexEntry.indexId,\r\n            this.uid,\r\n            indexEntry.arrayValue,\r\n            indexEntry.directionalValue,\r\n            this.encodeDirectionalKey(fieldIndex, document.key),\r\n            document.key.path.toArray()\r\n        ]);\r\n    }\r\n    getExistingIndexEntries(transaction, documentKey, fieldIndex) {\r\n        const indexEntries = indexEntriesStore(transaction);\r\n        let results = new SortedSet(indexEntryComparator);\r\n        return indexEntries\r\n            .iterate({\r\n            index: DbIndexEntryDocumentKeyIndex,\r\n            range: IDBKeyRange.only([\r\n                fieldIndex.indexId,\r\n                this.uid,\r\n                this.encodeDirectionalKey(fieldIndex, documentKey)\r\n            ])\r\n        }, (_, entry) => {\r\n            results = results.add(new IndexEntry(fieldIndex.indexId, documentKey, entry.arrayValue, entry.directionalValue));\r\n        })\r\n            .next(() => results);\r\n    }\r\n    /** Creates the index entries for the given document. */\r\n    computeIndexEntries(document, fieldIndex) {\r\n        let results = new SortedSet(indexEntryComparator);\r\n        const directionalValue = this.encodeDirectionalElements(fieldIndex, document);\r\n        if (directionalValue == null) {\r\n            return results;\r\n        }\r\n        const arraySegment = fieldIndexGetArraySegment(fieldIndex);\r\n        if (arraySegment != null) {\r\n            const value = document.data.field(arraySegment.fieldPath);\r\n            if (isArray(value)) {\r\n                for (const arrayValue of value.arrayValue.values || []) {\r\n                    results = results.add(new IndexEntry(fieldIndex.indexId, document.key, this.encodeSingleElement(arrayValue), directionalValue));\r\n                }\r\n            }\r\n        }\r\n        else {\r\n            results = results.add(new IndexEntry(fieldIndex.indexId, document.key, EMPTY_VALUE, directionalValue));\r\n        }\r\n        return results;\r\n    }\r\n    /**\r\n     * Updates the index entries for the provided document by deleting entries\r\n     * that are no longer referenced in `newEntries` and adding all newly added\r\n     * entries.\r\n     */\r\n    updateEntries(transaction, document, fieldIndex, existingEntries, newEntries) {\r\n        logDebug(LOG_TAG$f, \"Updating index entries for document '%s'\", document.key);\r\n        const promises = [];\r\n        diffSortedSets(existingEntries, newEntries, indexEntryComparator, \r\n        /* onAdd= */ entry => {\r\n            promises.push(this.addIndexEntry(transaction, document, fieldIndex, entry));\r\n        }, \r\n        /* onRemove= */ entry => {\r\n            promises.push(this.deleteIndexEntry(transaction, document, fieldIndex, entry));\r\n        });\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    getNextSequenceNumber(transaction) {\r\n        let nextSequenceNumber = 1;\r\n        const states = indexStateStore(transaction);\r\n        return states\r\n            .iterate({\r\n            index: DbIndexStateSequenceNumberIndex,\r\n            reverse: true,\r\n            range: IDBKeyRange.upperBound([this.uid, Number.MAX_SAFE_INTEGER])\r\n        }, (_, state, controller) => {\r\n            controller.done();\r\n            nextSequenceNumber = state.sequenceNumber + 1;\r\n        })\r\n            .next(() => nextSequenceNumber);\r\n    }\r\n    /**\r\n     * Returns a new set of IDB ranges that splits the existing range and excludes\r\n     * any values that match the `notInValue` from these ranges. As an example,\r\n     * '[foo > 2 && foo != 3]` becomes  `[foo > 2 && < 3, foo > 3]`.\r\n     */\r\n    createRange(lower, upper, notInValues) {\r\n        // The notIn values need to be sorted and unique so that we can return a\r\n        // sorted set of non-overlapping ranges.\r\n        notInValues = notInValues\r\n            .sort((l, r) => indexEntryComparator(l, r))\r\n            .filter((el, i, values) => !i || indexEntryComparator(el, values[i - 1]) !== 0);\r\n        const bounds = [];\r\n        bounds.push(lower);\r\n        for (const notInValue of notInValues) {\r\n            const cmpToLower = indexEntryComparator(notInValue, lower);\r\n            const cmpToUpper = indexEntryComparator(notInValue, upper);\r\n            if (cmpToLower === 0) {\r\n                // `notInValue` is the lower bound. We therefore need to raise the bound\r\n                // to the next value.\r\n                bounds[0] = lower.successor();\r\n            }\r\n            else if (cmpToLower > 0 && cmpToUpper < 0) {\r\n                // `notInValue` is in the middle of the range\r\n                bounds.push(notInValue);\r\n                bounds.push(notInValue.successor());\r\n            }\r\n            else if (cmpToUpper > 0) {\r\n                // `notInValue` (and all following values) are out of the range\r\n                break;\r\n            }\r\n        }\r\n        bounds.push(upper);\r\n        const ranges = [];\r\n        for (let i = 0; i < bounds.length; i += 2) {\r\n            ranges.push(IDBKeyRange.bound([\r\n                bounds[i].indexId,\r\n                this.uid,\r\n                bounds[i].arrayValue,\r\n                bounds[i].directionalValue,\r\n                EMPTY_VALUE,\r\n                []\r\n            ], [\r\n                bounds[i + 1].indexId,\r\n                this.uid,\r\n                bounds[i + 1].arrayValue,\r\n                bounds[i + 1].directionalValue,\r\n                EMPTY_VALUE,\r\n                []\r\n            ]));\r\n        }\r\n        return ranges;\r\n    }\r\n    getMinOffsetFromCollectionGroup(transaction, collectionGroup) {\r\n        return this.getFieldIndexes(transaction, collectionGroup).next(getMinOffsetFromFieldIndexes);\r\n    }\r\n    getMinOffset(transaction, target) {\r\n        return PersistencePromise.mapArray(this.getSubTargets(target), (subTarget) => this.getFieldIndex(transaction, subTarget).next(index => index ? index : fail())).next(getMinOffsetFromFieldIndexes);\r\n    }\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the collectionParents\r\n * document store.\r\n */\r\nfunction collectionParentsStore(txn) {\r\n    return getStore(txn, DbCollectionParentStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the index entry object store.\r\n */\r\nfunction indexEntriesStore(txn) {\r\n    return getStore(txn, DbIndexEntryStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the index configuration object store.\r\n */\r\nfunction indexConfigurationStore(txn) {\r\n    return getStore(txn, DbIndexConfigurationStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the index state object store.\r\n */\r\nfunction indexStateStore(txn) {\r\n    return getStore(txn, DbIndexStateStore);\r\n}\r\nfunction getMinOffsetFromFieldIndexes(fieldIndexes) {\r\n    hardAssert(fieldIndexes.length !== 0);\r\n    let minOffset = fieldIndexes[0].indexState.offset;\r\n    let maxBatchId = minOffset.largestBatchId;\r\n    for (let i = 1; i < fieldIndexes.length; i++) {\r\n        const newOffset = fieldIndexes[i].indexState.offset;\r\n        if (indexOffsetComparator(newOffset, minOffset) < 0) {\r\n            minOffset = newOffset;\r\n        }\r\n        if (maxBatchId < newOffset.largestBatchId) {\r\n            maxBatchId = newOffset.largestBatchId;\r\n        }\r\n    }\r\n    return new IndexOffset(minOffset.readTime, minOffset.documentKey, maxBatchId);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Delete a mutation batch and the associated document mutations.\r\n * @returns A PersistencePromise of the document mutations that were removed.\r\n */\r\nfunction removeMutationBatch(txn, userId, batch) {\r\n    const mutationStore = txn.store(DbMutationBatchStore);\r\n    const indexTxn = txn.store(DbDocumentMutationStore);\r\n    const promises = [];\r\n    const range = IDBKeyRange.only(batch.batchId);\r\n    let numDeleted = 0;\r\n    const removePromise = mutationStore.iterate({ range }, (key, value, control) => {\r\n        numDeleted++;\r\n        return control.delete();\r\n    });\r\n    promises.push(removePromise.next(() => {\r\n        hardAssert(numDeleted === 1);\r\n    }));\r\n    const removedDocuments = [];\r\n    for (const mutation of batch.mutations) {\r\n        const indexKey = newDbDocumentMutationKey(userId, mutation.key.path, batch.batchId);\r\n        promises.push(indexTxn.delete(indexKey));\r\n        removedDocuments.push(mutation.key);\r\n    }\r\n    return PersistencePromise.waitFor(promises).next(() => removedDocuments);\r\n}\r\n/**\r\n * Returns an approximate size for the given document.\r\n */\r\nfunction dbDocumentSize(doc) {\r\n    if (!doc) {\r\n        return 0;\r\n    }\r\n    let value;\r\n    if (doc.document) {\r\n        value = doc.document;\r\n    }\r\n    else if (doc.unknownDocument) {\r\n        value = doc.unknownDocument;\r\n    }\r\n    else if (doc.noDocument) {\r\n        value = doc.noDocument;\r\n    }\r\n    else {\r\n        throw fail();\r\n    }\r\n    return JSON.stringify(value).length;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** A mutation queue for a specific user, backed by IndexedDB. */\r\nclass IndexedDbMutationQueue {\r\n    constructor(\r\n    /**\r\n     * The normalized userId (e.g. null UID => \"\" userId) used to store /\r\n     * retrieve mutations.\r\n     */\r\n    userId, serializer, indexManager, referenceDelegate) {\r\n        this.userId = userId;\r\n        this.serializer = serializer;\r\n        this.indexManager = indexManager;\r\n        this.referenceDelegate = referenceDelegate;\r\n        /**\r\n         * Caches the document keys for pending mutation batches. If the mutation\r\n         * has been removed from IndexedDb, the cached value may continue to\r\n         * be used to retrieve the batch's document keys. To remove a cached value\r\n         * locally, `removeCachedMutationKeys()` should be invoked either directly\r\n         * or through `removeMutationBatches()`.\r\n         *\r\n         * With multi-tab, when the primary client acknowledges or rejects a mutation,\r\n         * this cache is used by secondary clients to invalidate the local\r\n         * view of the documents that were previously affected by the mutation.\r\n         */\r\n        // PORTING NOTE: Multi-tab only.\r\n        this.documentKeysByBatchId = {};\r\n    }\r\n    /**\r\n     * Creates a new mutation queue for the given user.\r\n     * @param user - The user for which to create a mutation queue.\r\n     * @param serializer - The serializer to use when persisting to IndexedDb.\r\n     */\r\n    static forUser(user, serializer, indexManager, referenceDelegate) {\r\n        // TODO(mcg): Figure out what constraints there are on userIDs\r\n        // In particular, are there any reserved characters? are empty ids allowed?\r\n        // For the moment store these together in the same mutations table assuming\r\n        // that empty userIDs aren't allowed.\r\n        hardAssert(user.uid !== '');\r\n        const userId = user.isAuthenticated() ? user.uid : '';\r\n        return new IndexedDbMutationQueue(userId, serializer, indexManager, referenceDelegate);\r\n    }\r\n    checkEmpty(transaction) {\r\n        let empty = true;\r\n        const range = IDBKeyRange.bound([this.userId, Number.NEGATIVE_INFINITY], [this.userId, Number.POSITIVE_INFINITY]);\r\n        return mutationsStore(transaction)\r\n            .iterate({ index: DbMutationBatchUserMutationsIndex, range }, (key, value, control) => {\r\n            empty = false;\r\n            control.done();\r\n        })\r\n            .next(() => empty);\r\n    }\r\n    addMutationBatch(transaction, localWriteTime, baseMutations, mutations) {\r\n        const documentStore = documentMutationsStore(transaction);\r\n        const mutationStore = mutationsStore(transaction);\r\n        // The IndexedDb implementation in Chrome (and Firefox) does not handle\r\n        // compound indices that include auto-generated keys correctly. To ensure\r\n        // that the index entry is added correctly in all browsers, we perform two\r\n        // writes: The first write is used to retrieve the next auto-generated Batch\r\n        // ID, and the second write populates the index and stores the actual\r\n        // mutation batch.\r\n        // See: https://bugs.chromium.org/p/chromium/issues/detail?id=701972\r\n        // We write an empty object to obtain key\r\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n        return mutationStore.add({}).next(batchId => {\r\n            hardAssert(typeof batchId === 'number');\r\n            const batch = new MutationBatch(batchId, localWriteTime, baseMutations, mutations);\r\n            const dbBatch = toDbMutationBatch(this.serializer, this.userId, batch);\r\n            const promises = [];\r\n            let collectionParents = new SortedSet((l, r) => primitiveComparator(l.canonicalString(), r.canonicalString()));\r\n            for (const mutation of mutations) {\r\n                const indexKey = newDbDocumentMutationKey(this.userId, mutation.key.path, batchId);\r\n                collectionParents = collectionParents.add(mutation.key.path.popLast());\r\n                promises.push(mutationStore.put(dbBatch));\r\n                promises.push(documentStore.put(indexKey, DbDocumentMutationPlaceholder));\r\n            }\r\n            collectionParents.forEach(parent => {\r\n                promises.push(this.indexManager.addToCollectionParentIndex(transaction, parent));\r\n            });\r\n            transaction.addOnCommittedListener(() => {\r\n                this.documentKeysByBatchId[batchId] = batch.keys();\r\n            });\r\n            return PersistencePromise.waitFor(promises).next(() => batch);\r\n        });\r\n    }\r\n    lookupMutationBatch(transaction, batchId) {\r\n        return mutationsStore(transaction)\r\n            .get(batchId)\r\n            .next(dbBatch => {\r\n            if (dbBatch) {\r\n                hardAssert(dbBatch.userId === this.userId);\r\n                return fromDbMutationBatch(this.serializer, dbBatch);\r\n            }\r\n            return null;\r\n        });\r\n    }\r\n    /**\r\n     * Returns the document keys for the mutation batch with the given batchId.\r\n     * For primary clients, this method returns `null` after\r\n     * `removeMutationBatches()` has been called. Secondary clients return a\r\n     * cached result until `removeCachedMutationKeys()` is invoked.\r\n     */\r\n    // PORTING NOTE: Multi-tab only.\r\n    lookupMutationKeys(transaction, batchId) {\r\n        if (this.documentKeysByBatchId[batchId]) {\r\n            return PersistencePromise.resolve(this.documentKeysByBatchId[batchId]);\r\n        }\r\n        else {\r\n            return this.lookupMutationBatch(transaction, batchId).next(batch => {\r\n                if (batch) {\r\n                    const keys = batch.keys();\r\n                    this.documentKeysByBatchId[batchId] = keys;\r\n                    return keys;\r\n                }\r\n                else {\r\n                    return null;\r\n                }\r\n            });\r\n        }\r\n    }\r\n    getNextMutationBatchAfterBatchId(transaction, batchId) {\r\n        const nextBatchId = batchId + 1;\r\n        const range = IDBKeyRange.lowerBound([this.userId, nextBatchId]);\r\n        let foundBatch = null;\r\n        return mutationsStore(transaction)\r\n            .iterate({ index: DbMutationBatchUserMutationsIndex, range }, (key, dbBatch, control) => {\r\n            if (dbBatch.userId === this.userId) {\r\n                hardAssert(dbBatch.batchId >= nextBatchId);\r\n                foundBatch = fromDbMutationBatch(this.serializer, dbBatch);\r\n            }\r\n            control.done();\r\n        })\r\n            .next(() => foundBatch);\r\n    }\r\n    getHighestUnacknowledgedBatchId(transaction) {\r\n        const range = IDBKeyRange.upperBound([\r\n            this.userId,\r\n            Number.POSITIVE_INFINITY\r\n        ]);\r\n        let batchId = BATCHID_UNKNOWN;\r\n        return mutationsStore(transaction)\r\n            .iterate({ index: DbMutationBatchUserMutationsIndex, range, reverse: true }, (key, dbBatch, control) => {\r\n            batchId = dbBatch.batchId;\r\n            control.done();\r\n        })\r\n            .next(() => batchId);\r\n    }\r\n    getAllMutationBatches(transaction) {\r\n        const range = IDBKeyRange.bound([this.userId, BATCHID_UNKNOWN], [this.userId, Number.POSITIVE_INFINITY]);\r\n        return mutationsStore(transaction)\r\n            .loadAll(DbMutationBatchUserMutationsIndex, range)\r\n            .next(dbBatches => dbBatches.map(dbBatch => fromDbMutationBatch(this.serializer, dbBatch)));\r\n    }\r\n    getAllMutationBatchesAffectingDocumentKey(transaction, documentKey) {\r\n        // Scan the document-mutation index starting with a prefix starting with\r\n        // the given documentKey.\r\n        const indexPrefix = newDbDocumentMutationPrefixForPath(this.userId, documentKey.path);\r\n        const indexStart = IDBKeyRange.lowerBound(indexPrefix);\r\n        const results = [];\r\n        return documentMutationsStore(transaction)\r\n            .iterate({ range: indexStart }, (indexKey, _, control) => {\r\n            const [userID, encodedPath, batchId] = indexKey;\r\n            // Only consider rows matching exactly the specific key of\r\n            // interest. Note that because we order by path first, and we\r\n            // order terminators before path separators, we'll encounter all\r\n            // the index rows for documentKey contiguously. In particular, all\r\n            // the rows for documentKey will occur before any rows for\r\n            // documents nested in a subcollection beneath documentKey so we\r\n            // can stop as soon as we hit any such row.\r\n            const path = decodeResourcePath(encodedPath);\r\n            if (userID !== this.userId || !documentKey.path.isEqual(path)) {\r\n                control.done();\r\n                return;\r\n            }\r\n            // Look up the mutation batch in the store.\r\n            return mutationsStore(transaction)\r\n                .get(batchId)\r\n                .next(mutation => {\r\n                if (!mutation) {\r\n                    throw fail();\r\n                }\r\n                hardAssert(mutation.userId === this.userId);\r\n                results.push(fromDbMutationBatch(this.serializer, mutation));\r\n            });\r\n        })\r\n            .next(() => results);\r\n    }\r\n    getAllMutationBatchesAffectingDocumentKeys(transaction, documentKeys) {\r\n        let uniqueBatchIDs = new SortedSet(primitiveComparator);\r\n        const promises = [];\r\n        documentKeys.forEach(documentKey => {\r\n            const indexStart = newDbDocumentMutationPrefixForPath(this.userId, documentKey.path);\r\n            const range = IDBKeyRange.lowerBound(indexStart);\r\n            const promise = documentMutationsStore(transaction).iterate({ range }, (indexKey, _, control) => {\r\n                const [userID, encodedPath, batchID] = indexKey;\r\n                // Only consider rows matching exactly the specific key of\r\n                // interest. Note that because we order by path first, and we\r\n                // order terminators before path separators, we'll encounter all\r\n                // the index rows for documentKey contiguously. In particular, all\r\n                // the rows for documentKey will occur before any rows for\r\n                // documents nested in a subcollection beneath documentKey so we\r\n                // can stop as soon as we hit any such row.\r\n                const path = decodeResourcePath(encodedPath);\r\n                if (userID !== this.userId || !documentKey.path.isEqual(path)) {\r\n                    control.done();\r\n                    return;\r\n                }\r\n                uniqueBatchIDs = uniqueBatchIDs.add(batchID);\r\n            });\r\n            promises.push(promise);\r\n        });\r\n        return PersistencePromise.waitFor(promises).next(() => this.lookupMutationBatches(transaction, uniqueBatchIDs));\r\n    }\r\n    getAllMutationBatchesAffectingQuery(transaction, query) {\r\n        const queryPath = query.path;\r\n        const immediateChildrenLength = queryPath.length + 1;\r\n        // TODO(mcg): Actually implement a single-collection query\r\n        //\r\n        // This is actually executing an ancestor query, traversing the whole\r\n        // subtree below the collection which can be horrifically inefficient for\r\n        // some structures. The right way to solve this is to implement the full\r\n        // value index, but that's not in the cards in the near future so this is\r\n        // the best we can do for the moment.\r\n        //\r\n        // Since we don't yet index the actual properties in the mutations, our\r\n        // current approach is to just return all mutation batches that affect\r\n        // documents in the collection being queried.\r\n        const indexPrefix = newDbDocumentMutationPrefixForPath(this.userId, queryPath);\r\n        const indexStart = IDBKeyRange.lowerBound(indexPrefix);\r\n        // Collect up unique batchIDs encountered during a scan of the index. Use a\r\n        // SortedSet to accumulate batch IDs so they can be traversed in order in a\r\n        // scan of the main table.\r\n        let uniqueBatchIDs = new SortedSet(primitiveComparator);\r\n        return documentMutationsStore(transaction)\r\n            .iterate({ range: indexStart }, (indexKey, _, control) => {\r\n            const [userID, encodedPath, batchID] = indexKey;\r\n            const path = decodeResourcePath(encodedPath);\r\n            if (userID !== this.userId || !queryPath.isPrefixOf(path)) {\r\n                control.done();\r\n                return;\r\n            }\r\n            // Rows with document keys more than one segment longer than the\r\n            // query path can't be matches. For example, a query on 'rooms'\r\n            // can't match the document /rooms/abc/messages/xyx.\r\n            // TODO(mcg): we'll need a different scanner when we implement\r\n            // ancestor queries.\r\n            if (path.length !== immediateChildrenLength) {\r\n                return;\r\n            }\r\n            uniqueBatchIDs = uniqueBatchIDs.add(batchID);\r\n        })\r\n            .next(() => this.lookupMutationBatches(transaction, uniqueBatchIDs));\r\n    }\r\n    lookupMutationBatches(transaction, batchIDs) {\r\n        const results = [];\r\n        const promises = [];\r\n        // TODO(rockwood): Implement this using iterate.\r\n        batchIDs.forEach(batchId => {\r\n            promises.push(mutationsStore(transaction)\r\n                .get(batchId)\r\n                .next(mutation => {\r\n                if (mutation === null) {\r\n                    throw fail();\r\n                }\r\n                hardAssert(mutation.userId === this.userId);\r\n                results.push(fromDbMutationBatch(this.serializer, mutation));\r\n            }));\r\n        });\r\n        return PersistencePromise.waitFor(promises).next(() => results);\r\n    }\r\n    removeMutationBatch(transaction, batch) {\r\n        return removeMutationBatch(transaction.simpleDbTransaction, this.userId, batch).next(removedDocuments => {\r\n            transaction.addOnCommittedListener(() => {\r\n                this.removeCachedMutationKeys(batch.batchId);\r\n            });\r\n            return PersistencePromise.forEach(removedDocuments, (key) => {\r\n                return this.referenceDelegate.markPotentiallyOrphaned(transaction, key);\r\n            });\r\n        });\r\n    }\r\n    /**\r\n     * Clears the cached keys for a mutation batch. This method should be\r\n     * called by secondary clients after they process mutation updates.\r\n     *\r\n     * Note that this method does not have to be called from primary clients as\r\n     * the corresponding cache entries are cleared when an acknowledged or\r\n     * rejected batch is removed from the mutation queue.\r\n     */\r\n    // PORTING NOTE: Multi-tab only\r\n    removeCachedMutationKeys(batchId) {\r\n        delete this.documentKeysByBatchId[batchId];\r\n    }\r\n    performConsistencyCheck(txn) {\r\n        return this.checkEmpty(txn).next(empty => {\r\n            if (!empty) {\r\n                return PersistencePromise.resolve();\r\n            }\r\n            // Verify that there are no entries in the documentMutations index if\r\n            // the queue is empty.\r\n            const startRange = IDBKeyRange.lowerBound(newDbDocumentMutationPrefixForUser(this.userId));\r\n            const danglingMutationReferences = [];\r\n            return documentMutationsStore(txn)\r\n                .iterate({ range: startRange }, (key, _, control) => {\r\n                const userID = key[0];\r\n                if (userID !== this.userId) {\r\n                    control.done();\r\n                    return;\r\n                }\r\n                else {\r\n                    const path = decodeResourcePath(key[1]);\r\n                    danglingMutationReferences.push(path);\r\n                }\r\n            })\r\n                .next(() => {\r\n                hardAssert(danglingMutationReferences.length === 0);\r\n            });\r\n        });\r\n    }\r\n    containsKey(txn, key) {\r\n        return mutationQueueContainsKey(txn, this.userId, key);\r\n    }\r\n    // PORTING NOTE: Multi-tab only (state is held in memory in other clients).\r\n    /** Returns the mutation queue's metadata from IndexedDb. */\r\n    getMutationQueueMetadata(transaction) {\r\n        return mutationQueuesStore(transaction)\r\n            .get(this.userId)\r\n            .next((metadata) => {\r\n            return (metadata || {\r\n                userId: this.userId,\r\n                lastAcknowledgedBatchId: BATCHID_UNKNOWN,\r\n                lastStreamToken: ''\r\n            });\r\n        });\r\n    }\r\n}\r\n/**\r\n * @returns true if the mutation queue for the given user contains a pending\r\n *         mutation for the given key.\r\n */\r\nfunction mutationQueueContainsKey(txn, userId, key) {\r\n    const indexKey = newDbDocumentMutationPrefixForPath(userId, key.path);\r\n    const encodedPath = indexKey[1];\r\n    const startRange = IDBKeyRange.lowerBound(indexKey);\r\n    let containsKey = false;\r\n    return documentMutationsStore(txn)\r\n        .iterate({ range: startRange, keysOnly: true }, (key, value, control) => {\r\n        const [userID, keyPath, /*batchID*/ _] = key;\r\n        if (userID === userId && keyPath === encodedPath) {\r\n            containsKey = true;\r\n        }\r\n        control.done();\r\n    })\r\n        .next(() => containsKey);\r\n}\r\n/** Returns true if any mutation queue contains the given document. */\r\nfunction mutationQueuesContainKey(txn, docKey) {\r\n    let found = false;\r\n    return mutationQueuesStore(txn)\r\n        .iterateSerial(userId => {\r\n        return mutationQueueContainsKey(txn, userId, docKey).next(containsKey => {\r\n            if (containsKey) {\r\n                found = true;\r\n            }\r\n            return PersistencePromise.resolve(!containsKey);\r\n        });\r\n    })\r\n        .next(() => found);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the mutations object store.\r\n */\r\nfunction mutationsStore(txn) {\r\n    return getStore(txn, DbMutationBatchStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the mutationQueues object store.\r\n */\r\nfunction documentMutationsStore(txn) {\r\n    return getStore(txn, DbDocumentMutationStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the mutationQueues object store.\r\n */\r\nfunction mutationQueuesStore(txn) {\r\n    return getStore(txn, DbMutationQueueStore);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Offset to ensure non-overlapping target ids. */\r\nconst OFFSET = 2;\r\n/**\r\n * Generates monotonically increasing target IDs for sending targets to the\r\n * watch stream.\r\n *\r\n * The client constructs two generators, one for the target cache, and one for\r\n * for the sync engine (to generate limbo documents targets). These\r\n * generators produce non-overlapping IDs (by using even and odd IDs\r\n * respectively).\r\n *\r\n * By separating the target ID space, the query cache can generate target IDs\r\n * that persist across client restarts, while sync engine can independently\r\n * generate in-memory target IDs that are transient and can be reused after a\r\n * restart.\r\n */\r\nclass TargetIdGenerator {\r\n    constructor(lastId) {\r\n        this.lastId = lastId;\r\n    }\r\n    next() {\r\n        this.lastId += OFFSET;\r\n        return this.lastId;\r\n    }\r\n    static forTargetCache() {\r\n        // The target cache generator must return '2' in its first call to `next()`\r\n        // as there is no differentiation in the protocol layer between an unset\r\n        // number and the number '0'. If we were to sent a target with target ID\r\n        // '0', the backend would consider it unset and replace it with its own ID.\r\n        return new TargetIdGenerator(2 - OFFSET);\r\n    }\r\n    static forSyncEngine() {\r\n        // Sync engine assigns target IDs for limbo document detection.\r\n        return new TargetIdGenerator(1 - OFFSET);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass IndexedDbTargetCache {\r\n    constructor(referenceDelegate, serializer) {\r\n        this.referenceDelegate = referenceDelegate;\r\n        this.serializer = serializer;\r\n    }\r\n    // PORTING NOTE: We don't cache global metadata for the target cache, since\r\n    // some of it (in particular `highestTargetId`) can be modified by secondary\r\n    // tabs. We could perhaps be more granular (and e.g. still cache\r\n    // `lastRemoteSnapshotVersion` in memory) but for simplicity we currently go\r\n    // to IndexedDb whenever we need to read metadata. We can revisit if it turns\r\n    // out to have a meaningful performance impact.\r\n    allocateTargetId(transaction) {\r\n        return this.retrieveMetadata(transaction).next(metadata => {\r\n            const targetIdGenerator = new TargetIdGenerator(metadata.highestTargetId);\r\n            metadata.highestTargetId = targetIdGenerator.next();\r\n            return this.saveMetadata(transaction, metadata).next(() => metadata.highestTargetId);\r\n        });\r\n    }\r\n    getLastRemoteSnapshotVersion(transaction) {\r\n        return this.retrieveMetadata(transaction).next(metadata => {\r\n            return SnapshotVersion.fromTimestamp(new Timestamp(metadata.lastRemoteSnapshotVersion.seconds, metadata.lastRemoteSnapshotVersion.nanoseconds));\r\n        });\r\n    }\r\n    getHighestSequenceNumber(transaction) {\r\n        return this.retrieveMetadata(transaction).next(targetGlobal => targetGlobal.highestListenSequenceNumber);\r\n    }\r\n    setTargetsMetadata(transaction, highestListenSequenceNumber, lastRemoteSnapshotVersion) {\r\n        return this.retrieveMetadata(transaction).next(metadata => {\r\n            metadata.highestListenSequenceNumber = highestListenSequenceNumber;\r\n            if (lastRemoteSnapshotVersion) {\r\n                metadata.lastRemoteSnapshotVersion =\r\n                    lastRemoteSnapshotVersion.toTimestamp();\r\n            }\r\n            if (highestListenSequenceNumber > metadata.highestListenSequenceNumber) {\r\n                metadata.highestListenSequenceNumber = highestListenSequenceNumber;\r\n            }\r\n            return this.saveMetadata(transaction, metadata);\r\n        });\r\n    }\r\n    addTargetData(transaction, targetData) {\r\n        return this.saveTargetData(transaction, targetData).next(() => {\r\n            return this.retrieveMetadata(transaction).next(metadata => {\r\n                metadata.targetCount += 1;\r\n                this.updateMetadataFromTargetData(targetData, metadata);\r\n                return this.saveMetadata(transaction, metadata);\r\n            });\r\n        });\r\n    }\r\n    updateTargetData(transaction, targetData) {\r\n        return this.saveTargetData(transaction, targetData);\r\n    }\r\n    removeTargetData(transaction, targetData) {\r\n        return this.removeMatchingKeysForTargetId(transaction, targetData.targetId)\r\n            .next(() => targetsStore(transaction).delete(targetData.targetId))\r\n            .next(() => this.retrieveMetadata(transaction))\r\n            .next(metadata => {\r\n            hardAssert(metadata.targetCount > 0);\r\n            metadata.targetCount -= 1;\r\n            return this.saveMetadata(transaction, metadata);\r\n        });\r\n    }\r\n    /**\r\n     * Drops any targets with sequence number less than or equal to the upper bound, excepting those\r\n     * present in `activeTargetIds`. Document associations for the removed targets are also removed.\r\n     * Returns the number of targets removed.\r\n     */\r\n    removeTargets(txn, upperBound, activeTargetIds) {\r\n        let count = 0;\r\n        const promises = [];\r\n        return targetsStore(txn)\r\n            .iterate((key, value) => {\r\n            const targetData = fromDbTarget(value);\r\n            if (targetData.sequenceNumber <= upperBound &&\r\n                activeTargetIds.get(targetData.targetId) === null) {\r\n                count++;\r\n                promises.push(this.removeTargetData(txn, targetData));\r\n            }\r\n        })\r\n            .next(() => PersistencePromise.waitFor(promises))\r\n            .next(() => count);\r\n    }\r\n    /**\r\n     * Call provided function with each `TargetData` that we have cached.\r\n     */\r\n    forEachTarget(txn, f) {\r\n        return targetsStore(txn).iterate((key, value) => {\r\n            const targetData = fromDbTarget(value);\r\n            f(targetData);\r\n        });\r\n    }\r\n    retrieveMetadata(transaction) {\r\n        return globalTargetStore(transaction)\r\n            .get(DbTargetGlobalKey)\r\n            .next(metadata => {\r\n            hardAssert(metadata !== null);\r\n            return metadata;\r\n        });\r\n    }\r\n    saveMetadata(transaction, metadata) {\r\n        return globalTargetStore(transaction).put(DbTargetGlobalKey, metadata);\r\n    }\r\n    saveTargetData(transaction, targetData) {\r\n        return targetsStore(transaction).put(toDbTarget(this.serializer, targetData));\r\n    }\r\n    /**\r\n     * In-place updates the provided metadata to account for values in the given\r\n     * TargetData. Saving is done separately. Returns true if there were any\r\n     * changes to the metadata.\r\n     */\r\n    updateMetadataFromTargetData(targetData, metadata) {\r\n        let updated = false;\r\n        if (targetData.targetId > metadata.highestTargetId) {\r\n            metadata.highestTargetId = targetData.targetId;\r\n            updated = true;\r\n        }\r\n        if (targetData.sequenceNumber > metadata.highestListenSequenceNumber) {\r\n            metadata.highestListenSequenceNumber = targetData.sequenceNumber;\r\n            updated = true;\r\n        }\r\n        return updated;\r\n    }\r\n    getTargetCount(transaction) {\r\n        return this.retrieveMetadata(transaction).next(metadata => metadata.targetCount);\r\n    }\r\n    getTargetData(transaction, target) {\r\n        // Iterating by the canonicalId may yield more than one result because\r\n        // canonicalId values are not required to be unique per target. This query\r\n        // depends on the queryTargets index to be efficient.\r\n        const canonicalId = canonifyTarget(target);\r\n        const range = IDBKeyRange.bound([canonicalId, Number.NEGATIVE_INFINITY], [canonicalId, Number.POSITIVE_INFINITY]);\r\n        let result = null;\r\n        return targetsStore(transaction)\r\n            .iterate({ range, index: DbTargetQueryTargetsIndexName }, (key, value, control) => {\r\n            const found = fromDbTarget(value);\r\n            // After finding a potential match, check that the target is\r\n            // actually equal to the requested target.\r\n            if (targetEquals(target, found.target)) {\r\n                result = found;\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => result);\r\n    }\r\n    addMatchingKeys(txn, keys, targetId) {\r\n        // PORTING NOTE: The reverse index (documentsTargets) is maintained by\r\n        // IndexedDb.\r\n        const promises = [];\r\n        const store = documentTargetStore(txn);\r\n        keys.forEach(key => {\r\n            const path = encodeResourcePath(key.path);\r\n            promises.push(store.put({ targetId, path }));\r\n            promises.push(this.referenceDelegate.addReference(txn, targetId, key));\r\n        });\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    removeMatchingKeys(txn, keys, targetId) {\r\n        // PORTING NOTE: The reverse index (documentsTargets) is maintained by\r\n        // IndexedDb.\r\n        const store = documentTargetStore(txn);\r\n        return PersistencePromise.forEach(keys, (key) => {\r\n            const path = encodeResourcePath(key.path);\r\n            return PersistencePromise.waitFor([\r\n                store.delete([targetId, path]),\r\n                this.referenceDelegate.removeReference(txn, targetId, key)\r\n            ]);\r\n        });\r\n    }\r\n    removeMatchingKeysForTargetId(txn, targetId) {\r\n        const store = documentTargetStore(txn);\r\n        const range = IDBKeyRange.bound([targetId], [targetId + 1], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true);\r\n        return store.delete(range);\r\n    }\r\n    getMatchingKeysForTargetId(txn, targetId) {\r\n        const range = IDBKeyRange.bound([targetId], [targetId + 1], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true);\r\n        const store = documentTargetStore(txn);\r\n        let result = documentKeySet();\r\n        return store\r\n            .iterate({ range, keysOnly: true }, (key, _, control) => {\r\n            const path = decodeResourcePath(key[1]);\r\n            const docKey = new DocumentKey(path);\r\n            result = result.add(docKey);\r\n        })\r\n            .next(() => result);\r\n    }\r\n    containsKey(txn, key) {\r\n        const path = encodeResourcePath(key.path);\r\n        const range = IDBKeyRange.bound([path], [immediateSuccessor(path)], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true);\r\n        let count = 0;\r\n        return documentTargetStore(txn)\r\n            .iterate({\r\n            index: DbTargetDocumentDocumentTargetsIndex,\r\n            keysOnly: true,\r\n            range\r\n        }, ([targetId, path], _, control) => {\r\n            // Having a sentinel row for a document does not count as containing that document;\r\n            // For the target cache, containing the document means the document is part of some\r\n            // target.\r\n            if (targetId !== 0) {\r\n                count++;\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => count > 0);\r\n    }\r\n    /**\r\n     * Looks up a TargetData entry by target ID.\r\n     *\r\n     * @param targetId - The target ID of the TargetData entry to look up.\r\n     * @returns The cached TargetData entry, or null if the cache has no entry for\r\n     * the target.\r\n     */\r\n    // PORTING NOTE: Multi-tab only.\r\n    getTargetDataForTarget(transaction, targetId) {\r\n        return targetsStore(transaction)\r\n            .get(targetId)\r\n            .next(found => {\r\n            if (found) {\r\n                return fromDbTarget(found);\r\n            }\r\n            else {\r\n                return null;\r\n            }\r\n        });\r\n    }\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the queries object store.\r\n */\r\nfunction targetsStore(txn) {\r\n    return getStore(txn, DbTargetStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the target globals object store.\r\n */\r\nfunction globalTargetStore(txn) {\r\n    return getStore(txn, DbTargetGlobalStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the document target object store.\r\n */\r\nfunction documentTargetStore(txn) {\r\n    return getStore(txn, DbTargetDocumentStore);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst GC_DID_NOT_RUN = {\r\n    didRun: false,\r\n    sequenceNumbersCollected: 0,\r\n    targetsRemoved: 0,\r\n    documentsRemoved: 0\r\n};\r\nconst LRU_COLLECTION_DISABLED = -1;\r\nconst LRU_DEFAULT_CACHE_SIZE_BYTES = 40 * 1024 * 1024;\r\nclass LruParams {\r\n    constructor(\r\n    // When we attempt to collect, we will only do so if the cache size is greater than this\r\n    // threshold. Passing `COLLECTION_DISABLED` here will cause collection to always be skipped.\r\n    cacheSizeCollectionThreshold, \r\n    // The percentage of sequence numbers that we will attempt to collect\r\n    percentileToCollect, \r\n    // A cap on the total number of sequence numbers that will be collected. This prevents\r\n    // us from collecting a huge number of sequence numbers if the cache has grown very large.\r\n    maximumSequenceNumbersToCollect) {\r\n        this.cacheSizeCollectionThreshold = cacheSizeCollectionThreshold;\r\n        this.percentileToCollect = percentileToCollect;\r\n        this.maximumSequenceNumbersToCollect = maximumSequenceNumbersToCollect;\r\n    }\r\n    static withCacheSize(cacheSize) {\r\n        return new LruParams(cacheSize, LruParams.DEFAULT_COLLECTION_PERCENTILE, LruParams.DEFAULT_MAX_SEQUENCE_NUMBERS_TO_COLLECT);\r\n    }\r\n}\r\nLruParams.DEFAULT_COLLECTION_PERCENTILE = 10;\r\nLruParams.DEFAULT_MAX_SEQUENCE_NUMBERS_TO_COLLECT = 1000;\r\nLruParams.DEFAULT = new LruParams(LRU_DEFAULT_CACHE_SIZE_BYTES, LruParams.DEFAULT_COLLECTION_PERCENTILE, LruParams.DEFAULT_MAX_SEQUENCE_NUMBERS_TO_COLLECT);\r\nLruParams.DISABLED = new LruParams(LRU_COLLECTION_DISABLED, 0, 0);\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst LOG_TAG$e = 'LruGarbageCollector';\r\nconst LRU_MINIMUM_CACHE_SIZE_BYTES = 1 * 1024 * 1024;\r\n/** How long we wait to try running LRU GC after SDK initialization. */\r\nconst INITIAL_GC_DELAY_MS = 1 * 60 * 1000;\r\n/** Minimum amount of time between GC checks, after the first one. */\r\nconst REGULAR_GC_DELAY_MS = 5 * 60 * 1000;\r\nfunction bufferEntryComparator([aSequence, aIndex], [bSequence, bIndex]) {\r\n    const seqCmp = primitiveComparator(aSequence, bSequence);\r\n    if (seqCmp === 0) {\r\n        // This order doesn't matter, but we can bias against churn by sorting\r\n        // entries created earlier as less than newer entries.\r\n        return primitiveComparator(aIndex, bIndex);\r\n    }\r\n    else {\r\n        return seqCmp;\r\n    }\r\n}\r\n/**\r\n * Used to calculate the nth sequence number. Keeps a rolling buffer of the\r\n * lowest n values passed to `addElement`, and finally reports the largest of\r\n * them in `maxValue`.\r\n */\r\nclass RollingSequenceNumberBuffer {\r\n    constructor(maxElements) {\r\n        this.maxElements = maxElements;\r\n        this.buffer = new SortedSet(bufferEntryComparator);\r\n        this.previousIndex = 0;\r\n    }\r\n    nextIndex() {\r\n        return ++this.previousIndex;\r\n    }\r\n    addElement(sequenceNumber) {\r\n        const entry = [sequenceNumber, this.nextIndex()];\r\n        if (this.buffer.size < this.maxElements) {\r\n            this.buffer = this.buffer.add(entry);\r\n        }\r\n        else {\r\n            const highestValue = this.buffer.last();\r\n            if (bufferEntryComparator(entry, highestValue) < 0) {\r\n                this.buffer = this.buffer.delete(highestValue).add(entry);\r\n            }\r\n        }\r\n    }\r\n    get maxValue() {\r\n        // Guaranteed to be non-empty. If we decide we are not collecting any\r\n        // sequence numbers, nthSequenceNumber below short-circuits. If we have\r\n        // decided that we are collecting n sequence numbers, it's because n is some\r\n        // percentage of the existing sequence numbers. That means we should never\r\n        // be in a situation where we are collecting sequence numbers but don't\r\n        // actually have any.\r\n        return this.buffer.last()[0];\r\n    }\r\n}\r\n/**\r\n * This class is responsible for the scheduling of LRU garbage collection. It handles checking\r\n * whether or not GC is enabled, as well as which delay to use before the next run.\r\n */\r\nclass LruScheduler {\r\n    constructor(garbageCollector, asyncQueue, localStore) {\r\n        this.garbageCollector = garbageCollector;\r\n        this.asyncQueue = asyncQueue;\r\n        this.localStore = localStore;\r\n        this.gcTask = null;\r\n    }\r\n    start() {\r\n        if (this.garbageCollector.params.cacheSizeCollectionThreshold !==\r\n            LRU_COLLECTION_DISABLED) {\r\n            this.scheduleGC(INITIAL_GC_DELAY_MS);\r\n        }\r\n    }\r\n    stop() {\r\n        if (this.gcTask) {\r\n            this.gcTask.cancel();\r\n            this.gcTask = null;\r\n        }\r\n    }\r\n    get started() {\r\n        return this.gcTask !== null;\r\n    }\r\n    scheduleGC(delay) {\r\n        logDebug(LOG_TAG$e, `Garbage collection scheduled in ${delay}ms`);\r\n        this.gcTask = this.asyncQueue.enqueueAfterDelay(\"lru_garbage_collection\" /* LruGarbageCollection */, delay, async () => {\r\n            this.gcTask = null;\r\n            try {\r\n                await this.localStore.collectGarbage(this.garbageCollector);\r\n            }\r\n            catch (e) {\r\n                if (isIndexedDbTransactionError(e)) {\r\n                    logDebug(LOG_TAG$e, 'Ignoring IndexedDB error during garbage collection: ', e);\r\n                }\r\n                else {\r\n                    await ignoreIfPrimaryLeaseLoss(e);\r\n                }\r\n            }\r\n            await this.scheduleGC(REGULAR_GC_DELAY_MS);\r\n        });\r\n    }\r\n}\r\n/** Implements the steps for LRU garbage collection. */\r\nclass LruGarbageCollectorImpl {\r\n    constructor(delegate, params) {\r\n        this.delegate = delegate;\r\n        this.params = params;\r\n    }\r\n    calculateTargetCount(txn, percentile) {\r\n        return this.delegate.getSequenceNumberCount(txn).next(targetCount => {\r\n            return Math.floor((percentile / 100.0) * targetCount);\r\n        });\r\n    }\r\n    nthSequenceNumber(txn, n) {\r\n        if (n === 0) {\r\n            return PersistencePromise.resolve(ListenSequence.INVALID);\r\n        }\r\n        const buffer = new RollingSequenceNumberBuffer(n);\r\n        return this.delegate\r\n            .forEachTarget(txn, target => buffer.addElement(target.sequenceNumber))\r\n            .next(() => {\r\n            return this.delegate.forEachOrphanedDocumentSequenceNumber(txn, sequenceNumber => buffer.addElement(sequenceNumber));\r\n        })\r\n            .next(() => buffer.maxValue);\r\n    }\r\n    removeTargets(txn, upperBound, activeTargetIds) {\r\n        return this.delegate.removeTargets(txn, upperBound, activeTargetIds);\r\n    }\r\n    removeOrphanedDocuments(txn, upperBound) {\r\n        return this.delegate.removeOrphanedDocuments(txn, upperBound);\r\n    }\r\n    collect(txn, activeTargetIds) {\r\n        if (this.params.cacheSizeCollectionThreshold === LRU_COLLECTION_DISABLED) {\r\n            logDebug('LruGarbageCollector', 'Garbage collection skipped; disabled');\r\n            return PersistencePromise.resolve(GC_DID_NOT_RUN);\r\n        }\r\n        return this.getCacheSize(txn).next(cacheSize => {\r\n            if (cacheSize < this.params.cacheSizeCollectionThreshold) {\r\n                logDebug('LruGarbageCollector', `Garbage collection skipped; Cache size ${cacheSize} ` +\r\n                    `is lower than threshold ${this.params.cacheSizeCollectionThreshold}`);\r\n                return GC_DID_NOT_RUN;\r\n            }\r\n            else {\r\n                return this.runGarbageCollection(txn, activeTargetIds);\r\n            }\r\n        });\r\n    }\r\n    getCacheSize(txn) {\r\n        return this.delegate.getCacheSize(txn);\r\n    }\r\n    runGarbageCollection(txn, activeTargetIds) {\r\n        let upperBoundSequenceNumber;\r\n        let sequenceNumbersToCollect, targetsRemoved;\r\n        // Timestamps for various pieces of the process\r\n        let countedTargetsTs, foundUpperBoundTs, removedTargetsTs, removedDocumentsTs;\r\n        const startTs = Date.now();\r\n        return this.calculateTargetCount(txn, this.params.percentileToCollect)\r\n            .next(sequenceNumbers => {\r\n            // Cap at the configured max\r\n            if (sequenceNumbers > this.params.maximumSequenceNumbersToCollect) {\r\n                logDebug('LruGarbageCollector', 'Capping sequence numbers to collect down ' +\r\n                    `to the maximum of ${this.params.maximumSequenceNumbersToCollect} ` +\r\n                    `from ${sequenceNumbers}`);\r\n                sequenceNumbersToCollect =\r\n                    this.params.maximumSequenceNumbersToCollect;\r\n            }\r\n            else {\r\n                sequenceNumbersToCollect = sequenceNumbers;\r\n            }\r\n            countedTargetsTs = Date.now();\r\n            return this.nthSequenceNumber(txn, sequenceNumbersToCollect);\r\n        })\r\n            .next(upperBound => {\r\n            upperBoundSequenceNumber = upperBound;\r\n            foundUpperBoundTs = Date.now();\r\n            return this.removeTargets(txn, upperBoundSequenceNumber, activeTargetIds);\r\n        })\r\n            .next(numTargetsRemoved => {\r\n            targetsRemoved = numTargetsRemoved;\r\n            removedTargetsTs = Date.now();\r\n            return this.removeOrphanedDocuments(txn, upperBoundSequenceNumber);\r\n        })\r\n            .next(documentsRemoved => {\r\n            removedDocumentsTs = Date.now();\r\n            if (getLogLevel() <= LogLevel.DEBUG) {\r\n                const desc = 'LRU Garbage Collection\\n' +\r\n                    `\\tCounted targets in ${countedTargetsTs - startTs}ms\\n` +\r\n                    `\\tDetermined least recently used ${sequenceNumbersToCollect} in ` +\r\n                    `${foundUpperBoundTs - countedTargetsTs}ms\\n` +\r\n                    `\\tRemoved ${targetsRemoved} targets in ` +\r\n                    `${removedTargetsTs - foundUpperBoundTs}ms\\n` +\r\n                    `\\tRemoved ${documentsRemoved} documents in ` +\r\n                    `${removedDocumentsTs - removedTargetsTs}ms\\n` +\r\n                    `Total Duration: ${removedDocumentsTs - startTs}ms`;\r\n                logDebug('LruGarbageCollector', desc);\r\n            }\r\n            return PersistencePromise.resolve({\r\n                didRun: true,\r\n                sequenceNumbersCollected: sequenceNumbersToCollect,\r\n                targetsRemoved,\r\n                documentsRemoved\r\n            });\r\n        });\r\n    }\r\n}\r\nfunction newLruGarbageCollector(delegate, params) {\r\n    return new LruGarbageCollectorImpl(delegate, params);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Provides LRU functionality for IndexedDB persistence. */\r\nclass IndexedDbLruDelegateImpl {\r\n    constructor(db, params) {\r\n        this.db = db;\r\n        this.garbageCollector = newLruGarbageCollector(this, params);\r\n    }\r\n    getSequenceNumberCount(txn) {\r\n        const docCountPromise = this.orphanedDocumentCount(txn);\r\n        const targetCountPromise = this.db.getTargetCache().getTargetCount(txn);\r\n        return targetCountPromise.next(targetCount => docCountPromise.next(docCount => targetCount + docCount));\r\n    }\r\n    orphanedDocumentCount(txn) {\r\n        let orphanedCount = 0;\r\n        return this.forEachOrphanedDocumentSequenceNumber(txn, _ => {\r\n            orphanedCount++;\r\n        }).next(() => orphanedCount);\r\n    }\r\n    forEachTarget(txn, f) {\r\n        return this.db.getTargetCache().forEachTarget(txn, f);\r\n    }\r\n    forEachOrphanedDocumentSequenceNumber(txn, f) {\r\n        return this.forEachOrphanedDocument(txn, (docKey, sequenceNumber) => f(sequenceNumber));\r\n    }\r\n    addReference(txn, targetId, key) {\r\n        return writeSentinelKey(txn, key);\r\n    }\r\n    removeReference(txn, targetId, key) {\r\n        return writeSentinelKey(txn, key);\r\n    }\r\n    removeTargets(txn, upperBound, activeTargetIds) {\r\n        return this.db.getTargetCache().removeTargets(txn, upperBound, activeTargetIds);\r\n    }\r\n    markPotentiallyOrphaned(txn, key) {\r\n        return writeSentinelKey(txn, key);\r\n    }\r\n    /**\r\n     * Returns true if anything would prevent this document from being garbage\r\n     * collected, given that the document in question is not present in any\r\n     * targets and has a sequence number less than or equal to the upper bound for\r\n     * the collection run.\r\n     */\r\n    isPinned(txn, docKey) {\r\n        return mutationQueuesContainKey(txn, docKey);\r\n    }\r\n    removeOrphanedDocuments(txn, upperBound) {\r\n        const documentCache = this.db.getRemoteDocumentCache();\r\n        const changeBuffer = documentCache.newChangeBuffer();\r\n        const promises = [];\r\n        let documentCount = 0;\r\n        const iteration = this.forEachOrphanedDocument(txn, (docKey, sequenceNumber) => {\r\n            if (sequenceNumber <= upperBound) {\r\n                const p = this.isPinned(txn, docKey).next(isPinned => {\r\n                    if (!isPinned) {\r\n                        documentCount++;\r\n                        // Our size accounting requires us to read all documents before\r\n                        // removing them.\r\n                        return changeBuffer.getEntry(txn, docKey).next(() => {\r\n                            changeBuffer.removeEntry(docKey, SnapshotVersion.min());\r\n                            return documentTargetStore(txn).delete(sentinelKey$1(docKey));\r\n                        });\r\n                    }\r\n                });\r\n                promises.push(p);\r\n            }\r\n        });\r\n        return iteration\r\n            .next(() => PersistencePromise.waitFor(promises))\r\n            .next(() => changeBuffer.apply(txn))\r\n            .next(() => documentCount);\r\n    }\r\n    removeTarget(txn, targetData) {\r\n        const updated = targetData.withSequenceNumber(txn.currentSequenceNumber);\r\n        return this.db.getTargetCache().updateTargetData(txn, updated);\r\n    }\r\n    updateLimboDocument(txn, key) {\r\n        return writeSentinelKey(txn, key);\r\n    }\r\n    /**\r\n     * Call provided function for each document in the cache that is 'orphaned'. Orphaned\r\n     * means not a part of any target, so the only entry in the target-document index for\r\n     * that document will be the sentinel row (targetId 0), which will also have the sequence\r\n     * number for the last time the document was accessed.\r\n     */\r\n    forEachOrphanedDocument(txn, f) {\r\n        const store = documentTargetStore(txn);\r\n        let nextToReport = ListenSequence.INVALID;\r\n        let nextPath;\r\n        return store\r\n            .iterate({\r\n            index: DbTargetDocumentDocumentTargetsIndex\r\n        }, ([targetId, docKey], { path, sequenceNumber }) => {\r\n            if (targetId === 0) {\r\n                // if nextToReport is valid, report it, this is a new key so the\r\n                // last one must not be a member of any targets.\r\n                if (nextToReport !== ListenSequence.INVALID) {\r\n                    f(new DocumentKey(decodeResourcePath(nextPath)), nextToReport);\r\n                }\r\n                // set nextToReport to be this sequence number. It's the next one we\r\n                // might report, if we don't find any targets for this document.\r\n                // Note that the sequence number must be defined when the targetId\r\n                // is 0.\r\n                nextToReport = sequenceNumber;\r\n                nextPath = path;\r\n            }\r\n            else {\r\n                // set nextToReport to be invalid, we know we don't need to report\r\n                // this one since we found a target for it.\r\n                nextToReport = ListenSequence.INVALID;\r\n            }\r\n        })\r\n            .next(() => {\r\n            // Since we report sequence numbers after getting to the next key, we\r\n            // need to check if the last key we iterated over was an orphaned\r\n            // document and report it.\r\n            if (nextToReport !== ListenSequence.INVALID) {\r\n                f(new DocumentKey(decodeResourcePath(nextPath)), nextToReport);\r\n            }\r\n        });\r\n    }\r\n    getCacheSize(txn) {\r\n        return this.db.getRemoteDocumentCache().getSize(txn);\r\n    }\r\n}\r\nfunction sentinelKey$1(key) {\r\n    return [0, encodeResourcePath(key.path)];\r\n}\r\n/**\r\n * @returns A value suitable for writing a sentinel row in the target-document\r\n * store.\r\n */\r\nfunction sentinelRow(key, sequenceNumber) {\r\n    return { targetId: 0, path: encodeResourcePath(key.path), sequenceNumber };\r\n}\r\nfunction writeSentinelKey(txn, key) {\r\n    return documentTargetStore(txn).put(sentinelRow(key, txn.currentSequenceNumber));\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An in-memory buffer of entries to be written to a RemoteDocumentCache.\r\n * It can be used to batch up a set of changes to be written to the cache, but\r\n * additionally supports reading entries back with the `getEntry()` method,\r\n * falling back to the underlying RemoteDocumentCache if no entry is\r\n * buffered.\r\n *\r\n * Entries added to the cache *must* be read first. This is to facilitate\r\n * calculating the size delta of the pending changes.\r\n *\r\n * PORTING NOTE: This class was implemented then removed from other platforms.\r\n * If byte-counting ends up being needed on the other platforms, consider\r\n * porting this class as part of that implementation work.\r\n */\r\nclass RemoteDocumentChangeBuffer {\r\n    constructor() {\r\n        // A mapping of document key to the new cache entry that should be written.\r\n        this.changes = new ObjectMap(key => key.toString(), (l, r) => l.isEqual(r));\r\n        this.changesApplied = false;\r\n    }\r\n    /**\r\n     * Buffers a `RemoteDocumentCache.addEntry()` call.\r\n     *\r\n     * You can only modify documents that have already been retrieved via\r\n     * `getEntry()/getEntries()` (enforced via IndexedDbs `apply()`).\r\n     */\r\n    addEntry(document) {\r\n        this.assertNotApplied();\r\n        this.changes.set(document.key, document);\r\n    }\r\n    /**\r\n     * Buffers a `RemoteDocumentCache.removeEntry()` call.\r\n     *\r\n     * You can only remove documents that have already been retrieved via\r\n     * `getEntry()/getEntries()` (enforced via IndexedDbs `apply()`).\r\n     */\r\n    removeEntry(key, readTime) {\r\n        this.assertNotApplied();\r\n        this.changes.set(key, MutableDocument.newInvalidDocument(key).setReadTime(readTime));\r\n    }\r\n    /**\r\n     * Looks up an entry in the cache. The buffered changes will first be checked,\r\n     * and if no buffered change applies, this will forward to\r\n     * `RemoteDocumentCache.getEntry()`.\r\n     *\r\n     * @param transaction - The transaction in which to perform any persistence\r\n     *     operations.\r\n     * @param documentKey - The key of the entry to look up.\r\n     * @returns The cached document or an invalid document if we have nothing\r\n     * cached.\r\n     */\r\n    getEntry(transaction, documentKey) {\r\n        this.assertNotApplied();\r\n        const bufferedEntry = this.changes.get(documentKey);\r\n        if (bufferedEntry !== undefined) {\r\n            return PersistencePromise.resolve(bufferedEntry);\r\n        }\r\n        else {\r\n            return this.getFromCache(transaction, documentKey);\r\n        }\r\n    }\r\n    /**\r\n     * Looks up several entries in the cache, forwarding to\r\n     * `RemoteDocumentCache.getEntry()`.\r\n     *\r\n     * @param transaction - The transaction in which to perform any persistence\r\n     *     operations.\r\n     * @param documentKeys - The keys of the entries to look up.\r\n     * @returns A map of cached documents, indexed by key. If an entry cannot be\r\n     *     found, the corresponding key will be mapped to an invalid document.\r\n     */\r\n    getEntries(transaction, documentKeys) {\r\n        return this.getAllFromCache(transaction, documentKeys);\r\n    }\r\n    /**\r\n     * Applies buffered changes to the underlying RemoteDocumentCache, using\r\n     * the provided transaction.\r\n     */\r\n    apply(transaction) {\r\n        this.assertNotApplied();\r\n        this.changesApplied = true;\r\n        return this.applyChanges(transaction);\r\n    }\r\n    /** Helper to assert this.changes is not null  */\r\n    assertNotApplied() {\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * The RemoteDocumentCache for IndexedDb. To construct, invoke\r\n * `newIndexedDbRemoteDocumentCache()`.\r\n */\r\nclass IndexedDbRemoteDocumentCacheImpl {\r\n    constructor(serializer) {\r\n        this.serializer = serializer;\r\n    }\r\n    setIndexManager(indexManager) {\r\n        this.indexManager = indexManager;\r\n    }\r\n    /**\r\n     * Adds the supplied entries to the cache.\r\n     *\r\n     * All calls of `addEntry` are required to go through the RemoteDocumentChangeBuffer\r\n     * returned by `newChangeBuffer()` to ensure proper accounting of metadata.\r\n     */\r\n    addEntry(transaction, key, doc) {\r\n        const documentStore = remoteDocumentsStore(transaction);\r\n        return documentStore.put(doc);\r\n    }\r\n    /**\r\n     * Removes a document from the cache.\r\n     *\r\n     * All calls of `removeEntry`  are required to go through the RemoteDocumentChangeBuffer\r\n     * returned by `newChangeBuffer()` to ensure proper accounting of metadata.\r\n     */\r\n    removeEntry(transaction, documentKey, readTime) {\r\n        const store = remoteDocumentsStore(transaction);\r\n        return store.delete(dbReadTimeKey(documentKey, readTime));\r\n    }\r\n    /**\r\n     * Updates the current cache size.\r\n     *\r\n     * Callers to `addEntry()` and `removeEntry()` *must* call this afterwards to update the\r\n     * cache's metadata.\r\n     */\r\n    updateMetadata(transaction, sizeDelta) {\r\n        return this.getMetadata(transaction).next(metadata => {\r\n            metadata.byteSize += sizeDelta;\r\n            return this.setMetadata(transaction, metadata);\r\n        });\r\n    }\r\n    getEntry(transaction, documentKey) {\r\n        let doc = MutableDocument.newInvalidDocument(documentKey);\r\n        return remoteDocumentsStore(transaction)\r\n            .iterate({\r\n            index: DbRemoteDocumentDocumentKeyIndex,\r\n            range: IDBKeyRange.only(dbKey(documentKey))\r\n        }, (_, dbRemoteDoc) => {\r\n            doc = this.maybeDecodeDocument(documentKey, dbRemoteDoc);\r\n        })\r\n            .next(() => doc);\r\n    }\r\n    /**\r\n     * Looks up an entry in the cache.\r\n     *\r\n     * @param documentKey - The key of the entry to look up.\r\n     * @returns The cached document entry and its size.\r\n     */\r\n    getSizedEntry(transaction, documentKey) {\r\n        let result = {\r\n            size: 0,\r\n            document: MutableDocument.newInvalidDocument(documentKey)\r\n        };\r\n        return remoteDocumentsStore(transaction)\r\n            .iterate({\r\n            index: DbRemoteDocumentDocumentKeyIndex,\r\n            range: IDBKeyRange.only(dbKey(documentKey))\r\n        }, (_, dbRemoteDoc) => {\r\n            result = {\r\n                document: this.maybeDecodeDocument(documentKey, dbRemoteDoc),\r\n                size: dbDocumentSize(dbRemoteDoc)\r\n            };\r\n        })\r\n            .next(() => result);\r\n    }\r\n    getEntries(transaction, documentKeys) {\r\n        let results = mutableDocumentMap();\r\n        return this.forEachDbEntry(transaction, documentKeys, (key, dbRemoteDoc) => {\r\n            const doc = this.maybeDecodeDocument(key, dbRemoteDoc);\r\n            results = results.insert(key, doc);\r\n        }).next(() => results);\r\n    }\r\n    /**\r\n     * Looks up several entries in the cache.\r\n     *\r\n     * @param documentKeys - The set of keys entries to look up.\r\n     * @returns A map of documents indexed by key and a map of sizes indexed by\r\n     *     key (zero if the document does not exist).\r\n     */\r\n    getSizedEntries(transaction, documentKeys) {\r\n        let results = mutableDocumentMap();\r\n        let sizeMap = new SortedMap(DocumentKey.comparator);\r\n        return this.forEachDbEntry(transaction, documentKeys, (key, dbRemoteDoc) => {\r\n            const doc = this.maybeDecodeDocument(key, dbRemoteDoc);\r\n            results = results.insert(key, doc);\r\n            sizeMap = sizeMap.insert(key, dbDocumentSize(dbRemoteDoc));\r\n        }).next(() => {\r\n            return { documents: results, sizeMap };\r\n        });\r\n    }\r\n    forEachDbEntry(transaction, documentKeys, callback) {\r\n        if (documentKeys.isEmpty()) {\r\n            return PersistencePromise.resolve();\r\n        }\r\n        let sortedKeys = new SortedSet(dbKeyComparator);\r\n        documentKeys.forEach(e => (sortedKeys = sortedKeys.add(e)));\r\n        const range = IDBKeyRange.bound(dbKey(sortedKeys.first()), dbKey(sortedKeys.last()));\r\n        const keyIter = sortedKeys.getIterator();\r\n        let nextKey = keyIter.getNext();\r\n        return remoteDocumentsStore(transaction)\r\n            .iterate({ index: DbRemoteDocumentDocumentKeyIndex, range }, (_, dbRemoteDoc, control) => {\r\n            const potentialKey = DocumentKey.fromSegments([\r\n                ...dbRemoteDoc.prefixPath,\r\n                dbRemoteDoc.collectionGroup,\r\n                dbRemoteDoc.documentId\r\n            ]);\r\n            // Go through keys not found in cache.\r\n            while (nextKey && dbKeyComparator(nextKey, potentialKey) < 0) {\r\n                callback(nextKey, null);\r\n                nextKey = keyIter.getNext();\r\n            }\r\n            if (nextKey && nextKey.isEqual(potentialKey)) {\r\n                // Key found in cache.\r\n                callback(nextKey, dbRemoteDoc);\r\n                nextKey = keyIter.hasNext() ? keyIter.getNext() : null;\r\n            }\r\n            // Skip to the next key (if there is one).\r\n            if (nextKey) {\r\n                control.skip(dbKey(nextKey));\r\n            }\r\n            else {\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => {\r\n            // The rest of the keys are not in the cache. One case where `iterate`\r\n            // above won't go through them is when the cache is empty.\r\n            while (nextKey) {\r\n                callback(nextKey, null);\r\n                nextKey = keyIter.hasNext() ? keyIter.getNext() : null;\r\n            }\r\n        });\r\n    }\r\n    getAllFromCollection(transaction, collection, offset) {\r\n        const startKey = [\r\n            collection.popLast().toArray(),\r\n            collection.lastSegment(),\r\n            toDbTimestampKey(offset.readTime),\r\n            offset.documentKey.path.isEmpty()\r\n                ? ''\r\n                : offset.documentKey.path.lastSegment()\r\n        ];\r\n        const endKey = [\r\n            collection.popLast().toArray(),\r\n            collection.lastSegment(),\r\n            [Number.MAX_SAFE_INTEGER, Number.MAX_SAFE_INTEGER],\r\n            ''\r\n        ];\r\n        return remoteDocumentsStore(transaction)\r\n            .loadAll(IDBKeyRange.bound(startKey, endKey, true))\r\n            .next(dbRemoteDocs => {\r\n            let results = mutableDocumentMap();\r\n            for (const dbRemoteDoc of dbRemoteDocs) {\r\n                const document = this.maybeDecodeDocument(DocumentKey.fromSegments(dbRemoteDoc.prefixPath.concat(dbRemoteDoc.collectionGroup, dbRemoteDoc.documentId)), dbRemoteDoc);\r\n                results = results.insert(document.key, document);\r\n            }\r\n            return results;\r\n        });\r\n    }\r\n    getAllFromCollectionGroup(transaction, collectionGroup, offset, limit) {\r\n        let results = mutableDocumentMap();\r\n        const startKey = dbCollectionGroupKey(collectionGroup, offset);\r\n        const endKey = dbCollectionGroupKey(collectionGroup, IndexOffset.max());\r\n        return remoteDocumentsStore(transaction)\r\n            .iterate({\r\n            index: DbRemoteDocumentCollectionGroupIndex,\r\n            range: IDBKeyRange.bound(startKey, endKey, true)\r\n        }, (_, dbRemoteDoc, control) => {\r\n            const document = this.maybeDecodeDocument(DocumentKey.fromSegments(dbRemoteDoc.prefixPath.concat(dbRemoteDoc.collectionGroup, dbRemoteDoc.documentId)), dbRemoteDoc);\r\n            results = results.insert(document.key, document);\r\n            if (results.size === limit) {\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => results);\r\n    }\r\n    newChangeBuffer(options) {\r\n        return new IndexedDbRemoteDocumentChangeBuffer(this, !!options && options.trackRemovals);\r\n    }\r\n    getSize(txn) {\r\n        return this.getMetadata(txn).next(metadata => metadata.byteSize);\r\n    }\r\n    getMetadata(txn) {\r\n        return documentGlobalStore(txn)\r\n            .get(DbRemoteDocumentGlobalKey)\r\n            .next(metadata => {\r\n            hardAssert(!!metadata);\r\n            return metadata;\r\n        });\r\n    }\r\n    setMetadata(txn, metadata) {\r\n        return documentGlobalStore(txn).put(DbRemoteDocumentGlobalKey, metadata);\r\n    }\r\n    /**\r\n     * Decodes `dbRemoteDoc` and returns the document (or an invalid document if\r\n     * the document corresponds to the format used for sentinel deletes).\r\n     */\r\n    maybeDecodeDocument(documentKey, dbRemoteDoc) {\r\n        if (dbRemoteDoc) {\r\n            const doc = fromDbRemoteDocument(this.serializer, dbRemoteDoc);\r\n            // Whether the document is a sentinel removal and should only be used in the\r\n            // `getNewDocumentChanges()`\r\n            const isSentinelRemoval = doc.isNoDocument() && doc.version.isEqual(SnapshotVersion.min());\r\n            if (!isSentinelRemoval) {\r\n                return doc;\r\n            }\r\n        }\r\n        return MutableDocument.newInvalidDocument(documentKey);\r\n    }\r\n}\r\n/** Creates a new IndexedDbRemoteDocumentCache. */\r\nfunction newIndexedDbRemoteDocumentCache(serializer) {\r\n    return new IndexedDbRemoteDocumentCacheImpl(serializer);\r\n}\r\n/**\r\n * Handles the details of adding and updating documents in the IndexedDbRemoteDocumentCache.\r\n *\r\n * Unlike the MemoryRemoteDocumentChangeBuffer, the IndexedDb implementation computes the size\r\n * delta for all submitted changes. This avoids having to re-read all documents from IndexedDb\r\n * when we apply the changes.\r\n */\r\nclass IndexedDbRemoteDocumentChangeBuffer extends RemoteDocumentChangeBuffer {\r\n    /**\r\n     * @param documentCache - The IndexedDbRemoteDocumentCache to apply the changes to.\r\n     * @param trackRemovals - Whether to create sentinel deletes that can be tracked by\r\n     * `getNewDocumentChanges()`.\r\n     */\r\n    constructor(documentCache, trackRemovals) {\r\n        super();\r\n        this.documentCache = documentCache;\r\n        this.trackRemovals = trackRemovals;\r\n        // A map of document sizes and read times prior to applying the changes in\r\n        // this buffer.\r\n        this.documentStates = new ObjectMap(key => key.toString(), (l, r) => l.isEqual(r));\r\n    }\r\n    applyChanges(transaction) {\r\n        const promises = [];\r\n        let sizeDelta = 0;\r\n        let collectionParents = new SortedSet((l, r) => primitiveComparator(l.canonicalString(), r.canonicalString()));\r\n        this.changes.forEach((key, documentChange) => {\r\n            const previousDoc = this.documentStates.get(key);\r\n            promises.push(this.documentCache.removeEntry(transaction, key, previousDoc.readTime));\r\n            if (documentChange.isValidDocument()) {\r\n                const doc = toDbRemoteDocument(this.documentCache.serializer, documentChange);\r\n                collectionParents = collectionParents.add(key.path.popLast());\r\n                const size = dbDocumentSize(doc);\r\n                sizeDelta += size - previousDoc.size;\r\n                promises.push(this.documentCache.addEntry(transaction, key, doc));\r\n            }\r\n            else {\r\n                sizeDelta -= previousDoc.size;\r\n                if (this.trackRemovals) {\r\n                    // In order to track removals, we store a \"sentinel delete\" in the\r\n                    // RemoteDocumentCache. This entry is represented by a NoDocument\r\n                    // with a version of 0 and ignored by `maybeDecodeDocument()` but\r\n                    // preserved in `getNewDocumentChanges()`.\r\n                    const deletedDoc = toDbRemoteDocument(this.documentCache.serializer, documentChange.convertToNoDocument(SnapshotVersion.min()));\r\n                    promises.push(this.documentCache.addEntry(transaction, key, deletedDoc));\r\n                }\r\n            }\r\n        });\r\n        collectionParents.forEach(parent => {\r\n            promises.push(this.documentCache.indexManager.addToCollectionParentIndex(transaction, parent));\r\n        });\r\n        promises.push(this.documentCache.updateMetadata(transaction, sizeDelta));\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    getFromCache(transaction, documentKey) {\r\n        // Record the size of everything we load from the cache so we can compute a delta later.\r\n        return this.documentCache\r\n            .getSizedEntry(transaction, documentKey)\r\n            .next(getResult => {\r\n            this.documentStates.set(documentKey, {\r\n                size: getResult.size,\r\n                readTime: getResult.document.readTime\r\n            });\r\n            return getResult.document;\r\n        });\r\n    }\r\n    getAllFromCache(transaction, documentKeys) {\r\n        // Record the size of everything we load from the cache so we can compute\r\n        // a delta later.\r\n        return this.documentCache\r\n            .getSizedEntries(transaction, documentKeys)\r\n            .next(({ documents, sizeMap }) => {\r\n            // Note: `getAllFromCache` returns two maps instead of a single map from\r\n            // keys to `DocumentSizeEntry`s. This is to allow returning the\r\n            // `MutableDocumentMap` directly, without a conversion.\r\n            sizeMap.forEach((documentKey, size) => {\r\n                this.documentStates.set(documentKey, {\r\n                    size,\r\n                    readTime: documents.get(documentKey).readTime\r\n                });\r\n            });\r\n            return documents;\r\n        });\r\n    }\r\n}\r\nfunction documentGlobalStore(txn) {\r\n    return getStore(txn, DbRemoteDocumentGlobalStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the remoteDocuments object store.\r\n */\r\nfunction remoteDocumentsStore(txn) {\r\n    return getStore(txn, DbRemoteDocumentStore);\r\n}\r\n/**\r\n * Returns a key that can be used for document lookups on the\r\n * `DbRemoteDocumentDocumentKeyIndex` index.\r\n */\r\nfunction dbKey(documentKey) {\r\n    const path = documentKey.path.toArray();\r\n    return [\r\n        /* prefix path */ path.slice(0, path.length - 2),\r\n        /* collection id */ path[path.length - 2],\r\n        /* document id */ path[path.length - 1]\r\n    ];\r\n}\r\n/**\r\n * Returns a key that can be used for document lookups via the primary key of\r\n * the DbRemoteDocument object store.\r\n */\r\nfunction dbReadTimeKey(documentKey, readTime) {\r\n    const path = documentKey.path.toArray();\r\n    return [\r\n        /* prefix path */ path.slice(0, path.length - 2),\r\n        /* collection id */ path[path.length - 2],\r\n        toDbTimestampKey(readTime),\r\n        /* document id */ path[path.length - 1]\r\n    ];\r\n}\r\n/**\r\n * Returns a key that can be used for document lookups on the\r\n * `DbRemoteDocumentDocumentCollectionGroupIndex` index.\r\n */\r\nfunction dbCollectionGroupKey(collectionGroup, offset) {\r\n    const path = offset.documentKey.path.toArray();\r\n    return [\r\n        /* collection id */ collectionGroup,\r\n        toDbTimestampKey(offset.readTime),\r\n        /* prefix path */ path.slice(0, path.length - 2),\r\n        /* document id */ path.length > 0 ? path[path.length - 1] : ''\r\n    ];\r\n}\r\n/**\r\n * Comparator that compares document keys according to the primary key sorting\r\n * used by the `DbRemoteDocumentDocument` store (by prefix path, collection id\r\n * and then document ID).\r\n *\r\n * Visible for testing.\r\n */\r\nfunction dbKeyComparator(l, r) {\r\n    const left = l.path.toArray();\r\n    const right = r.path.toArray();\r\n    // The ordering is based on https://chromium.googlesource.com/chromium/blink/+/fe5c21fef94dae71c1c3344775b8d8a7f7e6d9ec/Source/modules/indexeddb/IDBKey.cpp#74\r\n    let cmp = 0;\r\n    for (let i = 0; i < left.length - 2 && i < right.length - 2; ++i) {\r\n        cmp = primitiveComparator(left[i], right[i]);\r\n        if (cmp) {\r\n            return cmp;\r\n        }\r\n    }\r\n    cmp = primitiveComparator(left.length, right.length);\r\n    if (cmp) {\r\n        return cmp;\r\n    }\r\n    cmp = primitiveComparator(left[left.length - 2], right[right.length - 2]);\r\n    if (cmp) {\r\n        return cmp;\r\n    }\r\n    return primitiveComparator(left[left.length - 1], right[right.length - 1]);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Schema Version for the Web client:\r\n * 1.  Initial version including Mutation Queue, Query Cache, and Remote\r\n *     Document Cache\r\n * 2.  Used to ensure a targetGlobal object exists and add targetCount to it. No\r\n *     longer required because migration 3 unconditionally clears it.\r\n * 3.  Dropped and re-created Query Cache to deal with cache corruption related\r\n *     to limbo resolution. Addresses\r\n *     https://github.com/firebase/firebase-ios-sdk/issues/1548\r\n * 4.  Multi-Tab Support.\r\n * 5.  Removal of held write acks.\r\n * 6.  Create document global for tracking document cache size.\r\n * 7.  Ensure every cached document has a sentinel row with a sequence number.\r\n * 8.  Add collection-parent index for Collection Group queries.\r\n * 9.  Change RemoteDocumentChanges store to be keyed by readTime rather than\r\n *     an auto-incrementing ID. This is required for Index-Free queries.\r\n * 10. Rewrite the canonical IDs to the explicit Protobuf-based format.\r\n * 11. Add bundles and named_queries for bundle support.\r\n * 12. Add document overlays.\r\n * 13. Rewrite the keys of the remote document cache to allow for efficient\r\n *     document lookup via `getAll()`.\r\n * 14. Add overlays.\r\n * 15. Add indexing support.\r\n */\r\nconst SCHEMA_VERSION = 15;\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Represents a local view (overlay) of a document, and the fields that are\r\n * locally mutated.\r\n */\r\nclass OverlayedDocument {\r\n    constructor(overlayedDocument, \r\n    /**\r\n     * The fields that are locally mutated by patch mutations. If the overlayed\r\n     * document is from set or delete mutations, this returns null.\r\n     */\r\n    mutatedFields) {\r\n        this.overlayedDocument = overlayedDocument;\r\n        this.mutatedFields = mutatedFields;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A readonly view of the local state of all documents we're tracking (i.e. we\r\n * have a cached version in remoteDocumentCache or local mutations for the\r\n * document). The view is computed by applying the mutations in the\r\n * MutationQueue to the RemoteDocumentCache.\r\n */\r\nclass LocalDocumentsView {\r\n    constructor(remoteDocumentCache, mutationQueue, documentOverlayCache, indexManager) {\r\n        this.remoteDocumentCache = remoteDocumentCache;\r\n        this.mutationQueue = mutationQueue;\r\n        this.documentOverlayCache = documentOverlayCache;\r\n        this.indexManager = indexManager;\r\n    }\r\n    /**\r\n     * Get the local view of the document identified by `key`.\r\n     *\r\n     * @returns Local view of the document or null if we don't have any cached\r\n     * state for it.\r\n     */\r\n    getDocument(transaction, key) {\r\n        let overlay = null;\r\n        return this.documentOverlayCache\r\n            .getOverlay(transaction, key)\r\n            .next(value => {\r\n            overlay = value;\r\n            return this.getBaseDocument(transaction, key, overlay);\r\n        })\r\n            .next(document => {\r\n            if (overlay !== null) {\r\n                mutationApplyToLocalView(overlay.mutation, document, FieldMask.empty(), Timestamp.now());\r\n            }\r\n            return document;\r\n        });\r\n    }\r\n    /**\r\n     * Gets the local view of the documents identified by `keys`.\r\n     *\r\n     * If we don't have cached state for a document in `keys`, a NoDocument will\r\n     * be stored for that key in the resulting set.\r\n     */\r\n    getDocuments(transaction, keys) {\r\n        return this.remoteDocumentCache\r\n            .getEntries(transaction, keys)\r\n            .next(docs => this.getLocalViewOfDocuments(transaction, docs, documentKeySet()).next(() => docs));\r\n    }\r\n    /**\r\n     * Similar to `getDocuments`, but creates the local view from the given\r\n     * `baseDocs` without retrieving documents from the local store.\r\n     *\r\n     * @param transaction - The transaction this operation is scoped to.\r\n     * @param docs - The documents to apply local mutations to get the local views.\r\n     * @param existenceStateChanged - The set of document keys whose existence state\r\n     *   is changed. This is useful to determine if some documents overlay needs\r\n     *   to be recalculated.\r\n     */\r\n    getLocalViewOfDocuments(transaction, docs, existenceStateChanged = documentKeySet()) {\r\n        const overlays = newOverlayMap();\r\n        return this.populateOverlays(transaction, overlays, docs).next(() => {\r\n            return this.computeViews(transaction, docs, overlays, existenceStateChanged).next(computeViewsResult => {\r\n                let result = documentMap();\r\n                computeViewsResult.forEach((documentKey, overlayedDocument) => {\r\n                    result = result.insert(documentKey, overlayedDocument.overlayedDocument);\r\n                });\r\n                return result;\r\n            });\r\n        });\r\n    }\r\n    /**\r\n     * Gets the overlayed documents for the given document map, which will include\r\n     * the local view of those documents and a `FieldMask` indicating which fields\r\n     * are mutated locally, `null` if overlay is a Set or Delete mutation.\r\n     */\r\n    getOverlayedDocuments(transaction, docs) {\r\n        const overlays = newOverlayMap();\r\n        return this.populateOverlays(transaction, overlays, docs).next(() => this.computeViews(transaction, docs, overlays, documentKeySet()));\r\n    }\r\n    /**\r\n     * Fetches the overlays for {@code docs} and adds them to provided overlay map\r\n     * if the map does not already contain an entry for the given document key.\r\n     */\r\n    populateOverlays(transaction, overlays, docs) {\r\n        const missingOverlays = [];\r\n        docs.forEach(key => {\r\n            if (!overlays.has(key)) {\r\n                missingOverlays.push(key);\r\n            }\r\n        });\r\n        return this.documentOverlayCache\r\n            .getOverlays(transaction, missingOverlays)\r\n            .next(result => {\r\n            result.forEach((key, val) => {\r\n                overlays.set(key, val);\r\n            });\r\n        });\r\n    }\r\n    /**\r\n     * Computes the local view for the given documents.\r\n     *\r\n     * @param docs - The documents to compute views for. It also has the base\r\n     *   version of the documents.\r\n     * @param overlays - The overlays that need to be applied to the given base\r\n     *   version of the documents.\r\n     * @param existenceStateChanged - A set of documents whose existence states\r\n     *   might have changed. This is used to determine if we need to re-calculate\r\n     *   overlays from mutation queues.\r\n     * @return A map represents the local documents view.\r\n     */\r\n    computeViews(transaction, docs, overlays, existenceStateChanged) {\r\n        let recalculateDocuments = mutableDocumentMap();\r\n        const mutatedFields = newDocumentKeyMap();\r\n        const results = newOverlayedDocumentMap();\r\n        docs.forEach((_, doc) => {\r\n            const overlay = overlays.get(doc.key);\r\n            // Recalculate an overlay if the document's existence state changed due to\r\n            // a remote event *and* the overlay is a PatchMutation. This is because\r\n            // document existence state can change if some patch mutation's\r\n            // preconditions are met.\r\n            // NOTE: we recalculate when `overlay` is undefined as well, because there\r\n            // might be a patch mutation whose precondition does not match before the\r\n            // change (hence overlay is undefined), but would now match.\r\n            if (existenceStateChanged.has(doc.key) &&\r\n                (overlay === undefined || overlay.mutation instanceof PatchMutation)) {\r\n                recalculateDocuments = recalculateDocuments.insert(doc.key, doc);\r\n            }\r\n            else if (overlay !== undefined) {\r\n                mutatedFields.set(doc.key, overlay.mutation.getFieldMask());\r\n                mutationApplyToLocalView(overlay.mutation, doc, overlay.mutation.getFieldMask(), Timestamp.now());\r\n            }\r\n        });\r\n        return this.recalculateAndSaveOverlays(transaction, recalculateDocuments).next(recalculatedFields => {\r\n            recalculatedFields.forEach((documentKey, mask) => mutatedFields.set(documentKey, mask));\r\n            docs.forEach((documentKey, document) => {\r\n                var _a;\r\n                return results.set(documentKey, new OverlayedDocument(document, (_a = mutatedFields.get(documentKey)) !== null && _a !== void 0 ? _a : null));\r\n            });\r\n            return results;\r\n        });\r\n    }\r\n    recalculateAndSaveOverlays(transaction, docs) {\r\n        const masks = newDocumentKeyMap();\r\n        // A reverse lookup map from batch id to the documents within that batch.\r\n        let documentsByBatchId = new SortedMap((key1, key2) => key1 - key2);\r\n        let processed = documentKeySet();\r\n        return this.mutationQueue\r\n            .getAllMutationBatchesAffectingDocumentKeys(transaction, docs)\r\n            .next(batches => {\r\n            for (const batch of batches) {\r\n                batch.keys().forEach(key => {\r\n                    const baseDoc = docs.get(key);\r\n                    if (baseDoc === null) {\r\n                        return;\r\n                    }\r\n                    let mask = masks.get(key) || FieldMask.empty();\r\n                    mask = batch.applyToLocalView(baseDoc, mask);\r\n                    masks.set(key, mask);\r\n                    const newSet = (documentsByBatchId.get(batch.batchId) || documentKeySet()).add(key);\r\n                    documentsByBatchId = documentsByBatchId.insert(batch.batchId, newSet);\r\n                });\r\n            }\r\n        })\r\n            .next(() => {\r\n            const promises = [];\r\n            // Iterate in descending order of batch IDs, and skip documents that are\r\n            // already saved.\r\n            const iter = documentsByBatchId.getReverseIterator();\r\n            while (iter.hasNext()) {\r\n                const entry = iter.getNext();\r\n                const batchId = entry.key;\r\n                const keys = entry.value;\r\n                const overlays = newMutationMap();\r\n                keys.forEach(key => {\r\n                    if (!processed.has(key)) {\r\n                        const overlayMutation = calculateOverlayMutation(docs.get(key), masks.get(key));\r\n                        if (overlayMutation !== null) {\r\n                            overlays.set(key, overlayMutation);\r\n                        }\r\n                        processed = processed.add(key);\r\n                    }\r\n                });\r\n                promises.push(this.documentOverlayCache.saveOverlays(transaction, batchId, overlays));\r\n            }\r\n            return PersistencePromise.waitFor(promises);\r\n        })\r\n            .next(() => masks);\r\n    }\r\n    /**\r\n     * Recalculates overlays by reading the documents from remote document cache\r\n     * first, and saves them after they are calculated.\r\n     */\r\n    recalculateAndSaveOverlaysForDocumentKeys(transaction, documentKeys) {\r\n        return this.remoteDocumentCache\r\n            .getEntries(transaction, documentKeys)\r\n            .next(docs => this.recalculateAndSaveOverlays(transaction, docs));\r\n    }\r\n    /**\r\n     * Performs a query against the local view of all documents.\r\n     *\r\n     * @param transaction - The persistence transaction.\r\n     * @param query - The query to match documents against.\r\n     * @param offset - Read time and key to start scanning by (exclusive).\r\n     */\r\n    getDocumentsMatchingQuery(transaction, query, offset) {\r\n        if (isDocumentQuery$1(query)) {\r\n            return this.getDocumentsMatchingDocumentQuery(transaction, query.path);\r\n        }\r\n        else if (isCollectionGroupQuery(query)) {\r\n            return this.getDocumentsMatchingCollectionGroupQuery(transaction, query, offset);\r\n        }\r\n        else {\r\n            return this.getDocumentsMatchingCollectionQuery(transaction, query, offset);\r\n        }\r\n    }\r\n    /**\r\n     * Given a collection group, returns the next documents that follow the provided offset, along\r\n     * with an updated batch ID.\r\n     *\r\n     * <p>The documents returned by this method are ordered by remote version from the provided\r\n     * offset. If there are no more remote documents after the provided offset, documents with\r\n     * mutations in order of batch id from the offset are returned. Since all documents in a batch are\r\n     * returned together, the total number of documents returned can exceed {@code count}.\r\n     *\r\n     * @param transaction\r\n     * @param collectionGroup The collection group for the documents.\r\n     * @param offset The offset to index into.\r\n     * @param count The number of documents to return\r\n     * @return A LocalWriteResult with the documents that follow the provided offset and the last processed batch id.\r\n     */\r\n    getNextDocuments(transaction, collectionGroup, offset, count) {\r\n        return this.remoteDocumentCache\r\n            .getAllFromCollectionGroup(transaction, collectionGroup, offset, count)\r\n            .next((originalDocs) => {\r\n            const overlaysPromise = count - originalDocs.size > 0\r\n                ? this.documentOverlayCache.getOverlaysForCollectionGroup(transaction, collectionGroup, offset.largestBatchId, count - originalDocs.size)\r\n                : PersistencePromise.resolve(newOverlayMap());\r\n            // The callsite will use the largest batch ID together with the latest read time to create\r\n            // a new index offset. Since we only process batch IDs if all remote documents have been read,\r\n            // no overlay will increase the overall read time. This is why we only need to special case\r\n            // the batch id.\r\n            let largestBatchId = INITIAL_LARGEST_BATCH_ID;\r\n            let modifiedDocs = originalDocs;\r\n            return overlaysPromise.next(overlays => {\r\n                return PersistencePromise.forEach(overlays, (key, overlay) => {\r\n                    if (largestBatchId < overlay.largestBatchId) {\r\n                        largestBatchId = overlay.largestBatchId;\r\n                    }\r\n                    if (originalDocs.get(key)) {\r\n                        return PersistencePromise.resolve();\r\n                    }\r\n                    return this.getBaseDocument(transaction, key, overlay).next(doc => {\r\n                        modifiedDocs = modifiedDocs.insert(key, doc);\r\n                    });\r\n                })\r\n                    .next(() => this.populateOverlays(transaction, overlays, originalDocs))\r\n                    .next(() => this.computeViews(transaction, modifiedDocs, overlays, documentKeySet()))\r\n                    .next(localDocs => ({\r\n                    batchId: largestBatchId,\r\n                    changes: convertOverlayedDocumentMapToDocumentMap(localDocs)\r\n                }));\r\n            });\r\n        });\r\n    }\r\n    getDocumentsMatchingDocumentQuery(transaction, docPath) {\r\n        // Just do a simple document lookup.\r\n        return this.getDocument(transaction, new DocumentKey(docPath)).next(document => {\r\n            let result = documentMap();\r\n            if (document.isFoundDocument()) {\r\n                result = result.insert(document.key, document);\r\n            }\r\n            return result;\r\n        });\r\n    }\r\n    getDocumentsMatchingCollectionGroupQuery(transaction, query, offset) {\r\n        const collectionId = query.collectionGroup;\r\n        let results = documentMap();\r\n        return this.indexManager\r\n            .getCollectionParents(transaction, collectionId)\r\n            .next(parents => {\r\n            // Perform a collection query against each parent that contains the\r\n            // collectionId and aggregate the results.\r\n            return PersistencePromise.forEach(parents, (parent) => {\r\n                const collectionQuery = asCollectionQueryAtPath(query, parent.child(collectionId));\r\n                return this.getDocumentsMatchingCollectionQuery(transaction, collectionQuery, offset).next(r => {\r\n                    r.forEach((key, doc) => {\r\n                        results = results.insert(key, doc);\r\n                    });\r\n                });\r\n            }).next(() => results);\r\n        });\r\n    }\r\n    getDocumentsMatchingCollectionQuery(transaction, query, offset) {\r\n        // Query the remote documents and overlay mutations.\r\n        let remoteDocuments;\r\n        return this.remoteDocumentCache\r\n            .getAllFromCollection(transaction, query.path, offset)\r\n            .next(queryResults => {\r\n            remoteDocuments = queryResults;\r\n            return this.documentOverlayCache.getOverlaysForCollection(transaction, query.path, offset.largestBatchId);\r\n        })\r\n            .next(overlays => {\r\n            // As documents might match the query because of their overlay we need to\r\n            // include documents for all overlays in the initial document set.\r\n            overlays.forEach((_, overlay) => {\r\n                const key = overlay.getKey();\r\n                if (remoteDocuments.get(key) === null) {\r\n                    remoteDocuments = remoteDocuments.insert(key, MutableDocument.newInvalidDocument(key));\r\n                }\r\n            });\r\n            // Apply the overlays and match against the query.\r\n            let results = documentMap();\r\n            remoteDocuments.forEach((key, document) => {\r\n                const overlay = overlays.get(key);\r\n                if (overlay !== undefined) {\r\n                    mutationApplyToLocalView(overlay.mutation, document, FieldMask.empty(), Timestamp.now());\r\n                }\r\n                // Finally, insert the documents that still match the query\r\n                if (queryMatches(query, document)) {\r\n                    results = results.insert(key, document);\r\n                }\r\n            });\r\n            return results;\r\n        });\r\n    }\r\n    /** Returns a base document that can be used to apply `overlay`. */\r\n    getBaseDocument(transaction, key, overlay) {\r\n        return overlay === null || overlay.mutation.type === 1 /* Patch */\r\n            ? this.remoteDocumentCache.getEntry(transaction, key)\r\n            : PersistencePromise.resolve(MutableDocument.newInvalidDocument(key));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass MemoryBundleCache {\r\n    constructor(serializer) {\r\n        this.serializer = serializer;\r\n        this.bundles = new Map();\r\n        this.namedQueries = new Map();\r\n    }\r\n    getBundleMetadata(transaction, bundleId) {\r\n        return PersistencePromise.resolve(this.bundles.get(bundleId));\r\n    }\r\n    saveBundleMetadata(transaction, bundleMetadata) {\r\n        this.bundles.set(bundleMetadata.id, fromBundleMetadata(bundleMetadata));\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getNamedQuery(transaction, queryName) {\r\n        return PersistencePromise.resolve(this.namedQueries.get(queryName));\r\n    }\r\n    saveNamedQuery(transaction, query) {\r\n        this.namedQueries.set(query.name, fromProtoNamedQuery(query));\r\n        return PersistencePromise.resolve();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An in-memory implementation of DocumentOverlayCache.\r\n */\r\nclass MemoryDocumentOverlayCache {\r\n    constructor() {\r\n        // A map sorted by DocumentKey, whose value is a pair of the largest batch id\r\n        // for the overlay and the overlay itself.\r\n        this.overlays = new SortedMap(DocumentKey.comparator);\r\n        this.overlayByBatchId = new Map();\r\n    }\r\n    getOverlay(transaction, key) {\r\n        return PersistencePromise.resolve(this.overlays.get(key));\r\n    }\r\n    getOverlays(transaction, keys) {\r\n        const result = newOverlayMap();\r\n        return PersistencePromise.forEach(keys, (key) => {\r\n            return this.getOverlay(transaction, key).next(overlay => {\r\n                if (overlay !== null) {\r\n                    result.set(key, overlay);\r\n                }\r\n            });\r\n        }).next(() => result);\r\n    }\r\n    saveOverlays(transaction, largestBatchId, overlays) {\r\n        overlays.forEach((_, mutation) => {\r\n            this.saveOverlay(transaction, largestBatchId, mutation);\r\n        });\r\n        return PersistencePromise.resolve();\r\n    }\r\n    removeOverlaysForBatchId(transaction, documentKeys, batchId) {\r\n        const keys = this.overlayByBatchId.get(batchId);\r\n        if (keys !== undefined) {\r\n            keys.forEach(key => (this.overlays = this.overlays.remove(key)));\r\n            this.overlayByBatchId.delete(batchId);\r\n        }\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getOverlaysForCollection(transaction, collection, sinceBatchId) {\r\n        const result = newOverlayMap();\r\n        const immediateChildrenPathLength = collection.length + 1;\r\n        const prefix = new DocumentKey(collection.child(''));\r\n        const iter = this.overlays.getIteratorFrom(prefix);\r\n        while (iter.hasNext()) {\r\n            const entry = iter.getNext();\r\n            const overlay = entry.value;\r\n            const key = overlay.getKey();\r\n            if (!collection.isPrefixOf(key.path)) {\r\n                break;\r\n            }\r\n            // Documents from sub-collections\r\n            if (key.path.length !== immediateChildrenPathLength) {\r\n                continue;\r\n            }\r\n            if (overlay.largestBatchId > sinceBatchId) {\r\n                result.set(overlay.getKey(), overlay);\r\n            }\r\n        }\r\n        return PersistencePromise.resolve(result);\r\n    }\r\n    getOverlaysForCollectionGroup(transaction, collectionGroup, sinceBatchId, count) {\r\n        let batchIdToOverlays = new SortedMap((key1, key2) => key1 - key2);\r\n        const iter = this.overlays.getIterator();\r\n        while (iter.hasNext()) {\r\n            const entry = iter.getNext();\r\n            const overlay = entry.value;\r\n            const key = overlay.getKey();\r\n            if (key.getCollectionGroup() !== collectionGroup) {\r\n                continue;\r\n            }\r\n            if (overlay.largestBatchId > sinceBatchId) {\r\n                let overlaysForBatchId = batchIdToOverlays.get(overlay.largestBatchId);\r\n                if (overlaysForBatchId === null) {\r\n                    overlaysForBatchId = newOverlayMap();\r\n                    batchIdToOverlays = batchIdToOverlays.insert(overlay.largestBatchId, overlaysForBatchId);\r\n                }\r\n                overlaysForBatchId.set(overlay.getKey(), overlay);\r\n            }\r\n        }\r\n        const result = newOverlayMap();\r\n        const batchIter = batchIdToOverlays.getIterator();\r\n        while (batchIter.hasNext()) {\r\n            const entry = batchIter.getNext();\r\n            const overlays = entry.value;\r\n            overlays.forEach((key, overlay) => result.set(key, overlay));\r\n            if (result.size() >= count) {\r\n                break;\r\n            }\r\n        }\r\n        return PersistencePromise.resolve(result);\r\n    }\r\n    saveOverlay(transaction, largestBatchId, mutation) {\r\n        // Remove the association of the overlay to its batch id.\r\n        const existing = this.overlays.get(mutation.key);\r\n        if (existing !== null) {\r\n            const newSet = this.overlayByBatchId\r\n                .get(existing.largestBatchId)\r\n                .delete(mutation.key);\r\n            this.overlayByBatchId.set(existing.largestBatchId, newSet);\r\n        }\r\n        this.overlays = this.overlays.insert(mutation.key, new Overlay(largestBatchId, mutation));\r\n        // Create the association of this overlay to the given largestBatchId.\r\n        let batch = this.overlayByBatchId.get(largestBatchId);\r\n        if (batch === undefined) {\r\n            batch = documentKeySet();\r\n            this.overlayByBatchId.set(largestBatchId, batch);\r\n        }\r\n        this.overlayByBatchId.set(largestBatchId, batch.add(mutation.key));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A collection of references to a document from some kind of numbered entity\r\n * (either a target ID or batch ID). As references are added to or removed from\r\n * the set corresponding events are emitted to a registered garbage collector.\r\n *\r\n * Each reference is represented by a DocumentReference object. Each of them\r\n * contains enough information to uniquely identify the reference. They are all\r\n * stored primarily in a set sorted by key. A document is considered garbage if\r\n * there's no references in that set (this can be efficiently checked thanks to\r\n * sorting by key).\r\n *\r\n * ReferenceSet also keeps a secondary set that contains references sorted by\r\n * IDs. This one is used to efficiently implement removal of all references by\r\n * some target ID.\r\n */\r\nclass ReferenceSet {\r\n    constructor() {\r\n        // A set of outstanding references to a document sorted by key.\r\n        this.refsByKey = new SortedSet(DocReference.compareByKey);\r\n        // A set of outstanding references to a document sorted by target id.\r\n        this.refsByTarget = new SortedSet(DocReference.compareByTargetId);\r\n    }\r\n    /** Returns true if the reference set contains no references. */\r\n    isEmpty() {\r\n        return this.refsByKey.isEmpty();\r\n    }\r\n    /** Adds a reference to the given document key for the given ID. */\r\n    addReference(key, id) {\r\n        const ref = new DocReference(key, id);\r\n        this.refsByKey = this.refsByKey.add(ref);\r\n        this.refsByTarget = this.refsByTarget.add(ref);\r\n    }\r\n    /** Add references to the given document keys for the given ID. */\r\n    addReferences(keys, id) {\r\n        keys.forEach(key => this.addReference(key, id));\r\n    }\r\n    /**\r\n     * Removes a reference to the given document key for the given\r\n     * ID.\r\n     */\r\n    removeReference(key, id) {\r\n        this.removeRef(new DocReference(key, id));\r\n    }\r\n    removeReferences(keys, id) {\r\n        keys.forEach(key => this.removeReference(key, id));\r\n    }\r\n    /**\r\n     * Clears all references with a given ID. Calls removeRef() for each key\r\n     * removed.\r\n     */\r\n    removeReferencesForId(id) {\r\n        const emptyKey = new DocumentKey(new ResourcePath([]));\r\n        const startRef = new DocReference(emptyKey, id);\r\n        const endRef = new DocReference(emptyKey, id + 1);\r\n        const keys = [];\r\n        this.refsByTarget.forEachInRange([startRef, endRef], ref => {\r\n            this.removeRef(ref);\r\n            keys.push(ref.key);\r\n        });\r\n        return keys;\r\n    }\r\n    removeAllReferences() {\r\n        this.refsByKey.forEach(ref => this.removeRef(ref));\r\n    }\r\n    removeRef(ref) {\r\n        this.refsByKey = this.refsByKey.delete(ref);\r\n        this.refsByTarget = this.refsByTarget.delete(ref);\r\n    }\r\n    referencesForId(id) {\r\n        const emptyKey = new DocumentKey(new ResourcePath([]));\r\n        const startRef = new DocReference(emptyKey, id);\r\n        const endRef = new DocReference(emptyKey, id + 1);\r\n        let keys = documentKeySet();\r\n        this.refsByTarget.forEachInRange([startRef, endRef], ref => {\r\n            keys = keys.add(ref.key);\r\n        });\r\n        return keys;\r\n    }\r\n    containsKey(key) {\r\n        const ref = new DocReference(key, 0);\r\n        const firstRef = this.refsByKey.firstAfterOrEqual(ref);\r\n        return firstRef !== null && key.isEqual(firstRef.key);\r\n    }\r\n}\r\nclass DocReference {\r\n    constructor(key, targetOrBatchId) {\r\n        this.key = key;\r\n        this.targetOrBatchId = targetOrBatchId;\r\n    }\r\n    /** Compare by key then by ID */\r\n    static compareByKey(left, right) {\r\n        return (DocumentKey.comparator(left.key, right.key) ||\r\n            primitiveComparator(left.targetOrBatchId, right.targetOrBatchId));\r\n    }\r\n    /** Compare by ID then by key */\r\n    static compareByTargetId(left, right) {\r\n        return (primitiveComparator(left.targetOrBatchId, right.targetOrBatchId) ||\r\n            DocumentKey.comparator(left.key, right.key));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass MemoryMutationQueue {\r\n    constructor(indexManager, referenceDelegate) {\r\n        this.indexManager = indexManager;\r\n        this.referenceDelegate = referenceDelegate;\r\n        /**\r\n         * The set of all mutations that have been sent but not yet been applied to\r\n         * the backend.\r\n         */\r\n        this.mutationQueue = [];\r\n        /** Next value to use when assigning sequential IDs to each mutation batch. */\r\n        this.nextBatchId = 1;\r\n        /** An ordered mapping between documents and the mutations batch IDs. */\r\n        this.batchesByDocumentKey = new SortedSet(DocReference.compareByKey);\r\n    }\r\n    checkEmpty(transaction) {\r\n        return PersistencePromise.resolve(this.mutationQueue.length === 0);\r\n    }\r\n    addMutationBatch(transaction, localWriteTime, baseMutations, mutations) {\r\n        const batchId = this.nextBatchId;\r\n        this.nextBatchId++;\r\n        if (this.mutationQueue.length > 0) {\r\n            this.mutationQueue[this.mutationQueue.length - 1];\r\n        }\r\n        const batch = new MutationBatch(batchId, localWriteTime, baseMutations, mutations);\r\n        this.mutationQueue.push(batch);\r\n        // Track references by document key and index collection parents.\r\n        for (const mutation of mutations) {\r\n            this.batchesByDocumentKey = this.batchesByDocumentKey.add(new DocReference(mutation.key, batchId));\r\n            this.indexManager.addToCollectionParentIndex(transaction, mutation.key.path.popLast());\r\n        }\r\n        return PersistencePromise.resolve(batch);\r\n    }\r\n    lookupMutationBatch(transaction, batchId) {\r\n        return PersistencePromise.resolve(this.findMutationBatch(batchId));\r\n    }\r\n    getNextMutationBatchAfterBatchId(transaction, batchId) {\r\n        const nextBatchId = batchId + 1;\r\n        // The requested batchId may still be out of range so normalize it to the\r\n        // start of the queue.\r\n        const rawIndex = this.indexOfBatchId(nextBatchId);\r\n        const index = rawIndex < 0 ? 0 : rawIndex;\r\n        return PersistencePromise.resolve(this.mutationQueue.length > index ? this.mutationQueue[index] : null);\r\n    }\r\n    getHighestUnacknowledgedBatchId() {\r\n        return PersistencePromise.resolve(this.mutationQueue.length === 0 ? BATCHID_UNKNOWN : this.nextBatchId - 1);\r\n    }\r\n    getAllMutationBatches(transaction) {\r\n        return PersistencePromise.resolve(this.mutationQueue.slice());\r\n    }\r\n    getAllMutationBatchesAffectingDocumentKey(transaction, documentKey) {\r\n        const start = new DocReference(documentKey, 0);\r\n        const end = new DocReference(documentKey, Number.POSITIVE_INFINITY);\r\n        const result = [];\r\n        this.batchesByDocumentKey.forEachInRange([start, end], ref => {\r\n            const batch = this.findMutationBatch(ref.targetOrBatchId);\r\n            result.push(batch);\r\n        });\r\n        return PersistencePromise.resolve(result);\r\n    }\r\n    getAllMutationBatchesAffectingDocumentKeys(transaction, documentKeys) {\r\n        let uniqueBatchIDs = new SortedSet(primitiveComparator);\r\n        documentKeys.forEach(documentKey => {\r\n            const start = new DocReference(documentKey, 0);\r\n            const end = new DocReference(documentKey, Number.POSITIVE_INFINITY);\r\n            this.batchesByDocumentKey.forEachInRange([start, end], ref => {\r\n                uniqueBatchIDs = uniqueBatchIDs.add(ref.targetOrBatchId);\r\n            });\r\n        });\r\n        return PersistencePromise.resolve(this.findMutationBatches(uniqueBatchIDs));\r\n    }\r\n    getAllMutationBatchesAffectingQuery(transaction, query) {\r\n        // Use the query path as a prefix for testing if a document matches the\r\n        // query.\r\n        const prefix = query.path;\r\n        const immediateChildrenPathLength = prefix.length + 1;\r\n        // Construct a document reference for actually scanning the index. Unlike\r\n        // the prefix the document key in this reference must have an even number of\r\n        // segments. The empty segment can be used a suffix of the query path\r\n        // because it precedes all other segments in an ordered traversal.\r\n        let startPath = prefix;\r\n        if (!DocumentKey.isDocumentKey(startPath)) {\r\n            startPath = startPath.child('');\r\n        }\r\n        const start = new DocReference(new DocumentKey(startPath), 0);\r\n        // Find unique batchIDs referenced by all documents potentially matching the\r\n        // query.\r\n        let uniqueBatchIDs = new SortedSet(primitiveComparator);\r\n        this.batchesByDocumentKey.forEachWhile(ref => {\r\n            const rowKeyPath = ref.key.path;\r\n            if (!prefix.isPrefixOf(rowKeyPath)) {\r\n                return false;\r\n            }\r\n            else {\r\n                // Rows with document keys more than one segment longer than the query\r\n                // path can't be matches. For example, a query on 'rooms' can't match\r\n                // the document /rooms/abc/messages/xyx.\r\n                // TODO(mcg): we'll need a different scanner when we implement\r\n                // ancestor queries.\r\n                if (rowKeyPath.length === immediateChildrenPathLength) {\r\n                    uniqueBatchIDs = uniqueBatchIDs.add(ref.targetOrBatchId);\r\n                }\r\n                return true;\r\n            }\r\n        }, start);\r\n        return PersistencePromise.resolve(this.findMutationBatches(uniqueBatchIDs));\r\n    }\r\n    findMutationBatches(batchIDs) {\r\n        // Construct an array of matching batches, sorted by batchID to ensure that\r\n        // multiple mutations affecting the same document key are applied in order.\r\n        const result = [];\r\n        batchIDs.forEach(batchId => {\r\n            const batch = this.findMutationBatch(batchId);\r\n            if (batch !== null) {\r\n                result.push(batch);\r\n            }\r\n        });\r\n        return result;\r\n    }\r\n    removeMutationBatch(transaction, batch) {\r\n        // Find the position of the first batch for removal.\r\n        const batchIndex = this.indexOfExistingBatchId(batch.batchId, 'removed');\r\n        hardAssert(batchIndex === 0);\r\n        this.mutationQueue.shift();\r\n        let references = this.batchesByDocumentKey;\r\n        return PersistencePromise.forEach(batch.mutations, (mutation) => {\r\n            const ref = new DocReference(mutation.key, batch.batchId);\r\n            references = references.delete(ref);\r\n            return this.referenceDelegate.markPotentiallyOrphaned(transaction, mutation.key);\r\n        }).next(() => {\r\n            this.batchesByDocumentKey = references;\r\n        });\r\n    }\r\n    removeCachedMutationKeys(batchId) {\r\n        // No-op since the memory mutation queue does not maintain a separate cache.\r\n    }\r\n    containsKey(txn, key) {\r\n        const ref = new DocReference(key, 0);\r\n        const firstRef = this.batchesByDocumentKey.firstAfterOrEqual(ref);\r\n        return PersistencePromise.resolve(key.isEqual(firstRef && firstRef.key));\r\n    }\r\n    performConsistencyCheck(txn) {\r\n        if (this.mutationQueue.length === 0) ;\r\n        return PersistencePromise.resolve();\r\n    }\r\n    /**\r\n     * Finds the index of the given batchId in the mutation queue and asserts that\r\n     * the resulting index is within the bounds of the queue.\r\n     *\r\n     * @param batchId - The batchId to search for\r\n     * @param action - A description of what the caller is doing, phrased in passive\r\n     * form (e.g. \"acknowledged\" in a routine that acknowledges batches).\r\n     */\r\n    indexOfExistingBatchId(batchId, action) {\r\n        const index = this.indexOfBatchId(batchId);\r\n        return index;\r\n    }\r\n    /**\r\n     * Finds the index of the given batchId in the mutation queue. This operation\r\n     * is O(1).\r\n     *\r\n     * @returns The computed index of the batch with the given batchId, based on\r\n     * the state of the queue. Note this index can be negative if the requested\r\n     * batchId has already been remvoed from the queue or past the end of the\r\n     * queue if the batchId is larger than the last added batch.\r\n     */\r\n    indexOfBatchId(batchId) {\r\n        if (this.mutationQueue.length === 0) {\r\n            // As an index this is past the end of the queue\r\n            return 0;\r\n        }\r\n        // Examine the front of the queue to figure out the difference between the\r\n        // batchId and indexes in the array. Note that since the queue is ordered\r\n        // by batchId, if the first batch has a larger batchId then the requested\r\n        // batchId doesn't exist in the queue.\r\n        const firstBatchId = this.mutationQueue[0].batchId;\r\n        return batchId - firstBatchId;\r\n    }\r\n    /**\r\n     * A version of lookupMutationBatch that doesn't return a promise, this makes\r\n     * other functions that uses this code easier to read and more efficent.\r\n     */\r\n    findMutationBatch(batchId) {\r\n        const index = this.indexOfBatchId(batchId);\r\n        if (index < 0 || index >= this.mutationQueue.length) {\r\n            return null;\r\n        }\r\n        const batch = this.mutationQueue[index];\r\n        return batch;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nfunction documentEntryMap() {\r\n    return new SortedMap(DocumentKey.comparator);\r\n}\r\n/**\r\n * The memory-only RemoteDocumentCache for IndexedDb. To construct, invoke\r\n * `newMemoryRemoteDocumentCache()`.\r\n */\r\nclass MemoryRemoteDocumentCacheImpl {\r\n    /**\r\n     * @param sizer - Used to assess the size of a document. For eager GC, this is\r\n     * expected to just return 0 to avoid unnecessarily doing the work of\r\n     * calculating the size.\r\n     */\r\n    constructor(sizer) {\r\n        this.sizer = sizer;\r\n        /** Underlying cache of documents and their read times. */\r\n        this.docs = documentEntryMap();\r\n        /** Size of all cached documents. */\r\n        this.size = 0;\r\n    }\r\n    setIndexManager(indexManager) {\r\n        this.indexManager = indexManager;\r\n    }\r\n    /**\r\n     * Adds the supplied entry to the cache and updates the cache size as appropriate.\r\n     *\r\n     * All calls of `addEntry`  are required to go through the RemoteDocumentChangeBuffer\r\n     * returned by `newChangeBuffer()`.\r\n     */\r\n    addEntry(transaction, doc) {\r\n        const key = doc.key;\r\n        const entry = this.docs.get(key);\r\n        const previousSize = entry ? entry.size : 0;\r\n        const currentSize = this.sizer(doc);\r\n        this.docs = this.docs.insert(key, {\r\n            document: doc.mutableCopy(),\r\n            size: currentSize\r\n        });\r\n        this.size += currentSize - previousSize;\r\n        return this.indexManager.addToCollectionParentIndex(transaction, key.path.popLast());\r\n    }\r\n    /**\r\n     * Removes the specified entry from the cache and updates the cache size as appropriate.\r\n     *\r\n     * All calls of `removeEntry` are required to go through the RemoteDocumentChangeBuffer\r\n     * returned by `newChangeBuffer()`.\r\n     */\r\n    removeEntry(documentKey) {\r\n        const entry = this.docs.get(documentKey);\r\n        if (entry) {\r\n            this.docs = this.docs.remove(documentKey);\r\n            this.size -= entry.size;\r\n        }\r\n    }\r\n    getEntry(transaction, documentKey) {\r\n        const entry = this.docs.get(documentKey);\r\n        return PersistencePromise.resolve(entry\r\n            ? entry.document.mutableCopy()\r\n            : MutableDocument.newInvalidDocument(documentKey));\r\n    }\r\n    getEntries(transaction, documentKeys) {\r\n        let results = mutableDocumentMap();\r\n        documentKeys.forEach(documentKey => {\r\n            const entry = this.docs.get(documentKey);\r\n            results = results.insert(documentKey, entry\r\n                ? entry.document.mutableCopy()\r\n                : MutableDocument.newInvalidDocument(documentKey));\r\n        });\r\n        return PersistencePromise.resolve(results);\r\n    }\r\n    getAllFromCollection(transaction, collectionPath, offset) {\r\n        let results = mutableDocumentMap();\r\n        // Documents are ordered by key, so we can use a prefix scan to narrow down\r\n        // the documents we need to match the query against.\r\n        const prefix = new DocumentKey(collectionPath.child(''));\r\n        const iterator = this.docs.getIteratorFrom(prefix);\r\n        while (iterator.hasNext()) {\r\n            const { key, value: { document } } = iterator.getNext();\r\n            if (!collectionPath.isPrefixOf(key.path)) {\r\n                break;\r\n            }\r\n            if (key.path.length > collectionPath.length + 1) {\r\n                // Exclude entries from subcollections.\r\n                continue;\r\n            }\r\n            if (indexOffsetComparator(newIndexOffsetFromDocument(document), offset) <= 0) {\r\n                // The document sorts before the offset.\r\n                continue;\r\n            }\r\n            results = results.insert(document.key, document.mutableCopy());\r\n        }\r\n        return PersistencePromise.resolve(results);\r\n    }\r\n    getAllFromCollectionGroup(transaction, collectionGroup, offset, limti) {\r\n        // This method should only be called from the IndexBackfiller if persistence\r\n        // is enabled.\r\n        fail();\r\n    }\r\n    forEachDocumentKey(transaction, f) {\r\n        return PersistencePromise.forEach(this.docs, (key) => f(key));\r\n    }\r\n    newChangeBuffer(options) {\r\n        // `trackRemovals` is ignores since the MemoryRemoteDocumentCache keeps\r\n        // a separate changelog and does not need special handling for removals.\r\n        return new MemoryRemoteDocumentChangeBuffer(this);\r\n    }\r\n    getSize(txn) {\r\n        return PersistencePromise.resolve(this.size);\r\n    }\r\n}\r\n/**\r\n * Creates a new memory-only RemoteDocumentCache.\r\n *\r\n * @param sizer - Used to assess the size of a document. For eager GC, this is\r\n * expected to just return 0 to avoid unnecessarily doing the work of\r\n * calculating the size.\r\n */\r\nfunction newMemoryRemoteDocumentCache(sizer) {\r\n    return new MemoryRemoteDocumentCacheImpl(sizer);\r\n}\r\n/**\r\n * Handles the details of adding and updating documents in the MemoryRemoteDocumentCache.\r\n */\r\nclass MemoryRemoteDocumentChangeBuffer extends RemoteDocumentChangeBuffer {\r\n    constructor(documentCache) {\r\n        super();\r\n        this.documentCache = documentCache;\r\n    }\r\n    applyChanges(transaction) {\r\n        const promises = [];\r\n        this.changes.forEach((key, doc) => {\r\n            if (doc.isValidDocument()) {\r\n                promises.push(this.documentCache.addEntry(transaction, doc));\r\n            }\r\n            else {\r\n                this.documentCache.removeEntry(key);\r\n            }\r\n        });\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    getFromCache(transaction, documentKey) {\r\n        return this.documentCache.getEntry(transaction, documentKey);\r\n    }\r\n    getAllFromCache(transaction, documentKeys) {\r\n        return this.documentCache.getEntries(transaction, documentKeys);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass MemoryTargetCache {\r\n    constructor(persistence) {\r\n        this.persistence = persistence;\r\n        /**\r\n         * Maps a target to the data about that target\r\n         */\r\n        this.targets = new ObjectMap(t => canonifyTarget(t), targetEquals);\r\n        /** The last received snapshot version. */\r\n        this.lastRemoteSnapshotVersion = SnapshotVersion.min();\r\n        /** The highest numbered target ID encountered. */\r\n        this.highestTargetId = 0;\r\n        /** The highest sequence number encountered. */\r\n        this.highestSequenceNumber = 0;\r\n        /**\r\n         * A ordered bidirectional mapping between documents and the remote target\r\n         * IDs.\r\n         */\r\n        this.references = new ReferenceSet();\r\n        this.targetCount = 0;\r\n        this.targetIdGenerator = TargetIdGenerator.forTargetCache();\r\n    }\r\n    forEachTarget(txn, f) {\r\n        this.targets.forEach((_, targetData) => f(targetData));\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getLastRemoteSnapshotVersion(transaction) {\r\n        return PersistencePromise.resolve(this.lastRemoteSnapshotVersion);\r\n    }\r\n    getHighestSequenceNumber(transaction) {\r\n        return PersistencePromise.resolve(this.highestSequenceNumber);\r\n    }\r\n    allocateTargetId(transaction) {\r\n        this.highestTargetId = this.targetIdGenerator.next();\r\n        return PersistencePromise.resolve(this.highestTargetId);\r\n    }\r\n    setTargetsMetadata(transaction, highestListenSequenceNumber, lastRemoteSnapshotVersion) {\r\n        if (lastRemoteSnapshotVersion) {\r\n            this.lastRemoteSnapshotVersion = lastRemoteSnapshotVersion;\r\n        }\r\n        if (highestListenSequenceNumber > this.highestSequenceNumber) {\r\n            this.highestSequenceNumber = highestListenSequenceNumber;\r\n        }\r\n        return PersistencePromise.resolve();\r\n    }\r\n    saveTargetData(targetData) {\r\n        this.targets.set(targetData.target, targetData);\r\n        const targetId = targetData.targetId;\r\n        if (targetId > this.highestTargetId) {\r\n            this.targetIdGenerator = new TargetIdGenerator(targetId);\r\n            this.highestTargetId = targetId;\r\n        }\r\n        if (targetData.sequenceNumber > this.highestSequenceNumber) {\r\n            this.highestSequenceNumber = targetData.sequenceNumber;\r\n        }\r\n    }\r\n    addTargetData(transaction, targetData) {\r\n        this.saveTargetData(targetData);\r\n        this.targetCount += 1;\r\n        return PersistencePromise.resolve();\r\n    }\r\n    updateTargetData(transaction, targetData) {\r\n        this.saveTargetData(targetData);\r\n        return PersistencePromise.resolve();\r\n    }\r\n    removeTargetData(transaction, targetData) {\r\n        this.targets.delete(targetData.target);\r\n        this.references.removeReferencesForId(targetData.targetId);\r\n        this.targetCount -= 1;\r\n        return PersistencePromise.resolve();\r\n    }\r\n    removeTargets(transaction, upperBound, activeTargetIds) {\r\n        let count = 0;\r\n        const removals = [];\r\n        this.targets.forEach((key, targetData) => {\r\n            if (targetData.sequenceNumber <= upperBound &&\r\n                activeTargetIds.get(targetData.targetId) === null) {\r\n                this.targets.delete(key);\r\n                removals.push(this.removeMatchingKeysForTargetId(transaction, targetData.targetId));\r\n                count++;\r\n            }\r\n        });\r\n        return PersistencePromise.waitFor(removals).next(() => count);\r\n    }\r\n    getTargetCount(transaction) {\r\n        return PersistencePromise.resolve(this.targetCount);\r\n    }\r\n    getTargetData(transaction, target) {\r\n        const targetData = this.targets.get(target) || null;\r\n        return PersistencePromise.resolve(targetData);\r\n    }\r\n    addMatchingKeys(txn, keys, targetId) {\r\n        this.references.addReferences(keys, targetId);\r\n        return PersistencePromise.resolve();\r\n    }\r\n    removeMatchingKeys(txn, keys, targetId) {\r\n        this.references.removeReferences(keys, targetId);\r\n        const referenceDelegate = this.persistence.referenceDelegate;\r\n        const promises = [];\r\n        if (referenceDelegate) {\r\n            keys.forEach(key => {\r\n                promises.push(referenceDelegate.markPotentiallyOrphaned(txn, key));\r\n            });\r\n        }\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    removeMatchingKeysForTargetId(txn, targetId) {\r\n        this.references.removeReferencesForId(targetId);\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getMatchingKeysForTargetId(txn, targetId) {\r\n        const matchingKeys = this.references.referencesForId(targetId);\r\n        return PersistencePromise.resolve(matchingKeys);\r\n    }\r\n    containsKey(txn, key) {\r\n        return PersistencePromise.resolve(this.references.containsKey(key));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst LOG_TAG$d = 'MemoryPersistence';\r\n/**\r\n * A memory-backed instance of Persistence. Data is stored only in RAM and\r\n * not persisted across sessions.\r\n */\r\nclass MemoryPersistence {\r\n    /**\r\n     * The constructor accepts a factory for creating a reference delegate. This\r\n     * allows both the delegate and this instance to have strong references to\r\n     * each other without having nullable fields that would then need to be\r\n     * checked or asserted on every access.\r\n     */\r\n    constructor(referenceDelegateFactory, serializer) {\r\n        this.mutationQueues = {};\r\n        this.overlays = {};\r\n        this.listenSequence = new ListenSequence(0);\r\n        this._started = false;\r\n        this._started = true;\r\n        this.referenceDelegate = referenceDelegateFactory(this);\r\n        this.targetCache = new MemoryTargetCache(this);\r\n        const sizer = (doc) => this.referenceDelegate.documentSize(doc);\r\n        this.indexManager = new MemoryIndexManager();\r\n        this.remoteDocumentCache = newMemoryRemoteDocumentCache(sizer);\r\n        this.serializer = new LocalSerializer(serializer);\r\n        this.bundleCache = new MemoryBundleCache(this.serializer);\r\n    }\r\n    start() {\r\n        return Promise.resolve();\r\n    }\r\n    shutdown() {\r\n        // No durable state to ensure is closed on shutdown.\r\n        this._started = false;\r\n        return Promise.resolve();\r\n    }\r\n    get started() {\r\n        return this._started;\r\n    }\r\n    setDatabaseDeletedListener() {\r\n        // No op.\r\n    }\r\n    setNetworkEnabled() {\r\n        // No op.\r\n    }\r\n    getIndexManager(user) {\r\n        // We do not currently support indices for memory persistence, so we can\r\n        // return the same shared instance of the memory index manager.\r\n        return this.indexManager;\r\n    }\r\n    getDocumentOverlayCache(user) {\r\n        let overlay = this.overlays[user.toKey()];\r\n        if (!overlay) {\r\n            overlay = new MemoryDocumentOverlayCache();\r\n            this.overlays[user.toKey()] = overlay;\r\n        }\r\n        return overlay;\r\n    }\r\n    getMutationQueue(user, indexManager) {\r\n        let queue = this.mutationQueues[user.toKey()];\r\n        if (!queue) {\r\n            queue = new MemoryMutationQueue(indexManager, this.referenceDelegate);\r\n            this.mutationQueues[user.toKey()] = queue;\r\n        }\r\n        return queue;\r\n    }\r\n    getTargetCache() {\r\n        return this.targetCache;\r\n    }\r\n    getRemoteDocumentCache() {\r\n        return this.remoteDocumentCache;\r\n    }\r\n    getBundleCache() {\r\n        return this.bundleCache;\r\n    }\r\n    runTransaction(action, mode, transactionOperation) {\r\n        logDebug(LOG_TAG$d, 'Starting transaction:', action);\r\n        const txn = new MemoryTransaction(this.listenSequence.next());\r\n        this.referenceDelegate.onTransactionStarted();\r\n        return transactionOperation(txn)\r\n            .next(result => {\r\n            return this.referenceDelegate\r\n                .onTransactionCommitted(txn)\r\n                .next(() => result);\r\n        })\r\n            .toPromise()\r\n            .then(result => {\r\n            txn.raiseOnCommittedEvent();\r\n            return result;\r\n        });\r\n    }\r\n    mutationQueuesContainKey(transaction, key) {\r\n        return PersistencePromise.or(Object.values(this.mutationQueues).map(queue => () => queue.containsKey(transaction, key)));\r\n    }\r\n}\r\n/**\r\n * Memory persistence is not actually transactional, but future implementations\r\n * may have transaction-scoped state.\r\n */\r\nclass MemoryTransaction extends PersistenceTransaction {\r\n    constructor(currentSequenceNumber) {\r\n        super();\r\n        this.currentSequenceNumber = currentSequenceNumber;\r\n    }\r\n}\r\nclass MemoryEagerDelegate {\r\n    constructor(persistence) {\r\n        this.persistence = persistence;\r\n        /** Tracks all documents that are active in Query views. */\r\n        this.localViewReferences = new ReferenceSet();\r\n        /** The list of documents that are potentially GCed after each transaction. */\r\n        this._orphanedDocuments = null;\r\n    }\r\n    static factory(persistence) {\r\n        return new MemoryEagerDelegate(persistence);\r\n    }\r\n    get orphanedDocuments() {\r\n        if (!this._orphanedDocuments) {\r\n            throw fail();\r\n        }\r\n        else {\r\n            return this._orphanedDocuments;\r\n        }\r\n    }\r\n    addReference(txn, targetId, key) {\r\n        this.localViewReferences.addReference(key, targetId);\r\n        this.orphanedDocuments.delete(key.toString());\r\n        return PersistencePromise.resolve();\r\n    }\r\n    removeReference(txn, targetId, key) {\r\n        this.localViewReferences.removeReference(key, targetId);\r\n        this.orphanedDocuments.add(key.toString());\r\n        return PersistencePromise.resolve();\r\n    }\r\n    markPotentiallyOrphaned(txn, key) {\r\n        this.orphanedDocuments.add(key.toString());\r\n        return PersistencePromise.resolve();\r\n    }\r\n    removeTarget(txn, targetData) {\r\n        const orphaned = this.localViewReferences.removeReferencesForId(targetData.targetId);\r\n        orphaned.forEach(key => this.orphanedDocuments.add(key.toString()));\r\n        const cache = this.persistence.getTargetCache();\r\n        return cache\r\n            .getMatchingKeysForTargetId(txn, targetData.targetId)\r\n            .next(keys => {\r\n            keys.forEach(key => this.orphanedDocuments.add(key.toString()));\r\n        })\r\n            .next(() => cache.removeTargetData(txn, targetData));\r\n    }\r\n    onTransactionStarted() {\r\n        this._orphanedDocuments = new Set();\r\n    }\r\n    onTransactionCommitted(txn) {\r\n        // Remove newly orphaned documents.\r\n        const cache = this.persistence.getRemoteDocumentCache();\r\n        const changeBuffer = cache.newChangeBuffer();\r\n        return PersistencePromise.forEach(this.orphanedDocuments, (path) => {\r\n            const key = DocumentKey.fromPath(path);\r\n            return this.isReferenced(txn, key).next(isReferenced => {\r\n                if (!isReferenced) {\r\n                    changeBuffer.removeEntry(key, SnapshotVersion.min());\r\n                }\r\n            });\r\n        }).next(() => {\r\n            this._orphanedDocuments = null;\r\n            return changeBuffer.apply(txn);\r\n        });\r\n    }\r\n    updateLimboDocument(txn, key) {\r\n        return this.isReferenced(txn, key).next(isReferenced => {\r\n            if (isReferenced) {\r\n                this.orphanedDocuments.delete(key.toString());\r\n            }\r\n            else {\r\n                this.orphanedDocuments.add(key.toString());\r\n            }\r\n        });\r\n    }\r\n    documentSize(doc) {\r\n        // For eager GC, we don't care about the document size, there are no size thresholds.\r\n        return 0;\r\n    }\r\n    isReferenced(txn, key) {\r\n        return PersistencePromise.or([\r\n            () => PersistencePromise.resolve(this.localViewReferences.containsKey(key)),\r\n            () => this.persistence.getTargetCache().containsKey(txn, key),\r\n            () => this.persistence.mutationQueuesContainKey(txn, key)\r\n        ]);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Performs database creation and schema upgrades. */\r\nclass SchemaConverter {\r\n    constructor(serializer) {\r\n        this.serializer = serializer;\r\n    }\r\n    /**\r\n     * Performs database creation and schema upgrades.\r\n     *\r\n     * Note that in production, this method is only ever used to upgrade the schema\r\n     * to SCHEMA_VERSION. Different values of toVersion are only used for testing\r\n     * and local feature development.\r\n     */\r\n    createOrUpgrade(db, txn, fromVersion, toVersion) {\r\n        const simpleDbTransaction = new SimpleDbTransaction('createOrUpgrade', txn);\r\n        if (fromVersion < 1 && toVersion >= 1) {\r\n            createPrimaryClientStore(db);\r\n            createMutationQueue(db);\r\n            createQueryCache(db);\r\n            createLegacyRemoteDocumentCache(db);\r\n        }\r\n        // Migration 2 to populate the targetGlobal object no longer needed since\r\n        // migration 3 unconditionally clears it.\r\n        let p = PersistencePromise.resolve();\r\n        if (fromVersion < 3 && toVersion >= 3) {\r\n            // Brand new clients don't need to drop and recreate--only clients that\r\n            // potentially have corrupt data.\r\n            if (fromVersion !== 0) {\r\n                dropQueryCache(db);\r\n                createQueryCache(db);\r\n            }\r\n            p = p.next(() => writeEmptyTargetGlobalEntry(simpleDbTransaction));\r\n        }\r\n        if (fromVersion < 4 && toVersion >= 4) {\r\n            if (fromVersion !== 0) {\r\n                // Schema version 3 uses auto-generated keys to generate globally unique\r\n                // mutation batch IDs (this was previously ensured internally by the\r\n                // client). To migrate to the new schema, we have to read all mutations\r\n                // and write them back out. We preserve the existing batch IDs to guarantee\r\n                // consistency with other object stores. Any further mutation batch IDs will\r\n                // be auto-generated.\r\n                p = p.next(() => upgradeMutationBatchSchemaAndMigrateData(db, simpleDbTransaction));\r\n            }\r\n            p = p.next(() => {\r\n                createClientMetadataStore(db);\r\n            });\r\n        }\r\n        if (fromVersion < 5 && toVersion >= 5) {\r\n            p = p.next(() => this.removeAcknowledgedMutations(simpleDbTransaction));\r\n        }\r\n        if (fromVersion < 6 && toVersion >= 6) {\r\n            p = p.next(() => {\r\n                createDocumentGlobalStore(db);\r\n                return this.addDocumentGlobal(simpleDbTransaction);\r\n            });\r\n        }\r\n        if (fromVersion < 7 && toVersion >= 7) {\r\n            p = p.next(() => this.ensureSequenceNumbers(simpleDbTransaction));\r\n        }\r\n        if (fromVersion < 8 && toVersion >= 8) {\r\n            p = p.next(() => this.createCollectionParentIndex(db, simpleDbTransaction));\r\n        }\r\n        if (fromVersion < 9 && toVersion >= 9) {\r\n            p = p.next(() => {\r\n                // Multi-Tab used to manage its own changelog, but this has been moved\r\n                // to the DbRemoteDocument object store itself. Since the previous change\r\n                // log only contained transient data, we can drop its object store.\r\n                dropRemoteDocumentChangesStore(db);\r\n                // Note: Schema version 9 used to create a read time index for the\r\n                // RemoteDocumentCache. This is now done with schema version 13.\r\n            });\r\n        }\r\n        if (fromVersion < 10 && toVersion >= 10) {\r\n            p = p.next(() => this.rewriteCanonicalIds(simpleDbTransaction));\r\n        }\r\n        if (fromVersion < 11 && toVersion >= 11) {\r\n            p = p.next(() => {\r\n                createBundlesStore(db);\r\n                createNamedQueriesStore(db);\r\n            });\r\n        }\r\n        if (fromVersion < 12 && toVersion >= 12) {\r\n            p = p.next(() => {\r\n                createDocumentOverlayStore(db);\r\n            });\r\n        }\r\n        if (fromVersion < 13 && toVersion >= 13) {\r\n            p = p\r\n                .next(() => createRemoteDocumentCache(db))\r\n                .next(() => this.rewriteRemoteDocumentCache(db, simpleDbTransaction))\r\n                .next(() => db.deleteObjectStore(DbRemoteDocumentStore$1));\r\n        }\r\n        if (fromVersion < 14 && toVersion >= 14) {\r\n            p = p.next(() => this.runOverlayMigration(db, simpleDbTransaction));\r\n        }\r\n        if (fromVersion < 15 && toVersion >= 15) {\r\n            p = p.next(() => createFieldIndex(db));\r\n        }\r\n        return p;\r\n    }\r\n    addDocumentGlobal(txn) {\r\n        let byteSize = 0;\r\n        return txn\r\n            .store(DbRemoteDocumentStore$1)\r\n            .iterate((_, doc) => {\r\n            byteSize += dbDocumentSize(doc);\r\n        })\r\n            .next(() => {\r\n            const metadata = { byteSize };\r\n            return txn\r\n                .store(DbRemoteDocumentGlobalStore)\r\n                .put(DbRemoteDocumentGlobalKey, metadata);\r\n        });\r\n    }\r\n    removeAcknowledgedMutations(txn) {\r\n        const queuesStore = txn.store(DbMutationQueueStore);\r\n        const mutationsStore = txn.store(DbMutationBatchStore);\r\n        return queuesStore.loadAll().next(queues => {\r\n            return PersistencePromise.forEach(queues, (queue) => {\r\n                const range = IDBKeyRange.bound([queue.userId, BATCHID_UNKNOWN], [queue.userId, queue.lastAcknowledgedBatchId]);\r\n                return mutationsStore\r\n                    .loadAll(DbMutationBatchUserMutationsIndex, range)\r\n                    .next(dbBatches => {\r\n                    return PersistencePromise.forEach(dbBatches, (dbBatch) => {\r\n                        hardAssert(dbBatch.userId === queue.userId);\r\n                        const batch = fromDbMutationBatch(this.serializer, dbBatch);\r\n                        return removeMutationBatch(txn, queue.userId, batch).next(() => { });\r\n                    });\r\n                });\r\n            });\r\n        });\r\n    }\r\n    /**\r\n     * Ensures that every document in the remote document cache has a corresponding sentinel row\r\n     * with a sequence number. Missing rows are given the most recently used sequence number.\r\n     */\r\n    ensureSequenceNumbers(txn) {\r\n        const documentTargetStore = txn.store(DbTargetDocumentStore);\r\n        const documentsStore = txn.store(DbRemoteDocumentStore$1);\r\n        const globalTargetStore = txn.store(DbTargetGlobalStore);\r\n        return globalTargetStore.get(DbTargetGlobalKey).next(metadata => {\r\n            const writeSentinelKey = (path) => {\r\n                return documentTargetStore.put({\r\n                    targetId: 0,\r\n                    path: encodeResourcePath(path),\r\n                    sequenceNumber: metadata.highestListenSequenceNumber\r\n                });\r\n            };\r\n            const promises = [];\r\n            return documentsStore\r\n                .iterate((key, doc) => {\r\n                const path = new ResourcePath(key);\r\n                const docSentinelKey = sentinelKey(path);\r\n                promises.push(documentTargetStore.get(docSentinelKey).next(maybeSentinel => {\r\n                    if (!maybeSentinel) {\r\n                        return writeSentinelKey(path);\r\n                    }\r\n                    else {\r\n                        return PersistencePromise.resolve();\r\n                    }\r\n                }));\r\n            })\r\n                .next(() => PersistencePromise.waitFor(promises));\r\n        });\r\n    }\r\n    createCollectionParentIndex(db, txn) {\r\n        // Create the index.\r\n        db.createObjectStore(DbCollectionParentStore, {\r\n            keyPath: DbCollectionParentKeyPath\r\n        });\r\n        const collectionParentsStore = txn.store(DbCollectionParentStore);\r\n        // Helper to add an index entry iff we haven't already written it.\r\n        const cache = new MemoryCollectionParentIndex();\r\n        const addEntry = (collectionPath) => {\r\n            if (cache.add(collectionPath)) {\r\n                const collectionId = collectionPath.lastSegment();\r\n                const parentPath = collectionPath.popLast();\r\n                return collectionParentsStore.put({\r\n                    collectionId,\r\n                    parent: encodeResourcePath(parentPath)\r\n                });\r\n            }\r\n        };\r\n        // Index existing remote documents.\r\n        return txn\r\n            .store(DbRemoteDocumentStore$1)\r\n            .iterate({ keysOnly: true }, (pathSegments, _) => {\r\n            const path = new ResourcePath(pathSegments);\r\n            return addEntry(path.popLast());\r\n        })\r\n            .next(() => {\r\n            // Index existing mutations.\r\n            return txn\r\n                .store(DbDocumentMutationStore)\r\n                .iterate({ keysOnly: true }, ([userID, encodedPath, batchId], _) => {\r\n                const path = decodeResourcePath(encodedPath);\r\n                return addEntry(path.popLast());\r\n            });\r\n        });\r\n    }\r\n    rewriteCanonicalIds(txn) {\r\n        const targetStore = txn.store(DbTargetStore);\r\n        return targetStore.iterate((key, originalDbTarget) => {\r\n            const originalTargetData = fromDbTarget(originalDbTarget);\r\n            const updatedDbTarget = toDbTarget(this.serializer, originalTargetData);\r\n            return targetStore.put(updatedDbTarget);\r\n        });\r\n    }\r\n    rewriteRemoteDocumentCache(db, transaction) {\r\n        const legacyRemoteDocumentStore = transaction.store(DbRemoteDocumentStore$1);\r\n        const writes = [];\r\n        return legacyRemoteDocumentStore\r\n            .iterate((_, legacyDocument) => {\r\n            const remoteDocumentStore = transaction.store(DbRemoteDocumentStore);\r\n            const path = extractKey(legacyDocument).path.toArray();\r\n            const dbRemoteDocument = {\r\n                prefixPath: path.slice(0, path.length - 2),\r\n                collectionGroup: path[path.length - 2],\r\n                documentId: path[path.length - 1],\r\n                readTime: legacyDocument.readTime || [0, 0],\r\n                unknownDocument: legacyDocument.unknownDocument,\r\n                noDocument: legacyDocument.noDocument,\r\n                document: legacyDocument.document,\r\n                hasCommittedMutations: !!legacyDocument.hasCommittedMutations\r\n            };\r\n            writes.push(remoteDocumentStore.put(dbRemoteDocument));\r\n        })\r\n            .next(() => PersistencePromise.waitFor(writes));\r\n    }\r\n    runOverlayMigration(db, transaction) {\r\n        const mutationsStore = transaction.store(DbMutationBatchStore);\r\n        const remoteDocumentCache = newIndexedDbRemoteDocumentCache(this.serializer);\r\n        const memoryPersistence = new MemoryPersistence(MemoryEagerDelegate.factory, this.serializer.remoteSerializer);\r\n        return mutationsStore.loadAll().next(dbBatches => {\r\n            const userToDocumentSet = new Map();\r\n            dbBatches.forEach(dbBatch => {\r\n                var _a;\r\n                let documentSet = (_a = userToDocumentSet.get(dbBatch.userId)) !== null && _a !== void 0 ? _a : documentKeySet();\r\n                const batch = fromDbMutationBatch(this.serializer, dbBatch);\r\n                batch.keys().forEach(key => (documentSet = documentSet.add(key)));\r\n                userToDocumentSet.set(dbBatch.userId, documentSet);\r\n            });\r\n            return PersistencePromise.forEach(userToDocumentSet, (allDocumentKeysForUser, userId) => {\r\n                const user = new User(userId);\r\n                const documentOverlayCache = IndexedDbDocumentOverlayCache.forUser(this.serializer, user);\r\n                // NOTE: The index manager and the reference delegate are\r\n                // irrelevant for the purpose of recalculating and saving\r\n                // overlays. We can therefore simply use the memory\r\n                // implementation.\r\n                const indexManager = memoryPersistence.getIndexManager(user);\r\n                const mutationQueue = IndexedDbMutationQueue.forUser(user, this.serializer, indexManager, memoryPersistence.referenceDelegate);\r\n                const localDocumentsView = new LocalDocumentsView(remoteDocumentCache, mutationQueue, documentOverlayCache, indexManager);\r\n                return localDocumentsView\r\n                    .recalculateAndSaveOverlaysForDocumentKeys(new IndexedDbTransaction(transaction, ListenSequence.INVALID), allDocumentKeysForUser)\r\n                    .next();\r\n            });\r\n        });\r\n    }\r\n}\r\nfunction sentinelKey(path) {\r\n    return [0, encodeResourcePath(path)];\r\n}\r\nfunction createPrimaryClientStore(db) {\r\n    db.createObjectStore(DbPrimaryClientStore);\r\n}\r\nfunction createMutationQueue(db) {\r\n    db.createObjectStore(DbMutationQueueStore, {\r\n        keyPath: DbMutationQueueKeyPath\r\n    });\r\n    const mutationBatchesStore = db.createObjectStore(DbMutationBatchStore, {\r\n        keyPath: DbMutationBatchKeyPath,\r\n        autoIncrement: true\r\n    });\r\n    mutationBatchesStore.createIndex(DbMutationBatchUserMutationsIndex, DbMutationBatchUserMutationsKeyPath, { unique: true });\r\n    db.createObjectStore(DbDocumentMutationStore);\r\n}\r\n/**\r\n * Upgrade function to migrate the 'mutations' store from V1 to V3. Loads\r\n * and rewrites all data.\r\n */\r\nfunction upgradeMutationBatchSchemaAndMigrateData(db, txn) {\r\n    const v1MutationsStore = txn.store(DbMutationBatchStore);\r\n    return v1MutationsStore.loadAll().next(existingMutations => {\r\n        db.deleteObjectStore(DbMutationBatchStore);\r\n        const mutationsStore = db.createObjectStore(DbMutationBatchStore, {\r\n            keyPath: DbMutationBatchKeyPath,\r\n            autoIncrement: true\r\n        });\r\n        mutationsStore.createIndex(DbMutationBatchUserMutationsIndex, DbMutationBatchUserMutationsKeyPath, { unique: true });\r\n        const v3MutationsStore = txn.store(DbMutationBatchStore);\r\n        const writeAll = existingMutations.map(mutation => v3MutationsStore.put(mutation));\r\n        return PersistencePromise.waitFor(writeAll);\r\n    });\r\n}\r\nfunction createLegacyRemoteDocumentCache(db) {\r\n    db.createObjectStore(DbRemoteDocumentStore$1);\r\n}\r\nfunction createRemoteDocumentCache(db) {\r\n    const remoteDocumentStore = db.createObjectStore(DbRemoteDocumentStore, {\r\n        keyPath: DbRemoteDocumentKeyPath\r\n    });\r\n    remoteDocumentStore.createIndex(DbRemoteDocumentDocumentKeyIndex, DbRemoteDocumentDocumentKeyIndexPath);\r\n    remoteDocumentStore.createIndex(DbRemoteDocumentCollectionGroupIndex, DbRemoteDocumentCollectionGroupIndexPath);\r\n}\r\nfunction createDocumentGlobalStore(db) {\r\n    db.createObjectStore(DbRemoteDocumentGlobalStore);\r\n}\r\nfunction createQueryCache(db) {\r\n    const targetDocumentsStore = db.createObjectStore(DbTargetDocumentStore, {\r\n        keyPath: DbTargetDocumentKeyPath\r\n    });\r\n    targetDocumentsStore.createIndex(DbTargetDocumentDocumentTargetsIndex, DbTargetDocumentDocumentTargetsKeyPath, { unique: true });\r\n    const targetStore = db.createObjectStore(DbTargetStore, {\r\n        keyPath: DbTargetKeyPath\r\n    });\r\n    // NOTE: This is unique only because the TargetId is the suffix.\r\n    targetStore.createIndex(DbTargetQueryTargetsIndexName, DbTargetQueryTargetsKeyPath, { unique: true });\r\n    db.createObjectStore(DbTargetGlobalStore);\r\n}\r\nfunction dropQueryCache(db) {\r\n    db.deleteObjectStore(DbTargetDocumentStore);\r\n    db.deleteObjectStore(DbTargetStore);\r\n    db.deleteObjectStore(DbTargetGlobalStore);\r\n}\r\nfunction dropRemoteDocumentChangesStore(db) {\r\n    if (db.objectStoreNames.contains('remoteDocumentChanges')) {\r\n        db.deleteObjectStore('remoteDocumentChanges');\r\n    }\r\n}\r\n/**\r\n * Creates the target global singleton row.\r\n *\r\n * @param txn - The version upgrade transaction for indexeddb\r\n */\r\nfunction writeEmptyTargetGlobalEntry(txn) {\r\n    const globalStore = txn.store(DbTargetGlobalStore);\r\n    const metadata = {\r\n        highestTargetId: 0,\r\n        highestListenSequenceNumber: 0,\r\n        lastRemoteSnapshotVersion: SnapshotVersion.min().toTimestamp(),\r\n        targetCount: 0\r\n    };\r\n    return globalStore.put(DbTargetGlobalKey, metadata);\r\n}\r\nfunction createClientMetadataStore(db) {\r\n    db.createObjectStore(DbClientMetadataStore, {\r\n        keyPath: DbClientMetadataKeyPath\r\n    });\r\n}\r\nfunction createBundlesStore(db) {\r\n    db.createObjectStore(DbBundleStore, {\r\n        keyPath: DbBundleKeyPath\r\n    });\r\n}\r\nfunction createNamedQueriesStore(db) {\r\n    db.createObjectStore(DbNamedQueryStore, {\r\n        keyPath: DbNamedQueryKeyPath\r\n    });\r\n}\r\nfunction createFieldIndex(db) {\r\n    const indexConfigurationStore = db.createObjectStore(DbIndexConfigurationStore, {\r\n        keyPath: DbIndexConfigurationKeyPath,\r\n        autoIncrement: true\r\n    });\r\n    indexConfigurationStore.createIndex(DbIndexConfigurationCollectionGroupIndex, DbIndexConfigurationCollectionGroupIndexPath, { unique: false });\r\n    const indexStateStore = db.createObjectStore(DbIndexStateStore, {\r\n        keyPath: DbIndexStateKeyPath\r\n    });\r\n    indexStateStore.createIndex(DbIndexStateSequenceNumberIndex, DbIndexStateSequenceNumberIndexPath, { unique: false });\r\n    const indexEntryStore = db.createObjectStore(DbIndexEntryStore, {\r\n        keyPath: DbIndexEntryKeyPath\r\n    });\r\n    indexEntryStore.createIndex(DbIndexEntryDocumentKeyIndex, DbIndexEntryDocumentKeyIndexPath, { unique: false });\r\n}\r\nfunction createDocumentOverlayStore(db) {\r\n    const documentOverlayStore = db.createObjectStore(DbDocumentOverlayStore, {\r\n        keyPath: DbDocumentOverlayKeyPath\r\n    });\r\n    documentOverlayStore.createIndex(DbDocumentOverlayCollectionPathOverlayIndex, DbDocumentOverlayCollectionPathOverlayIndexPath, { unique: false });\r\n    documentOverlayStore.createIndex(DbDocumentOverlayCollectionGroupOverlayIndex, DbDocumentOverlayCollectionGroupOverlayIndexPath, { unique: false });\r\n}\r\nfunction extractKey(remoteDoc) {\r\n    if (remoteDoc.document) {\r\n        return new DocumentKey(ResourcePath.fromString(remoteDoc.document.name).popFirst(5));\r\n    }\r\n    else if (remoteDoc.noDocument) {\r\n        return DocumentKey.fromSegments(remoteDoc.noDocument.path);\r\n    }\r\n    else if (remoteDoc.unknownDocument) {\r\n        return DocumentKey.fromSegments(remoteDoc.unknownDocument.path);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst LOG_TAG$c = 'IndexedDbPersistence';\r\n/**\r\n * Oldest acceptable age in milliseconds for client metadata before the client\r\n * is considered inactive and its associated data is garbage collected.\r\n */\r\nconst MAX_CLIENT_AGE_MS = 30 * 60 * 1000; // 30 minutes\r\n/**\r\n * Oldest acceptable metadata age for clients that may participate in the\r\n * primary lease election. Clients that have not updated their client metadata\r\n * within 5 seconds are not eligible to receive a primary lease.\r\n */\r\nconst MAX_PRIMARY_ELIGIBLE_AGE_MS = 5000;\r\n/**\r\n * The interval at which clients will update their metadata, including\r\n * refreshing their primary lease if held or potentially trying to acquire it if\r\n * not held.\r\n *\r\n * Primary clients may opportunistically refresh their metadata earlier\r\n * if they're already performing an IndexedDB operation.\r\n */\r\nconst CLIENT_METADATA_REFRESH_INTERVAL_MS = 4000;\r\n/** User-facing error when the primary lease is required but not available. */\r\nconst PRIMARY_LEASE_EXCLUSIVE_ERROR_MSG = 'Failed to obtain exclusive access to the persistence layer. To allow ' +\r\n    'shared access, multi-tab synchronization has to be enabled in all tabs. ' +\r\n    'If you are using `experimentalForceOwningTab:true`, make sure that only ' +\r\n    'one tab has persistence enabled at any given time.';\r\nconst UNSUPPORTED_PLATFORM_ERROR_MSG = 'This platform is either missing IndexedDB or is known to have ' +\r\n    'an incomplete implementation. Offline persistence has been disabled.';\r\n// The format of the LocalStorage key that stores zombied client is:\r\n//     firestore_zombie_<persistence_prefix>_<instance_key>\r\nconst ZOMBIED_CLIENTS_KEY_PREFIX = 'firestore_zombie';\r\n/**\r\n * The name of the main (and currently only) IndexedDB database. This name is\r\n * appended to the prefix provided to the IndexedDbPersistence constructor.\r\n */\r\nconst MAIN_DATABASE = 'main';\r\n/**\r\n * An IndexedDB-backed instance of Persistence. Data is stored persistently\r\n * across sessions.\r\n *\r\n * On Web only, the Firestore SDKs support shared access to its persistence\r\n * layer. This allows multiple browser tabs to read and write to IndexedDb and\r\n * to synchronize state even without network connectivity. Shared access is\r\n * currently optional and not enabled unless all clients invoke\r\n * `enablePersistence()` with `{synchronizeTabs:true}`.\r\n *\r\n * In multi-tab mode, if multiple clients are active at the same time, the SDK\r\n * will designate one client as the “primary client”. An effort is made to pick\r\n * a visible, network-connected and active client, and this client is\r\n * responsible for letting other clients know about its presence. The primary\r\n * client writes a unique client-generated identifier (the client ID) to\r\n * IndexedDb’s “owner” store every 4 seconds. If the primary client fails to\r\n * update this entry, another client can acquire the lease and take over as\r\n * primary.\r\n *\r\n * Some persistence operations in the SDK are designated as primary-client only\r\n * operations. This includes the acknowledgment of mutations and all updates of\r\n * remote documents. The effects of these operations are written to persistence\r\n * and then broadcast to other tabs via LocalStorage (see\r\n * `WebStorageSharedClientState`), which then refresh their state from\r\n * persistence.\r\n *\r\n * Similarly, the primary client listens to notifications sent by secondary\r\n * clients to discover persistence changes written by secondary clients, such as\r\n * the addition of new mutations and query targets.\r\n *\r\n * If multi-tab is not enabled and another tab already obtained the primary\r\n * lease, IndexedDbPersistence enters a failed state and all subsequent\r\n * operations will automatically fail.\r\n *\r\n * Additionally, there is an optimization so that when a tab is closed, the\r\n * primary lease is released immediately (this is especially important to make\r\n * sure that a refreshed tab is able to immediately re-acquire the primary\r\n * lease). Unfortunately, IndexedDB cannot be reliably used in window.unload\r\n * since it is an asynchronous API. So in addition to attempting to give up the\r\n * lease, the leaseholder writes its client ID to a \"zombiedClient\" entry in\r\n * LocalStorage which acts as an indicator that another tab should go ahead and\r\n * take the primary lease immediately regardless of the current lease timestamp.\r\n *\r\n * TODO(b/114226234): Remove `synchronizeTabs` section when multi-tab is no\r\n * longer optional.\r\n */\r\nclass IndexedDbPersistence {\r\n    constructor(\r\n    /**\r\n     * Whether to synchronize the in-memory state of multiple tabs and share\r\n     * access to local persistence.\r\n     */\r\n    allowTabSynchronization, persistenceKey, clientId, lruParams, queue, window, document, serializer, sequenceNumberSyncer, \r\n    /**\r\n     * If set to true, forcefully obtains database access. Existing tabs will\r\n     * no longer be able to access IndexedDB.\r\n     */\r\n    forceOwningTab, schemaVersion = SCHEMA_VERSION) {\r\n        this.allowTabSynchronization = allowTabSynchronization;\r\n        this.persistenceKey = persistenceKey;\r\n        this.clientId = clientId;\r\n        this.queue = queue;\r\n        this.window = window;\r\n        this.document = document;\r\n        this.sequenceNumberSyncer = sequenceNumberSyncer;\r\n        this.forceOwningTab = forceOwningTab;\r\n        this.schemaVersion = schemaVersion;\r\n        this.listenSequence = null;\r\n        this._started = false;\r\n        this.isPrimary = false;\r\n        this.networkEnabled = true;\r\n        /** Our window.unload handler, if registered. */\r\n        this.windowUnloadHandler = null;\r\n        this.inForeground = false;\r\n        /** Our 'visibilitychange' listener if registered. */\r\n        this.documentVisibilityHandler = null;\r\n        /** The client metadata refresh task. */\r\n        this.clientMetadataRefresher = null;\r\n        /** The last time we garbage collected the client metadata object store. */\r\n        this.lastGarbageCollectionTime = Number.NEGATIVE_INFINITY;\r\n        /** A listener to notify on primary state changes. */\r\n        this.primaryStateListener = _ => Promise.resolve();\r\n        if (!IndexedDbPersistence.isAvailable()) {\r\n            throw new FirestoreError(Code.UNIMPLEMENTED, UNSUPPORTED_PLATFORM_ERROR_MSG);\r\n        }\r\n        this.referenceDelegate = new IndexedDbLruDelegateImpl(this, lruParams);\r\n        this.dbName = persistenceKey + MAIN_DATABASE;\r\n        this.serializer = new LocalSerializer(serializer);\r\n        this.simpleDb = new SimpleDb(this.dbName, this.schemaVersion, new SchemaConverter(this.serializer));\r\n        this.targetCache = new IndexedDbTargetCache(this.referenceDelegate, this.serializer);\r\n        this.remoteDocumentCache = newIndexedDbRemoteDocumentCache(this.serializer);\r\n        this.bundleCache = new IndexedDbBundleCache();\r\n        if (this.window && this.window.localStorage) {\r\n            this.webStorage = this.window.localStorage;\r\n        }\r\n        else {\r\n            this.webStorage = null;\r\n            if (forceOwningTab === false) {\r\n                logError(LOG_TAG$c, 'LocalStorage is unavailable. As a result, persistence may not work ' +\r\n                    'reliably. In particular enablePersistence() could fail immediately ' +\r\n                    'after refreshing the page.');\r\n            }\r\n        }\r\n    }\r\n    /**\r\n     * Attempt to start IndexedDb persistence.\r\n     *\r\n     * @returns Whether persistence was enabled.\r\n     */\r\n    start() {\r\n        // NOTE: This is expected to fail sometimes (in the case of another tab\r\n        // already having the persistence lock), so it's the first thing we should\r\n        // do.\r\n        return this.updateClientMetadataAndTryBecomePrimary()\r\n            .then(() => {\r\n            if (!this.isPrimary && !this.allowTabSynchronization) {\r\n                // Fail `start()` if `synchronizeTabs` is disabled and we cannot\r\n                // obtain the primary lease.\r\n                throw new FirestoreError(Code.FAILED_PRECONDITION, PRIMARY_LEASE_EXCLUSIVE_ERROR_MSG);\r\n            }\r\n            this.attachVisibilityHandler();\r\n            this.attachWindowUnloadHook();\r\n            this.scheduleClientMetadataAndPrimaryLeaseRefreshes();\r\n            return this.runTransaction('getHighestListenSequenceNumber', 'readonly', txn => this.targetCache.getHighestSequenceNumber(txn));\r\n        })\r\n            .then(highestListenSequenceNumber => {\r\n            this.listenSequence = new ListenSequence(highestListenSequenceNumber, this.sequenceNumberSyncer);\r\n        })\r\n            .then(() => {\r\n            this._started = true;\r\n        })\r\n            .catch(reason => {\r\n            this.simpleDb && this.simpleDb.close();\r\n            return Promise.reject(reason);\r\n        });\r\n    }\r\n    /**\r\n     * Registers a listener that gets called when the primary state of the\r\n     * instance changes. Upon registering, this listener is invoked immediately\r\n     * with the current primary state.\r\n     *\r\n     * PORTING NOTE: This is only used for Web multi-tab.\r\n     */\r\n    setPrimaryStateListener(primaryStateListener) {\r\n        this.primaryStateListener = async (primaryState) => {\r\n            if (this.started) {\r\n                return primaryStateListener(primaryState);\r\n            }\r\n        };\r\n        return primaryStateListener(this.isPrimary);\r\n    }\r\n    /**\r\n     * Registers a listener that gets called when the database receives a\r\n     * version change event indicating that it has deleted.\r\n     *\r\n     * PORTING NOTE: This is only used for Web multi-tab.\r\n     */\r\n    setDatabaseDeletedListener(databaseDeletedListener) {\r\n        this.simpleDb.setVersionChangeListener(async (event) => {\r\n            // Check if an attempt is made to delete IndexedDB.\r\n            if (event.newVersion === null) {\r\n                await databaseDeletedListener();\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * Adjusts the current network state in the client's metadata, potentially\r\n     * affecting the primary lease.\r\n     *\r\n     * PORTING NOTE: This is only used for Web multi-tab.\r\n     */\r\n    setNetworkEnabled(networkEnabled) {\r\n        if (this.networkEnabled !== networkEnabled) {\r\n            this.networkEnabled = networkEnabled;\r\n            // Schedule a primary lease refresh for immediate execution. The eventual\r\n            // lease update will be propagated via `primaryStateListener`.\r\n            this.queue.enqueueAndForget(async () => {\r\n                if (this.started) {\r\n                    await this.updateClientMetadataAndTryBecomePrimary();\r\n                }\r\n            });\r\n        }\r\n    }\r\n    /**\r\n     * Updates the client metadata in IndexedDb and attempts to either obtain or\r\n     * extend the primary lease for the local client. Asynchronously notifies the\r\n     * primary state listener if the client either newly obtained or released its\r\n     * primary lease.\r\n     */\r\n    updateClientMetadataAndTryBecomePrimary() {\r\n        return this.runTransaction('updateClientMetadataAndTryBecomePrimary', 'readwrite', txn => {\r\n            const metadataStore = clientMetadataStore(txn);\r\n            return metadataStore\r\n                .put({\r\n                clientId: this.clientId,\r\n                updateTimeMs: Date.now(),\r\n                networkEnabled: this.networkEnabled,\r\n                inForeground: this.inForeground\r\n            })\r\n                .next(() => {\r\n                if (this.isPrimary) {\r\n                    return this.verifyPrimaryLease(txn).next(success => {\r\n                        if (!success) {\r\n                            this.isPrimary = false;\r\n                            this.queue.enqueueRetryable(() => this.primaryStateListener(false));\r\n                        }\r\n                    });\r\n                }\r\n            })\r\n                .next(() => this.canActAsPrimary(txn))\r\n                .next(canActAsPrimary => {\r\n                if (this.isPrimary && !canActAsPrimary) {\r\n                    return this.releasePrimaryLeaseIfHeld(txn).next(() => false);\r\n                }\r\n                else if (canActAsPrimary) {\r\n                    return this.acquireOrExtendPrimaryLease(txn).next(() => true);\r\n                }\r\n                else {\r\n                    return /* canActAsPrimary= */ false;\r\n                }\r\n            });\r\n        })\r\n            .catch(e => {\r\n            if (isIndexedDbTransactionError(e)) {\r\n                logDebug(LOG_TAG$c, 'Failed to extend owner lease: ', e);\r\n                // Proceed with the existing state. Any subsequent access to\r\n                // IndexedDB will verify the lease.\r\n                return this.isPrimary;\r\n            }\r\n            if (!this.allowTabSynchronization) {\r\n                throw e;\r\n            }\r\n            logDebug(LOG_TAG$c, 'Releasing owner lease after error during lease refresh', e);\r\n            return /* isPrimary= */ false;\r\n        })\r\n            .then(isPrimary => {\r\n            if (this.isPrimary !== isPrimary) {\r\n                this.queue.enqueueRetryable(() => this.primaryStateListener(isPrimary));\r\n            }\r\n            this.isPrimary = isPrimary;\r\n        });\r\n    }\r\n    verifyPrimaryLease(txn) {\r\n        const store = primaryClientStore(txn);\r\n        return store.get(DbPrimaryClientKey).next(primaryClient => {\r\n            return PersistencePromise.resolve(this.isLocalClient(primaryClient));\r\n        });\r\n    }\r\n    removeClientMetadata(txn) {\r\n        const metadataStore = clientMetadataStore(txn);\r\n        return metadataStore.delete(this.clientId);\r\n    }\r\n    /**\r\n     * If the garbage collection threshold has passed, prunes the\r\n     * RemoteDocumentChanges and the ClientMetadata store based on the last update\r\n     * time of all clients.\r\n     */\r\n    async maybeGarbageCollectMultiClientState() {\r\n        if (this.isPrimary &&\r\n            !this.isWithinAge(this.lastGarbageCollectionTime, MAX_CLIENT_AGE_MS)) {\r\n            this.lastGarbageCollectionTime = Date.now();\r\n            const inactiveClients = await this.runTransaction('maybeGarbageCollectMultiClientState', 'readwrite-primary', txn => {\r\n                const metadataStore = getStore(txn, DbClientMetadataStore);\r\n                return metadataStore.loadAll().next(existingClients => {\r\n                    const active = this.filterActiveClients(existingClients, MAX_CLIENT_AGE_MS);\r\n                    const inactive = existingClients.filter(client => active.indexOf(client) === -1);\r\n                    // Delete metadata for clients that are no longer considered active.\r\n                    return PersistencePromise.forEach(inactive, (inactiveClient) => metadataStore.delete(inactiveClient.clientId)).next(() => inactive);\r\n                });\r\n            }).catch(() => {\r\n                // Ignore primary lease violations or any other type of error. The next\r\n                // primary will run `maybeGarbageCollectMultiClientState()` again.\r\n                // We don't use `ignoreIfPrimaryLeaseLoss()` since we don't want to depend\r\n                // on LocalStore.\r\n                return [];\r\n            });\r\n            // Delete potential leftover entries that may continue to mark the\r\n            // inactive clients as zombied in LocalStorage.\r\n            // Ideally we'd delete the IndexedDb and LocalStorage zombie entries for\r\n            // the client atomically, but we can't. So we opt to delete the IndexedDb\r\n            // entries first to avoid potentially reviving a zombied client.\r\n            if (this.webStorage) {\r\n                for (const inactiveClient of inactiveClients) {\r\n                    this.webStorage.removeItem(this.zombiedClientLocalStorageKey(inactiveClient.clientId));\r\n                }\r\n            }\r\n        }\r\n    }\r\n    /**\r\n     * Schedules a recurring timer to update the client metadata and to either\r\n     * extend or acquire the primary lease if the client is eligible.\r\n     */\r\n    scheduleClientMetadataAndPrimaryLeaseRefreshes() {\r\n        this.clientMetadataRefresher = this.queue.enqueueAfterDelay(\"client_metadata_refresh\" /* ClientMetadataRefresh */, CLIENT_METADATA_REFRESH_INTERVAL_MS, () => {\r\n            return this.updateClientMetadataAndTryBecomePrimary()\r\n                .then(() => this.maybeGarbageCollectMultiClientState())\r\n                .then(() => this.scheduleClientMetadataAndPrimaryLeaseRefreshes());\r\n        });\r\n    }\r\n    /** Checks whether `client` is the local client. */\r\n    isLocalClient(client) {\r\n        return client ? client.ownerId === this.clientId : false;\r\n    }\r\n    /**\r\n     * Evaluate the state of all active clients and determine whether the local\r\n     * client is or can act as the holder of the primary lease. Returns whether\r\n     * the client is eligible for the lease, but does not actually acquire it.\r\n     * May return 'false' even if there is no active leaseholder and another\r\n     * (foreground) client should become leaseholder instead.\r\n     */\r\n    canActAsPrimary(txn) {\r\n        if (this.forceOwningTab) {\r\n            return PersistencePromise.resolve(true);\r\n        }\r\n        const store = primaryClientStore(txn);\r\n        return store\r\n            .get(DbPrimaryClientKey)\r\n            .next(currentPrimary => {\r\n            const currentLeaseIsValid = currentPrimary !== null &&\r\n                this.isWithinAge(currentPrimary.leaseTimestampMs, MAX_PRIMARY_ELIGIBLE_AGE_MS) &&\r\n                !this.isClientZombied(currentPrimary.ownerId);\r\n            // A client is eligible for the primary lease if:\r\n            // - its network is enabled and the client's tab is in the foreground.\r\n            // - its network is enabled and no other client's tab is in the\r\n            //   foreground.\r\n            // - every clients network is disabled and the client's tab is in the\r\n            //   foreground.\r\n            // - every clients network is disabled and no other client's tab is in\r\n            //   the foreground.\r\n            // - the `forceOwningTab` setting was passed in.\r\n            if (currentLeaseIsValid) {\r\n                if (this.isLocalClient(currentPrimary) && this.networkEnabled) {\r\n                    return true;\r\n                }\r\n                if (!this.isLocalClient(currentPrimary)) {\r\n                    if (!currentPrimary.allowTabSynchronization) {\r\n                        // Fail the `canActAsPrimary` check if the current leaseholder has\r\n                        // not opted into multi-tab synchronization. If this happens at\r\n                        // client startup, we reject the Promise returned by\r\n                        // `enablePersistence()` and the user can continue to use Firestore\r\n                        // with in-memory persistence.\r\n                        // If this fails during a lease refresh, we will instead block the\r\n                        // AsyncQueue from executing further operations. Note that this is\r\n                        // acceptable since mixing & matching different `synchronizeTabs`\r\n                        // settings is not supported.\r\n                        //\r\n                        // TODO(b/114226234): Remove this check when `synchronizeTabs` can\r\n                        // no longer be turned off.\r\n                        throw new FirestoreError(Code.FAILED_PRECONDITION, PRIMARY_LEASE_EXCLUSIVE_ERROR_MSG);\r\n                    }\r\n                    return false;\r\n                }\r\n            }\r\n            if (this.networkEnabled && this.inForeground) {\r\n                return true;\r\n            }\r\n            return clientMetadataStore(txn)\r\n                .loadAll()\r\n                .next(existingClients => {\r\n                // Process all existing clients and determine whether at least one of\r\n                // them is better suited to obtain the primary lease.\r\n                const preferredCandidate = this.filterActiveClients(existingClients, MAX_PRIMARY_ELIGIBLE_AGE_MS).find(otherClient => {\r\n                    if (this.clientId !== otherClient.clientId) {\r\n                        const otherClientHasBetterNetworkState = !this.networkEnabled && otherClient.networkEnabled;\r\n                        const otherClientHasBetterVisibility = !this.inForeground && otherClient.inForeground;\r\n                        const otherClientHasSameNetworkState = this.networkEnabled === otherClient.networkEnabled;\r\n                        if (otherClientHasBetterNetworkState ||\r\n                            (otherClientHasBetterVisibility &&\r\n                                otherClientHasSameNetworkState)) {\r\n                            return true;\r\n                        }\r\n                    }\r\n                    return false;\r\n                });\r\n                return preferredCandidate === undefined;\r\n            });\r\n        })\r\n            .next(canActAsPrimary => {\r\n            if (this.isPrimary !== canActAsPrimary) {\r\n                logDebug(LOG_TAG$c, `Client ${canActAsPrimary ? 'is' : 'is not'} eligible for a primary lease.`);\r\n            }\r\n            return canActAsPrimary;\r\n        });\r\n    }\r\n    async shutdown() {\r\n        // The shutdown() operations are idempotent and can be called even when\r\n        // start() aborted (e.g. because it couldn't acquire the persistence lease).\r\n        this._started = false;\r\n        this.markClientZombied();\r\n        if (this.clientMetadataRefresher) {\r\n            this.clientMetadataRefresher.cancel();\r\n            this.clientMetadataRefresher = null;\r\n        }\r\n        this.detachVisibilityHandler();\r\n        this.detachWindowUnloadHook();\r\n        // Use `SimpleDb.runTransaction` directly to avoid failing if another tab\r\n        // has obtained the primary lease.\r\n        await this.simpleDb.runTransaction('shutdown', 'readwrite', [DbPrimaryClientStore, DbClientMetadataStore], simpleDbTxn => {\r\n            const persistenceTransaction = new IndexedDbTransaction(simpleDbTxn, ListenSequence.INVALID);\r\n            return this.releasePrimaryLeaseIfHeld(persistenceTransaction).next(() => this.removeClientMetadata(persistenceTransaction));\r\n        });\r\n        this.simpleDb.close();\r\n        // Remove the entry marking the client as zombied from LocalStorage since\r\n        // we successfully deleted its metadata from IndexedDb.\r\n        this.removeClientZombiedEntry();\r\n    }\r\n    /**\r\n     * Returns clients that are not zombied and have an updateTime within the\r\n     * provided threshold.\r\n     */\r\n    filterActiveClients(clients, activityThresholdMs) {\r\n        return clients.filter(client => this.isWithinAge(client.updateTimeMs, activityThresholdMs) &&\r\n            !this.isClientZombied(client.clientId));\r\n    }\r\n    /**\r\n     * Returns the IDs of the clients that are currently active. If multi-tab\r\n     * is not supported, returns an array that only contains the local client's\r\n     * ID.\r\n     *\r\n     * PORTING NOTE: This is only used for Web multi-tab.\r\n     */\r\n    getActiveClients() {\r\n        return this.runTransaction('getActiveClients', 'readonly', txn => {\r\n            return clientMetadataStore(txn)\r\n                .loadAll()\r\n                .next(clients => this.filterActiveClients(clients, MAX_CLIENT_AGE_MS).map(clientMetadata => clientMetadata.clientId));\r\n        });\r\n    }\r\n    get started() {\r\n        return this._started;\r\n    }\r\n    getMutationQueue(user, indexManager) {\r\n        return IndexedDbMutationQueue.forUser(user, this.serializer, indexManager, this.referenceDelegate);\r\n    }\r\n    getTargetCache() {\r\n        return this.targetCache;\r\n    }\r\n    getRemoteDocumentCache() {\r\n        return this.remoteDocumentCache;\r\n    }\r\n    getIndexManager(user) {\r\n        return new IndexedDbIndexManager(user, this.serializer.remoteSerializer.databaseId);\r\n    }\r\n    getDocumentOverlayCache(user) {\r\n        return IndexedDbDocumentOverlayCache.forUser(this.serializer, user);\r\n    }\r\n    getBundleCache() {\r\n        return this.bundleCache;\r\n    }\r\n    runTransaction(action, mode, transactionOperation) {\r\n        logDebug(LOG_TAG$c, 'Starting transaction:', action);\r\n        const simpleDbMode = mode === 'readonly' ? 'readonly' : 'readwrite';\r\n        const objectStores = getObjectStores(this.schemaVersion);\r\n        let persistenceTransaction;\r\n        // Do all transactions as readwrite against all object stores, since we\r\n        // are the only reader/writer.\r\n        return this.simpleDb\r\n            .runTransaction(action, simpleDbMode, objectStores, simpleDbTxn => {\r\n            persistenceTransaction = new IndexedDbTransaction(simpleDbTxn, this.listenSequence\r\n                ? this.listenSequence.next()\r\n                : ListenSequence.INVALID);\r\n            if (mode === 'readwrite-primary') {\r\n                // While we merely verify that we have (or can acquire) the lease\r\n                // immediately, we wait to extend the primary lease until after\r\n                // executing transactionOperation(). This ensures that even if the\r\n                // transactionOperation takes a long time, we'll use a recent\r\n                // leaseTimestampMs in the extended (or newly acquired) lease.\r\n                return this.verifyPrimaryLease(persistenceTransaction)\r\n                    .next(holdsPrimaryLease => {\r\n                    if (holdsPrimaryLease) {\r\n                        return /* holdsPrimaryLease= */ true;\r\n                    }\r\n                    return this.canActAsPrimary(persistenceTransaction);\r\n                })\r\n                    .next(holdsPrimaryLease => {\r\n                    if (!holdsPrimaryLease) {\r\n                        logError(`Failed to obtain primary lease for action '${action}'.`);\r\n                        this.isPrimary = false;\r\n                        this.queue.enqueueRetryable(() => this.primaryStateListener(false));\r\n                        throw new FirestoreError(Code.FAILED_PRECONDITION, PRIMARY_LEASE_LOST_ERROR_MSG);\r\n                    }\r\n                    return transactionOperation(persistenceTransaction);\r\n                })\r\n                    .next(result => {\r\n                    return this.acquireOrExtendPrimaryLease(persistenceTransaction).next(() => result);\r\n                });\r\n            }\r\n            else {\r\n                return this.verifyAllowTabSynchronization(persistenceTransaction).next(() => transactionOperation(persistenceTransaction));\r\n            }\r\n        })\r\n            .then(result => {\r\n            persistenceTransaction.raiseOnCommittedEvent();\r\n            return result;\r\n        });\r\n    }\r\n    /**\r\n     * Verifies that the current tab is the primary leaseholder or alternatively\r\n     * that the leaseholder has opted into multi-tab synchronization.\r\n     */\r\n    // TODO(b/114226234): Remove this check when `synchronizeTabs` can no longer\r\n    // be turned off.\r\n    verifyAllowTabSynchronization(txn) {\r\n        const store = primaryClientStore(txn);\r\n        return store.get(DbPrimaryClientKey).next(currentPrimary => {\r\n            const currentLeaseIsValid = currentPrimary !== null &&\r\n                this.isWithinAge(currentPrimary.leaseTimestampMs, MAX_PRIMARY_ELIGIBLE_AGE_MS) &&\r\n                !this.isClientZombied(currentPrimary.ownerId);\r\n            if (currentLeaseIsValid && !this.isLocalClient(currentPrimary)) {\r\n                if (!this.forceOwningTab &&\r\n                    (!this.allowTabSynchronization ||\r\n                        !currentPrimary.allowTabSynchronization)) {\r\n                    throw new FirestoreError(Code.FAILED_PRECONDITION, PRIMARY_LEASE_EXCLUSIVE_ERROR_MSG);\r\n                }\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * Obtains or extends the new primary lease for the local client. This\r\n     * method does not verify that the client is eligible for this lease.\r\n     */\r\n    acquireOrExtendPrimaryLease(txn) {\r\n        const newPrimary = {\r\n            ownerId: this.clientId,\r\n            allowTabSynchronization: this.allowTabSynchronization,\r\n            leaseTimestampMs: Date.now()\r\n        };\r\n        return primaryClientStore(txn).put(DbPrimaryClientKey, newPrimary);\r\n    }\r\n    static isAvailable() {\r\n        return SimpleDb.isAvailable();\r\n    }\r\n    /** Checks the primary lease and removes it if we are the current primary. */\r\n    releasePrimaryLeaseIfHeld(txn) {\r\n        const store = primaryClientStore(txn);\r\n        return store.get(DbPrimaryClientKey).next(primaryClient => {\r\n            if (this.isLocalClient(primaryClient)) {\r\n                logDebug(LOG_TAG$c, 'Releasing primary lease.');\r\n                return store.delete(DbPrimaryClientKey);\r\n            }\r\n            else {\r\n                return PersistencePromise.resolve();\r\n            }\r\n        });\r\n    }\r\n    /** Verifies that `updateTimeMs` is within `maxAgeMs`. */\r\n    isWithinAge(updateTimeMs, maxAgeMs) {\r\n        const now = Date.now();\r\n        const minAcceptable = now - maxAgeMs;\r\n        const maxAcceptable = now;\r\n        if (updateTimeMs < minAcceptable) {\r\n            return false;\r\n        }\r\n        else if (updateTimeMs > maxAcceptable) {\r\n            logError(`Detected an update time that is in the future: ${updateTimeMs} > ${maxAcceptable}`);\r\n            return false;\r\n        }\r\n        return true;\r\n    }\r\n    attachVisibilityHandler() {\r\n        if (this.document !== null &&\r\n            typeof this.document.addEventListener === 'function') {\r\n            this.documentVisibilityHandler = () => {\r\n                this.queue.enqueueAndForget(() => {\r\n                    this.inForeground = this.document.visibilityState === 'visible';\r\n                    return this.updateClientMetadataAndTryBecomePrimary();\r\n                });\r\n            };\r\n            this.document.addEventListener('visibilitychange', this.documentVisibilityHandler);\r\n            this.inForeground = this.document.visibilityState === 'visible';\r\n        }\r\n    }\r\n    detachVisibilityHandler() {\r\n        if (this.documentVisibilityHandler) {\r\n            this.document.removeEventListener('visibilitychange', this.documentVisibilityHandler);\r\n            this.documentVisibilityHandler = null;\r\n        }\r\n    }\r\n    /**\r\n     * Attaches a window.unload handler that will synchronously write our\r\n     * clientId to a \"zombie client id\" location in LocalStorage. This can be used\r\n     * by tabs trying to acquire the primary lease to determine that the lease\r\n     * is no longer valid even if the timestamp is recent. This is particularly\r\n     * important for the refresh case (so the tab correctly re-acquires the\r\n     * primary lease). LocalStorage is used for this rather than IndexedDb because\r\n     * it is a synchronous API and so can be used reliably from  an unload\r\n     * handler.\r\n     */\r\n    attachWindowUnloadHook() {\r\n        var _a;\r\n        if (typeof ((_a = this.window) === null || _a === void 0 ? void 0 : _a.addEventListener) === 'function') {\r\n            this.windowUnloadHandler = () => {\r\n                // Note: In theory, this should be scheduled on the AsyncQueue since it\r\n                // accesses internal state. We execute this code directly during shutdown\r\n                // to make sure it gets a chance to run.\r\n                this.markClientZombied();\r\n                if (isSafari() && navigator.appVersion.match(/Version\\/1[45]/)) {\r\n                    // On Safari 14 and 15, we do not run any cleanup actions as it might\r\n                    // trigger a bug that prevents Safari from re-opening IndexedDB during\r\n                    // the next page load.\r\n                    // See https://bugs.webkit.org/show_bug.cgi?id=226547\r\n                    this.queue.enterRestrictedMode(/* purgeExistingTasks= */ true);\r\n                }\r\n                this.queue.enqueueAndForget(() => {\r\n                    // Attempt graceful shutdown (including releasing our primary lease),\r\n                    // but there's no guarantee it will complete.\r\n                    return this.shutdown();\r\n                });\r\n            };\r\n            this.window.addEventListener('pagehide', this.windowUnloadHandler);\r\n        }\r\n    }\r\n    detachWindowUnloadHook() {\r\n        if (this.windowUnloadHandler) {\r\n            this.window.removeEventListener('pagehide', this.windowUnloadHandler);\r\n            this.windowUnloadHandler = null;\r\n        }\r\n    }\r\n    /**\r\n     * Returns whether a client is \"zombied\" based on its LocalStorage entry.\r\n     * Clients become zombied when their tab closes without running all of the\r\n     * cleanup logic in `shutdown()`.\r\n     */\r\n    isClientZombied(clientId) {\r\n        var _a;\r\n        try {\r\n            const isZombied = ((_a = this.webStorage) === null || _a === void 0 ? void 0 : _a.getItem(this.zombiedClientLocalStorageKey(clientId))) !== null;\r\n            logDebug(LOG_TAG$c, `Client '${clientId}' ${isZombied ? 'is' : 'is not'} zombied in LocalStorage`);\r\n            return isZombied;\r\n        }\r\n        catch (e) {\r\n            // Gracefully handle if LocalStorage isn't working.\r\n            logError(LOG_TAG$c, 'Failed to get zombied client id.', e);\r\n            return false;\r\n        }\r\n    }\r\n    /**\r\n     * Record client as zombied (a client that had its tab closed). Zombied\r\n     * clients are ignored during primary tab selection.\r\n     */\r\n    markClientZombied() {\r\n        if (!this.webStorage) {\r\n            return;\r\n        }\r\n        try {\r\n            this.webStorage.setItem(this.zombiedClientLocalStorageKey(this.clientId), String(Date.now()));\r\n        }\r\n        catch (e) {\r\n            // Gracefully handle if LocalStorage isn't available / working.\r\n            logError('Failed to set zombie client id.', e);\r\n        }\r\n    }\r\n    /** Removes the zombied client entry if it exists. */\r\n    removeClientZombiedEntry() {\r\n        if (!this.webStorage) {\r\n            return;\r\n        }\r\n        try {\r\n            this.webStorage.removeItem(this.zombiedClientLocalStorageKey(this.clientId));\r\n        }\r\n        catch (e) {\r\n            // Ignore\r\n        }\r\n    }\r\n    zombiedClientLocalStorageKey(clientId) {\r\n        return `${ZOMBIED_CLIENTS_KEY_PREFIX}_${this.persistenceKey}_${clientId}`;\r\n    }\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the primary client object store.\r\n */\r\nfunction primaryClientStore(txn) {\r\n    return getStore(txn, DbPrimaryClientStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the client metadata object store.\r\n */\r\nfunction clientMetadataStore(txn) {\r\n    return getStore(txn, DbClientMetadataStore);\r\n}\r\n/**\r\n * Generates a string used as a prefix when storing data in IndexedDB and\r\n * LocalStorage.\r\n */\r\nfunction indexedDbStoragePrefix(databaseId, persistenceKey) {\r\n    // Use two different prefix formats:\r\n    //\r\n    //   * firestore / persistenceKey / projectID . databaseID / ...\r\n    //   * firestore / persistenceKey / projectID / ...\r\n    //\r\n    // projectIDs are DNS-compatible names and cannot contain dots\r\n    // so there's no danger of collisions.\r\n    let database = databaseId.projectId;\r\n    if (!databaseId.isDefaultDatabase) {\r\n        database += '.' + databaseId.database;\r\n    }\r\n    return 'firestore/' + persistenceKey + '/' + database + '/';\r\n}\r\nasync function indexedDbClearPersistence(persistenceKey) {\r\n    if (!SimpleDb.isAvailable()) {\r\n        return Promise.resolve();\r\n    }\r\n    const dbName = persistenceKey + MAIN_DATABASE;\r\n    await SimpleDb.delete(dbName);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Compares two array for equality using comparator. The method computes the\r\n * intersection and invokes `onAdd` for every element that is in `after` but not\r\n * `before`. `onRemove` is invoked for every element in `before` but missing\r\n * from `after`.\r\n *\r\n * The method creates a copy of both `before` and `after` and runs in O(n log\r\n * n), where n is the size of the two lists.\r\n *\r\n * @param before - The elements that exist in the original array.\r\n * @param after - The elements to diff against the original array.\r\n * @param comparator - The comparator for the elements in before and after.\r\n * @param onAdd - A function to invoke for every element that is part of `\r\n * after` but not `before`.\r\n * @param onRemove - A function to invoke for every element that is part of\r\n * `before` but not `after`.\r\n */\r\nfunction diffArrays(before, after, comparator, onAdd, onRemove) {\r\n    before = [...before];\r\n    after = [...after];\r\n    before.sort(comparator);\r\n    after.sort(comparator);\r\n    const bLen = before.length;\r\n    const aLen = after.length;\r\n    let a = 0;\r\n    let b = 0;\r\n    while (a < aLen && b < bLen) {\r\n        const cmp = comparator(before[b], after[a]);\r\n        if (cmp < 0) {\r\n            // The element was removed if the next element in our ordered\r\n            // walkthrough is only in `before`.\r\n            onRemove(before[b++]);\r\n        }\r\n        else if (cmp > 0) {\r\n            // The element was added if the next element in our ordered walkthrough\r\n            // is only in `after`.\r\n            onAdd(after[a++]);\r\n        }\r\n        else {\r\n            a++;\r\n            b++;\r\n        }\r\n    }\r\n    while (a < aLen) {\r\n        onAdd(after[a++]);\r\n    }\r\n    while (b < bLen) {\r\n        onRemove(before[b++]);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst LOG_TAG$b = 'LocalStore';\r\n/**\r\n * The maximum time to leave a resume token buffered without writing it out.\r\n * This value is arbitrary: it's long enough to avoid several writes\r\n * (possibly indefinitely if updates come more frequently than this) but\r\n * short enough that restarting after crashing will still have a pretty\r\n * recent resume token.\r\n */\r\nconst RESUME_TOKEN_MAX_AGE_MICROS = 5 * 60 * 1e6;\r\n/**\r\n * Implements `LocalStore` interface.\r\n *\r\n * Note: some field defined in this class might have public access level, but\r\n * the class is not exported so they are only accessible from this module.\r\n * This is useful to implement optional features (like bundles) in free\r\n * functions, such that they are tree-shakeable.\r\n */\r\nclass LocalStoreImpl {\r\n    constructor(\r\n    /** Manages our in-memory or durable persistence. */\r\n    persistence, queryEngine, initialUser, serializer) {\r\n        this.persistence = persistence;\r\n        this.queryEngine = queryEngine;\r\n        this.serializer = serializer;\r\n        /**\r\n         * Maps a targetID to data about its target.\r\n         *\r\n         * PORTING NOTE: We are using an immutable data structure on Web to make re-runs\r\n         * of `applyRemoteEvent()` idempotent.\r\n         */\r\n        this.targetDataByTarget = new SortedMap(primitiveComparator);\r\n        /** Maps a target to its targetID. */\r\n        // TODO(wuandy): Evaluate if TargetId can be part of Target.\r\n        this.targetIdByTarget = new ObjectMap(t => canonifyTarget(t), targetEquals);\r\n        /**\r\n         * A per collection group index of the last read time processed by\r\n         * `getNewDocumentChanges()`.\r\n         *\r\n         * PORTING NOTE: This is only used for multi-tab synchronization.\r\n         */\r\n        this.collectionGroupReadTime = new Map();\r\n        this.remoteDocuments = persistence.getRemoteDocumentCache();\r\n        this.targetCache = persistence.getTargetCache();\r\n        this.bundleCache = persistence.getBundleCache();\r\n        this.initializeUserComponents(initialUser);\r\n    }\r\n    initializeUserComponents(user) {\r\n        // TODO(indexing): Add spec tests that test these components change after a\r\n        // user change\r\n        this.documentOverlayCache = this.persistence.getDocumentOverlayCache(user);\r\n        this.indexManager = this.persistence.getIndexManager(user);\r\n        this.mutationQueue = this.persistence.getMutationQueue(user, this.indexManager);\r\n        this.localDocuments = new LocalDocumentsView(this.remoteDocuments, this.mutationQueue, this.documentOverlayCache, this.indexManager);\r\n        this.remoteDocuments.setIndexManager(this.indexManager);\r\n        this.queryEngine.initialize(this.localDocuments, this.indexManager);\r\n    }\r\n    collectGarbage(garbageCollector) {\r\n        return this.persistence.runTransaction('Collect garbage', 'readwrite-primary', txn => garbageCollector.collect(txn, this.targetDataByTarget));\r\n    }\r\n}\r\nfunction newLocalStore(\r\n/** Manages our in-memory or durable persistence. */\r\npersistence, queryEngine, initialUser, serializer) {\r\n    return new LocalStoreImpl(persistence, queryEngine, initialUser, serializer);\r\n}\r\n/**\r\n * Tells the LocalStore that the currently authenticated user has changed.\r\n *\r\n * In response the local store switches the mutation queue to the new user and\r\n * returns any resulting document changes.\r\n */\r\n// PORTING NOTE: Android and iOS only return the documents affected by the\r\n// change.\r\nasync function localStoreHandleUserChange(localStore, user) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    const result = await localStoreImpl.persistence.runTransaction('Handle user change', 'readonly', txn => {\r\n        // Swap out the mutation queue, grabbing the pending mutation batches\r\n        // before and after.\r\n        let oldBatches;\r\n        return localStoreImpl.mutationQueue\r\n            .getAllMutationBatches(txn)\r\n            .next(promisedOldBatches => {\r\n            oldBatches = promisedOldBatches;\r\n            localStoreImpl.initializeUserComponents(user);\r\n            return localStoreImpl.mutationQueue.getAllMutationBatches(txn);\r\n        })\r\n            .next(newBatches => {\r\n            const removedBatchIds = [];\r\n            const addedBatchIds = [];\r\n            // Union the old/new changed keys.\r\n            let changedKeys = documentKeySet();\r\n            for (const batch of oldBatches) {\r\n                removedBatchIds.push(batch.batchId);\r\n                for (const mutation of batch.mutations) {\r\n                    changedKeys = changedKeys.add(mutation.key);\r\n                }\r\n            }\r\n            for (const batch of newBatches) {\r\n                addedBatchIds.push(batch.batchId);\r\n                for (const mutation of batch.mutations) {\r\n                    changedKeys = changedKeys.add(mutation.key);\r\n                }\r\n            }\r\n            // Return the set of all (potentially) changed documents and the list\r\n            // of mutation batch IDs that were affected by change.\r\n            return localStoreImpl.localDocuments\r\n                .getDocuments(txn, changedKeys)\r\n                .next(affectedDocuments => {\r\n                return {\r\n                    affectedDocuments,\r\n                    removedBatchIds,\r\n                    addedBatchIds\r\n                };\r\n            });\r\n        });\r\n    });\r\n    return result;\r\n}\r\n/* Accepts locally generated Mutations and commit them to storage. */\r\nfunction localStoreWriteLocally(localStore, mutations) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    const localWriteTime = Timestamp.now();\r\n    const keys = mutations.reduce((keys, m) => keys.add(m.key), documentKeySet());\r\n    let overlayedDocuments;\r\n    let mutationBatch;\r\n    return localStoreImpl.persistence\r\n        .runTransaction('Locally write mutations', 'readwrite', txn => {\r\n        // Figure out which keys do not have a remote version in the cache, this\r\n        // is needed to create the right overlay mutation: if no remote version\r\n        // presents, we do not need to create overlays as patch mutations.\r\n        // TODO(Overlay): Is there a better way to determine this? Using the\r\n        //  document version does not work because local mutations set them back\r\n        //  to 0.\r\n        let remoteDocs = mutableDocumentMap();\r\n        let docsWithoutRemoteVersion = documentKeySet();\r\n        return localStoreImpl.remoteDocuments\r\n            .getEntries(txn, keys)\r\n            .next(docs => {\r\n            remoteDocs = docs;\r\n            remoteDocs.forEach((key, doc) => {\r\n                if (!doc.isValidDocument()) {\r\n                    docsWithoutRemoteVersion = docsWithoutRemoteVersion.add(key);\r\n                }\r\n            });\r\n        })\r\n            .next(() => {\r\n            // Load and apply all existing mutations. This lets us compute the\r\n            // current base state for all non-idempotent transforms before applying\r\n            // any additional user-provided writes.\r\n            return localStoreImpl.localDocuments.getOverlayedDocuments(txn, remoteDocs);\r\n        })\r\n            .next((docs) => {\r\n            overlayedDocuments = docs;\r\n            // For non-idempotent mutations (such as `FieldValue.increment()`),\r\n            // we record the base state in a separate patch mutation. This is\r\n            // later used to guarantee consistent values and prevents flicker\r\n            // even if the backend sends us an update that already includes our\r\n            // transform.\r\n            const baseMutations = [];\r\n            for (const mutation of mutations) {\r\n                const baseValue = mutationExtractBaseValue(mutation, overlayedDocuments.get(mutation.key).overlayedDocument);\r\n                if (baseValue != null) {\r\n                    // NOTE: The base state should only be applied if there's some\r\n                    // existing document to override, so use a Precondition of\r\n                    // exists=true\r\n                    baseMutations.push(new PatchMutation(mutation.key, baseValue, extractFieldMask(baseValue.value.mapValue), Precondition.exists(true)));\r\n                }\r\n            }\r\n            return localStoreImpl.mutationQueue.addMutationBatch(txn, localWriteTime, baseMutations, mutations);\r\n        })\r\n            .next(batch => {\r\n            mutationBatch = batch;\r\n            const overlays = batch.applyToLocalDocumentSet(overlayedDocuments, docsWithoutRemoteVersion);\r\n            return localStoreImpl.documentOverlayCache.saveOverlays(txn, batch.batchId, overlays);\r\n        });\r\n    })\r\n        .then(() => ({\r\n        batchId: mutationBatch.batchId,\r\n        changes: convertOverlayedDocumentMapToDocumentMap(overlayedDocuments)\r\n    }));\r\n}\r\n/**\r\n * Acknowledges the given batch.\r\n *\r\n * On the happy path when a batch is acknowledged, the local store will\r\n *\r\n *  + remove the batch from the mutation queue;\r\n *  + apply the changes to the remote document cache;\r\n *  + recalculate the latency compensated view implied by those changes (there\r\n *    may be mutations in the queue that affect the documents but haven't been\r\n *    acknowledged yet); and\r\n *  + give the changed documents back the sync engine\r\n *\r\n * @returns The resulting (modified) documents.\r\n */\r\nfunction localStoreAcknowledgeBatch(localStore, batchResult) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence.runTransaction('Acknowledge batch', 'readwrite-primary', txn => {\r\n        const affected = batchResult.batch.keys();\r\n        const documentBuffer = localStoreImpl.remoteDocuments.newChangeBuffer({\r\n            trackRemovals: true // Make sure document removals show up in `getNewDocumentChanges()`\r\n        });\r\n        return applyWriteToRemoteDocuments(localStoreImpl, txn, batchResult, documentBuffer)\r\n            .next(() => documentBuffer.apply(txn))\r\n            .next(() => localStoreImpl.mutationQueue.performConsistencyCheck(txn))\r\n            .next(() => localStoreImpl.documentOverlayCache.removeOverlaysForBatchId(txn, affected, batchResult.batch.batchId))\r\n            .next(() => localStoreImpl.localDocuments.recalculateAndSaveOverlaysForDocumentKeys(txn, getKeysWithTransformResults(batchResult)))\r\n            .next(() => localStoreImpl.localDocuments.getDocuments(txn, affected));\r\n    });\r\n}\r\nfunction getKeysWithTransformResults(batchResult) {\r\n    let result = documentKeySet();\r\n    for (let i = 0; i < batchResult.mutationResults.length; ++i) {\r\n        const mutationResult = batchResult.mutationResults[i];\r\n        if (mutationResult.transformResults.length > 0) {\r\n            result = result.add(batchResult.batch.mutations[i].key);\r\n        }\r\n    }\r\n    return result;\r\n}\r\n/**\r\n * Removes mutations from the MutationQueue for the specified batch;\r\n * LocalDocuments will be recalculated.\r\n *\r\n * @returns The resulting modified documents.\r\n */\r\nfunction localStoreRejectBatch(localStore, batchId) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence.runTransaction('Reject batch', 'readwrite-primary', txn => {\r\n        let affectedKeys;\r\n        return localStoreImpl.mutationQueue\r\n            .lookupMutationBatch(txn, batchId)\r\n            .next((batch) => {\r\n            hardAssert(batch !== null);\r\n            affectedKeys = batch.keys();\r\n            return localStoreImpl.mutationQueue.removeMutationBatch(txn, batch);\r\n        })\r\n            .next(() => localStoreImpl.mutationQueue.performConsistencyCheck(txn))\r\n            .next(() => localStoreImpl.documentOverlayCache.removeOverlaysForBatchId(txn, affectedKeys, batchId))\r\n            .next(() => localStoreImpl.localDocuments.recalculateAndSaveOverlaysForDocumentKeys(txn, affectedKeys))\r\n            .next(() => localStoreImpl.localDocuments.getDocuments(txn, affectedKeys));\r\n    });\r\n}\r\n/**\r\n * Returns the largest (latest) batch id in mutation queue that is pending\r\n * server response.\r\n *\r\n * Returns `BATCHID_UNKNOWN` if the queue is empty.\r\n */\r\nfunction localStoreGetHighestUnacknowledgedBatchId(localStore) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence.runTransaction('Get highest unacknowledged batch id', 'readonly', txn => localStoreImpl.mutationQueue.getHighestUnacknowledgedBatchId(txn));\r\n}\r\n/**\r\n * Returns the last consistent snapshot processed (used by the RemoteStore to\r\n * determine whether to buffer incoming snapshots from the backend).\r\n */\r\nfunction localStoreGetLastRemoteSnapshotVersion(localStore) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence.runTransaction('Get last remote snapshot version', 'readonly', txn => localStoreImpl.targetCache.getLastRemoteSnapshotVersion(txn));\r\n}\r\n/**\r\n * Updates the \"ground-state\" (remote) documents. We assume that the remote\r\n * event reflects any write batches that have been acknowledged or rejected\r\n * (i.e. we do not re-apply local mutations to updates from this event).\r\n *\r\n * LocalDocuments are re-calculated if there are remaining mutations in the\r\n * queue.\r\n */\r\nfunction localStoreApplyRemoteEventToLocalCache(localStore, remoteEvent) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    const remoteVersion = remoteEvent.snapshotVersion;\r\n    let newTargetDataByTargetMap = localStoreImpl.targetDataByTarget;\r\n    return localStoreImpl.persistence\r\n        .runTransaction('Apply remote event', 'readwrite-primary', txn => {\r\n        const documentBuffer = localStoreImpl.remoteDocuments.newChangeBuffer({\r\n            trackRemovals: true // Make sure document removals show up in `getNewDocumentChanges()`\r\n        });\r\n        // Reset newTargetDataByTargetMap in case this transaction gets re-run.\r\n        newTargetDataByTargetMap = localStoreImpl.targetDataByTarget;\r\n        const promises = [];\r\n        remoteEvent.targetChanges.forEach((change, targetId) => {\r\n            const oldTargetData = newTargetDataByTargetMap.get(targetId);\r\n            if (!oldTargetData) {\r\n                return;\r\n            }\r\n            // Only update the remote keys if the target is still active. This\r\n            // ensures that we can persist the updated target data along with\r\n            // the updated assignment.\r\n            promises.push(localStoreImpl.targetCache\r\n                .removeMatchingKeys(txn, change.removedDocuments, targetId)\r\n                .next(() => {\r\n                return localStoreImpl.targetCache.addMatchingKeys(txn, change.addedDocuments, targetId);\r\n            }));\r\n            let newTargetData = oldTargetData.withSequenceNumber(txn.currentSequenceNumber);\r\n            if (remoteEvent.targetMismatches.has(targetId)) {\r\n                newTargetData = newTargetData\r\n                    .withResumeToken(ByteString.EMPTY_BYTE_STRING, SnapshotVersion.min())\r\n                    .withLastLimboFreeSnapshotVersion(SnapshotVersion.min());\r\n            }\r\n            else if (change.resumeToken.approximateByteSize() > 0) {\r\n                newTargetData = newTargetData.withResumeToken(change.resumeToken, remoteVersion);\r\n            }\r\n            newTargetDataByTargetMap = newTargetDataByTargetMap.insert(targetId, newTargetData);\r\n            // Update the target data if there are target changes (or if\r\n            // sufficient time has passed since the last update).\r\n            if (shouldPersistTargetData(oldTargetData, newTargetData, change)) {\r\n                promises.push(localStoreImpl.targetCache.updateTargetData(txn, newTargetData));\r\n            }\r\n        });\r\n        let changedDocs = mutableDocumentMap();\r\n        let existenceChangedKeys = documentKeySet();\r\n        remoteEvent.documentUpdates.forEach(key => {\r\n            if (remoteEvent.resolvedLimboDocuments.has(key)) {\r\n                promises.push(localStoreImpl.persistence.referenceDelegate.updateLimboDocument(txn, key));\r\n            }\r\n        });\r\n        // Each loop iteration only affects its \"own\" doc, so it's safe to get all\r\n        // the remote documents in advance in a single call.\r\n        promises.push(populateDocumentChangeBuffer(txn, documentBuffer, remoteEvent.documentUpdates).next(result => {\r\n            changedDocs = result.changedDocuments;\r\n            existenceChangedKeys = result.existenceChangedKeys;\r\n        }));\r\n        // HACK: The only reason we allow a null snapshot version is so that we\r\n        // can synthesize remote events when we get permission denied errors while\r\n        // trying to resolve the state of a locally cached document that is in\r\n        // limbo.\r\n        if (!remoteVersion.isEqual(SnapshotVersion.min())) {\r\n            const updateRemoteVersion = localStoreImpl.targetCache\r\n                .getLastRemoteSnapshotVersion(txn)\r\n                .next(lastRemoteSnapshotVersion => {\r\n                return localStoreImpl.targetCache.setTargetsMetadata(txn, txn.currentSequenceNumber, remoteVersion);\r\n            });\r\n            promises.push(updateRemoteVersion);\r\n        }\r\n        return PersistencePromise.waitFor(promises)\r\n            .next(() => documentBuffer.apply(txn))\r\n            .next(() => localStoreImpl.localDocuments.getLocalViewOfDocuments(txn, changedDocs, existenceChangedKeys))\r\n            .next(() => changedDocs);\r\n    })\r\n        .then(changedDocs => {\r\n        localStoreImpl.targetDataByTarget = newTargetDataByTargetMap;\r\n        return changedDocs;\r\n    });\r\n}\r\n/**\r\n * Populates document change buffer with documents from backend or a bundle.\r\n * Returns the document changes resulting from applying those documents, and\r\n * also a set of documents whose existence state are changed as a result.\r\n *\r\n * @param txn - Transaction to use to read existing documents from storage.\r\n * @param documentBuffer - Document buffer to collect the resulted changes to be\r\n *        applied to storage.\r\n * @param documents - Documents to be applied.\r\n */\r\nfunction populateDocumentChangeBuffer(txn, documentBuffer, documents) {\r\n    let updatedKeys = documentKeySet();\r\n    let existenceChangedKeys = documentKeySet();\r\n    documents.forEach(k => (updatedKeys = updatedKeys.add(k)));\r\n    return documentBuffer.getEntries(txn, updatedKeys).next(existingDocs => {\r\n        let changedDocuments = mutableDocumentMap();\r\n        documents.forEach((key, doc) => {\r\n            const existingDoc = existingDocs.get(key);\r\n            // Check if see if there is a existence state change for this document.\r\n            if (doc.isFoundDocument() !== existingDoc.isFoundDocument()) {\r\n                existenceChangedKeys = existenceChangedKeys.add(key);\r\n            }\r\n            // Note: The order of the steps below is important, since we want\r\n            // to ensure that rejected limbo resolutions (which fabricate\r\n            // NoDocuments with SnapshotVersion.min()) never add documents to\r\n            // cache.\r\n            if (doc.isNoDocument() && doc.version.isEqual(SnapshotVersion.min())) {\r\n                // NoDocuments with SnapshotVersion.min() are used in manufactured\r\n                // events. We remove these documents from cache since we lost\r\n                // access.\r\n                documentBuffer.removeEntry(key, doc.readTime);\r\n                changedDocuments = changedDocuments.insert(key, doc);\r\n            }\r\n            else if (!existingDoc.isValidDocument() ||\r\n                doc.version.compareTo(existingDoc.version) > 0 ||\r\n                (doc.version.compareTo(existingDoc.version) === 0 &&\r\n                    existingDoc.hasPendingWrites)) {\r\n                documentBuffer.addEntry(doc);\r\n                changedDocuments = changedDocuments.insert(key, doc);\r\n            }\r\n            else {\r\n                logDebug(LOG_TAG$b, 'Ignoring outdated watch update for ', key, '. Current version:', existingDoc.version, ' Watch version:', doc.version);\r\n            }\r\n        });\r\n        return { changedDocuments, existenceChangedKeys };\r\n    });\r\n}\r\n/**\r\n * Returns true if the newTargetData should be persisted during an update of\r\n * an active target. TargetData should always be persisted when a target is\r\n * being released and should not call this function.\r\n *\r\n * While the target is active, TargetData updates can be omitted when nothing\r\n * about the target has changed except metadata like the resume token or\r\n * snapshot version. Occasionally it's worth the extra write to prevent these\r\n * values from getting too stale after a crash, but this doesn't have to be\r\n * too frequent.\r\n */\r\nfunction shouldPersistTargetData(oldTargetData, newTargetData, change) {\r\n    // Always persist target data if we don't already have a resume token.\r\n    if (oldTargetData.resumeToken.approximateByteSize() === 0) {\r\n        return true;\r\n    }\r\n    // Don't allow resume token changes to be buffered indefinitely. This\r\n    // allows us to be reasonably up-to-date after a crash and avoids needing\r\n    // to loop over all active queries on shutdown. Especially in the browser\r\n    // we may not get time to do anything interesting while the current tab is\r\n    // closing.\r\n    const timeDelta = newTargetData.snapshotVersion.toMicroseconds() -\r\n        oldTargetData.snapshotVersion.toMicroseconds();\r\n    if (timeDelta >= RESUME_TOKEN_MAX_AGE_MICROS) {\r\n        return true;\r\n    }\r\n    // Otherwise if the only thing that has changed about a target is its resume\r\n    // token it's not worth persisting. Note that the RemoteStore keeps an\r\n    // in-memory view of the currently active targets which includes the current\r\n    // resume token, so stream failure or user changes will still use an\r\n    // up-to-date resume token regardless of what we do here.\r\n    const changes = change.addedDocuments.size +\r\n        change.modifiedDocuments.size +\r\n        change.removedDocuments.size;\r\n    return changes > 0;\r\n}\r\n/**\r\n * Notifies local store of the changed views to locally pin documents.\r\n */\r\nasync function localStoreNotifyLocalViewChanges(localStore, viewChanges) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    try {\r\n        await localStoreImpl.persistence.runTransaction('notifyLocalViewChanges', 'readwrite', txn => {\r\n            return PersistencePromise.forEach(viewChanges, (viewChange) => {\r\n                return PersistencePromise.forEach(viewChange.addedKeys, (key) => localStoreImpl.persistence.referenceDelegate.addReference(txn, viewChange.targetId, key)).next(() => PersistencePromise.forEach(viewChange.removedKeys, (key) => localStoreImpl.persistence.referenceDelegate.removeReference(txn, viewChange.targetId, key)));\r\n            });\r\n        });\r\n    }\r\n    catch (e) {\r\n        if (isIndexedDbTransactionError(e)) {\r\n            // If `notifyLocalViewChanges` fails, we did not advance the sequence\r\n            // number for the documents that were included in this transaction.\r\n            // This might trigger them to be deleted earlier than they otherwise\r\n            // would have, but it should not invalidate the integrity of the data.\r\n            logDebug(LOG_TAG$b, 'Failed to update sequence numbers: ' + e);\r\n        }\r\n        else {\r\n            throw e;\r\n        }\r\n    }\r\n    for (const viewChange of viewChanges) {\r\n        const targetId = viewChange.targetId;\r\n        if (!viewChange.fromCache) {\r\n            const targetData = localStoreImpl.targetDataByTarget.get(targetId);\r\n            // Advance the last limbo free snapshot version\r\n            const lastLimboFreeSnapshotVersion = targetData.snapshotVersion;\r\n            const updatedTargetData = targetData.withLastLimboFreeSnapshotVersion(lastLimboFreeSnapshotVersion);\r\n            localStoreImpl.targetDataByTarget =\r\n                localStoreImpl.targetDataByTarget.insert(targetId, updatedTargetData);\r\n        }\r\n    }\r\n}\r\n/**\r\n * Gets the mutation batch after the passed in batchId in the mutation queue\r\n * or null if empty.\r\n * @param afterBatchId - If provided, the batch to search after.\r\n * @returns The next mutation or null if there wasn't one.\r\n */\r\nfunction localStoreGetNextMutationBatch(localStore, afterBatchId) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence.runTransaction('Get next mutation batch', 'readonly', txn => {\r\n        if (afterBatchId === undefined) {\r\n            afterBatchId = BATCHID_UNKNOWN;\r\n        }\r\n        return localStoreImpl.mutationQueue.getNextMutationBatchAfterBatchId(txn, afterBatchId);\r\n    });\r\n}\r\n/**\r\n * Reads the current value of a Document with a given key or null if not\r\n * found - used for testing.\r\n */\r\nfunction localStoreReadDocument(localStore, key) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence.runTransaction('read document', 'readonly', txn => localStoreImpl.localDocuments.getDocument(txn, key));\r\n}\r\n/**\r\n * Assigns the given target an internal ID so that its results can be pinned so\r\n * they don't get GC'd. A target must be allocated in the local store before\r\n * the store can be used to manage its view.\r\n *\r\n * Allocating an already allocated `Target` will return the existing `TargetData`\r\n * for that `Target`.\r\n */\r\nfunction localStoreAllocateTarget(localStore, target) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence\r\n        .runTransaction('Allocate target', 'readwrite', txn => {\r\n        let targetData;\r\n        return localStoreImpl.targetCache\r\n            .getTargetData(txn, target)\r\n            .next((cached) => {\r\n            if (cached) {\r\n                // This target has been listened to previously, so reuse the\r\n                // previous targetID.\r\n                // TODO(mcg): freshen last accessed date?\r\n                targetData = cached;\r\n                return PersistencePromise.resolve(targetData);\r\n            }\r\n            else {\r\n                return localStoreImpl.targetCache\r\n                    .allocateTargetId(txn)\r\n                    .next(targetId => {\r\n                    targetData = new TargetData(target, targetId, 0 /* Listen */, txn.currentSequenceNumber);\r\n                    return localStoreImpl.targetCache\r\n                        .addTargetData(txn, targetData)\r\n                        .next(() => targetData);\r\n                });\r\n            }\r\n        });\r\n    })\r\n        .then(targetData => {\r\n        // If Multi-Tab is enabled, the existing target data may be newer than\r\n        // the in-memory data\r\n        const cachedTargetData = localStoreImpl.targetDataByTarget.get(targetData.targetId);\r\n        if (cachedTargetData === null ||\r\n            targetData.snapshotVersion.compareTo(cachedTargetData.snapshotVersion) >\r\n                0) {\r\n            localStoreImpl.targetDataByTarget =\r\n                localStoreImpl.targetDataByTarget.insert(targetData.targetId, targetData);\r\n            localStoreImpl.targetIdByTarget.set(target, targetData.targetId);\r\n        }\r\n        return targetData;\r\n    });\r\n}\r\n/**\r\n * Returns the TargetData as seen by the LocalStore, including updates that may\r\n * have not yet been persisted to the TargetCache.\r\n */\r\n// Visible for testing.\r\nfunction localStoreGetTargetData(localStore, transaction, target) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    const targetId = localStoreImpl.targetIdByTarget.get(target);\r\n    if (targetId !== undefined) {\r\n        return PersistencePromise.resolve(localStoreImpl.targetDataByTarget.get(targetId));\r\n    }\r\n    else {\r\n        return localStoreImpl.targetCache.getTargetData(transaction, target);\r\n    }\r\n}\r\n/**\r\n * Unpins all the documents associated with the given target. If\r\n * `keepPersistedTargetData` is set to false and Eager GC enabled, the method\r\n * directly removes the associated target data from the target cache.\r\n *\r\n * Releasing a non-existing `Target` is a no-op.\r\n */\r\n// PORTING NOTE: `keepPersistedTargetData` is multi-tab only.\r\nasync function localStoreReleaseTarget(localStore, targetId, keepPersistedTargetData) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    const targetData = localStoreImpl.targetDataByTarget.get(targetId);\r\n    const mode = keepPersistedTargetData ? 'readwrite' : 'readwrite-primary';\r\n    try {\r\n        if (!keepPersistedTargetData) {\r\n            await localStoreImpl.persistence.runTransaction('Release target', mode, txn => {\r\n                return localStoreImpl.persistence.referenceDelegate.removeTarget(txn, targetData);\r\n            });\r\n        }\r\n    }\r\n    catch (e) {\r\n        if (isIndexedDbTransactionError(e)) {\r\n            // All `releaseTarget` does is record the final metadata state for the\r\n            // target, but we've been recording this periodically during target\r\n            // activity. If we lose this write this could cause a very slight\r\n            // difference in the order of target deletion during GC, but we\r\n            // don't define exact LRU semantics so this is acceptable.\r\n            logDebug(LOG_TAG$b, `Failed to update sequence numbers for target ${targetId}: ${e}`);\r\n        }\r\n        else {\r\n            throw e;\r\n        }\r\n    }\r\n    localStoreImpl.targetDataByTarget =\r\n        localStoreImpl.targetDataByTarget.remove(targetId);\r\n    localStoreImpl.targetIdByTarget.delete(targetData.target);\r\n}\r\n/**\r\n * Runs the specified query against the local store and returns the results,\r\n * potentially taking advantage of query data from previous executions (such\r\n * as the set of remote keys).\r\n *\r\n * @param usePreviousResults - Whether results from previous executions can\r\n * be used to optimize this query execution.\r\n */\r\nfunction localStoreExecuteQuery(localStore, query, usePreviousResults) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    let lastLimboFreeSnapshotVersion = SnapshotVersion.min();\r\n    let remoteKeys = documentKeySet();\r\n    return localStoreImpl.persistence.runTransaction('Execute query', 'readonly', txn => {\r\n        return localStoreGetTargetData(localStoreImpl, txn, queryToTarget(query))\r\n            .next(targetData => {\r\n            if (targetData) {\r\n                lastLimboFreeSnapshotVersion =\r\n                    targetData.lastLimboFreeSnapshotVersion;\r\n                return localStoreImpl.targetCache\r\n                    .getMatchingKeysForTargetId(txn, targetData.targetId)\r\n                    .next(result => {\r\n                    remoteKeys = result;\r\n                });\r\n            }\r\n        })\r\n            .next(() => localStoreImpl.queryEngine.getDocumentsMatchingQuery(txn, query, usePreviousResults\r\n            ? lastLimboFreeSnapshotVersion\r\n            : SnapshotVersion.min(), usePreviousResults ? remoteKeys : documentKeySet()))\r\n            .next(documents => {\r\n            setMaxReadTime(localStoreImpl, queryCollectionGroup(query), documents);\r\n            return { documents, remoteKeys };\r\n        });\r\n    });\r\n}\r\nfunction applyWriteToRemoteDocuments(localStoreImpl, txn, batchResult, documentBuffer) {\r\n    const batch = batchResult.batch;\r\n    const docKeys = batch.keys();\r\n    let promiseChain = PersistencePromise.resolve();\r\n    docKeys.forEach(docKey => {\r\n        promiseChain = promiseChain\r\n            .next(() => documentBuffer.getEntry(txn, docKey))\r\n            .next(doc => {\r\n            const ackVersion = batchResult.docVersions.get(docKey);\r\n            hardAssert(ackVersion !== null);\r\n            if (doc.version.compareTo(ackVersion) < 0) {\r\n                batch.applyToRemoteDocument(doc, batchResult);\r\n                if (doc.isValidDocument()) {\r\n                    // We use the commitVersion as the readTime rather than the\r\n                    // document's updateTime since the updateTime is not advanced\r\n                    // for updates that do not modify the underlying document.\r\n                    doc.setReadTime(batchResult.commitVersion);\r\n                    documentBuffer.addEntry(doc);\r\n                }\r\n            }\r\n        });\r\n    });\r\n    return promiseChain.next(() => localStoreImpl.mutationQueue.removeMutationBatch(txn, batch));\r\n}\r\n/** Returns the local view of the documents affected by a mutation batch. */\r\n// PORTING NOTE: Multi-Tab only.\r\nfunction localStoreLookupMutationDocuments(localStore, batchId) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    const mutationQueueImpl = debugCast(localStoreImpl.mutationQueue);\r\n    return localStoreImpl.persistence.runTransaction('Lookup mutation documents', 'readonly', txn => {\r\n        return mutationQueueImpl.lookupMutationKeys(txn, batchId).next(keys => {\r\n            if (keys) {\r\n                return localStoreImpl.localDocuments.getDocuments(txn, keys);\r\n            }\r\n            else {\r\n                return PersistencePromise.resolve(null);\r\n            }\r\n        });\r\n    });\r\n}\r\n// PORTING NOTE: Multi-Tab only.\r\nfunction localStoreRemoveCachedMutationBatchMetadata(localStore, batchId) {\r\n    const mutationQueueImpl = debugCast(debugCast(localStore, LocalStoreImpl).mutationQueue);\r\n    mutationQueueImpl.removeCachedMutationKeys(batchId);\r\n}\r\n// PORTING NOTE: Multi-Tab only.\r\nfunction localStoreGetActiveClients(localStore) {\r\n    const persistenceImpl = debugCast(debugCast(localStore, LocalStoreImpl).persistence);\r\n    return persistenceImpl.getActiveClients();\r\n}\r\n// PORTING NOTE: Multi-Tab only.\r\nfunction localStoreGetCachedTarget(localStore, targetId) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    const targetCacheImpl = debugCast(localStoreImpl.targetCache);\r\n    const cachedTargetData = localStoreImpl.targetDataByTarget.get(targetId);\r\n    if (cachedTargetData) {\r\n        return Promise.resolve(cachedTargetData.target);\r\n    }\r\n    else {\r\n        return localStoreImpl.persistence.runTransaction('Get target data', 'readonly', txn => {\r\n            return targetCacheImpl\r\n                .getTargetDataForTarget(txn, targetId)\r\n                .next(targetData => (targetData ? targetData.target : null));\r\n        });\r\n    }\r\n}\r\n/**\r\n * Returns the set of documents that have been updated since the last call.\r\n * If this is the first call, returns the set of changes since client\r\n * initialization. Further invocations will return document that have changed\r\n * since the prior call.\r\n */\r\n// PORTING NOTE: Multi-Tab only.\r\nfunction localStoreGetNewDocumentChanges(localStore, collectionGroup) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    // Get the current maximum read time for the collection. This should always\r\n    // exist, but to reduce the chance for regressions we default to\r\n    // SnapshotVersion.Min()\r\n    // TODO(indexing): Consider removing the default value.\r\n    const readTime = localStoreImpl.collectionGroupReadTime.get(collectionGroup) ||\r\n        SnapshotVersion.min();\r\n    return localStoreImpl.persistence\r\n        .runTransaction('Get new document changes', 'readonly', txn => localStoreImpl.remoteDocuments.getAllFromCollectionGroup(txn, collectionGroup, newIndexOffsetSuccessorFromReadTime(readTime, INITIAL_LARGEST_BATCH_ID), \r\n    /* limit= */ Number.MAX_SAFE_INTEGER))\r\n        .then(changedDocs => {\r\n        setMaxReadTime(localStoreImpl, collectionGroup, changedDocs);\r\n        return changedDocs;\r\n    });\r\n}\r\n/** Sets the collection group's maximum read time from the given documents. */\r\n// PORTING NOTE: Multi-Tab only.\r\nfunction setMaxReadTime(localStoreImpl, collectionGroup, changedDocs) {\r\n    let readTime = SnapshotVersion.min();\r\n    changedDocs.forEach((_, doc) => {\r\n        if (doc.readTime.compareTo(readTime) > 0) {\r\n            readTime = doc.readTime;\r\n        }\r\n    });\r\n    localStoreImpl.collectionGroupReadTime.set(collectionGroup, readTime);\r\n}\r\n/**\r\n * Creates a new target using the given bundle name, which will be used to\r\n * hold the keys of all documents from the bundle in query-document mappings.\r\n * This ensures that the loaded documents do not get garbage collected\r\n * right away.\r\n */\r\nfunction umbrellaTarget(bundleName) {\r\n    // It is OK that the path used for the query is not valid, because this will\r\n    // not be read and queried.\r\n    return queryToTarget(newQueryForPath(ResourcePath.fromString(`__bundle__/docs/${bundleName}`)));\r\n}\r\n/**\r\n * Applies the documents from a bundle to the \"ground-state\" (remote)\r\n * documents.\r\n *\r\n * LocalDocuments are re-calculated if there are remaining mutations in the\r\n * queue.\r\n */\r\nasync function localStoreApplyBundledDocuments(localStore, bundleConverter, documents, bundleName) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    let documentKeys = documentKeySet();\r\n    let documentMap = mutableDocumentMap();\r\n    for (const bundleDoc of documents) {\r\n        const documentKey = bundleConverter.toDocumentKey(bundleDoc.metadata.name);\r\n        if (bundleDoc.document) {\r\n            documentKeys = documentKeys.add(documentKey);\r\n        }\r\n        const doc = bundleConverter.toMutableDocument(bundleDoc);\r\n        doc.setReadTime(bundleConverter.toSnapshotVersion(bundleDoc.metadata.readTime));\r\n        documentMap = documentMap.insert(documentKey, doc);\r\n    }\r\n    const documentBuffer = localStoreImpl.remoteDocuments.newChangeBuffer({\r\n        trackRemovals: true // Make sure document removals show up in `getNewDocumentChanges()`\r\n    });\r\n    // Allocates a target to hold all document keys from the bundle, such that\r\n    // they will not get garbage collected right away.\r\n    const umbrellaTargetData = await localStoreAllocateTarget(localStoreImpl, umbrellaTarget(bundleName));\r\n    return localStoreImpl.persistence.runTransaction('Apply bundle documents', 'readwrite', txn => {\r\n        return populateDocumentChangeBuffer(txn, documentBuffer, documentMap)\r\n            .next(documentChangeResult => {\r\n            documentBuffer.apply(txn);\r\n            return documentChangeResult;\r\n        })\r\n            .next(documentChangeResult => {\r\n            return localStoreImpl.targetCache\r\n                .removeMatchingKeysForTargetId(txn, umbrellaTargetData.targetId)\r\n                .next(() => localStoreImpl.targetCache.addMatchingKeys(txn, documentKeys, umbrellaTargetData.targetId))\r\n                .next(() => localStoreImpl.localDocuments.getLocalViewOfDocuments(txn, documentChangeResult.changedDocuments, documentChangeResult.existenceChangedKeys))\r\n                .next(() => documentChangeResult.changedDocuments);\r\n        });\r\n    });\r\n}\r\n/**\r\n * Returns a promise of a boolean to indicate if the given bundle has already\r\n * been loaded and the create time is newer than the current loading bundle.\r\n */\r\nfunction localStoreHasNewerBundle(localStore, bundleMetadata) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    const currentReadTime = fromVersion(bundleMetadata.createTime);\r\n    return localStoreImpl.persistence\r\n        .runTransaction('hasNewerBundle', 'readonly', transaction => {\r\n        return localStoreImpl.bundleCache.getBundleMetadata(transaction, bundleMetadata.id);\r\n    })\r\n        .then(cached => {\r\n        return !!cached && cached.createTime.compareTo(currentReadTime) >= 0;\r\n    });\r\n}\r\n/**\r\n * Saves the given `BundleMetadata` to local persistence.\r\n */\r\nfunction localStoreSaveBundle(localStore, bundleMetadata) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence.runTransaction('Save bundle', 'readwrite', transaction => {\r\n        return localStoreImpl.bundleCache.saveBundleMetadata(transaction, bundleMetadata);\r\n    });\r\n}\r\n/**\r\n * Returns a promise of a `NamedQuery` associated with given query name. Promise\r\n * resolves to undefined if no persisted data can be found.\r\n */\r\nfunction localStoreGetNamedQuery(localStore, queryName) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence.runTransaction('Get named query', 'readonly', transaction => localStoreImpl.bundleCache.getNamedQuery(transaction, queryName));\r\n}\r\n/**\r\n * Saves the given `NamedQuery` to local persistence.\r\n */\r\nasync function localStoreSaveNamedQuery(localStore, query, documents = documentKeySet()) {\r\n    // Allocate a target for the named query such that it can be resumed\r\n    // from associated read time if users use it to listen.\r\n    // NOTE: this also means if no corresponding target exists, the new target\r\n    // will remain active and will not get collected, unless users happen to\r\n    // unlisten the query somehow.\r\n    const allocated = await localStoreAllocateTarget(localStore, queryToTarget(fromBundledQuery(query.bundledQuery)));\r\n    const localStoreImpl = debugCast(localStore);\r\n    return localStoreImpl.persistence.runTransaction('Save named query', 'readwrite', transaction => {\r\n        const readTime = fromVersion(query.readTime);\r\n        // Simply save the query itself if it is older than what the SDK already\r\n        // has.\r\n        if (allocated.snapshotVersion.compareTo(readTime) >= 0) {\r\n            return localStoreImpl.bundleCache.saveNamedQuery(transaction, query);\r\n        }\r\n        // Update existing target data because the query from the bundle is newer.\r\n        const newTargetData = allocated.withResumeToken(ByteString.EMPTY_BYTE_STRING, readTime);\r\n        localStoreImpl.targetDataByTarget =\r\n            localStoreImpl.targetDataByTarget.insert(newTargetData.targetId, newTargetData);\r\n        return localStoreImpl.targetCache\r\n            .updateTargetData(transaction, newTargetData)\r\n            .next(() => localStoreImpl.targetCache.removeMatchingKeysForTargetId(transaction, allocated.targetId))\r\n            .next(() => localStoreImpl.targetCache.addMatchingKeys(transaction, documents, allocated.targetId))\r\n            .next(() => localStoreImpl.bundleCache.saveNamedQuery(transaction, query));\r\n    });\r\n}\r\nasync function localStoreConfigureFieldIndexes(localStore, newFieldIndexes) {\r\n    const localStoreImpl = debugCast(localStore);\r\n    const indexManager = localStoreImpl.indexManager;\r\n    const promises = [];\r\n    return localStoreImpl.persistence.runTransaction('Configure indexes', 'readwrite', transaction => indexManager\r\n        .getFieldIndexes(transaction)\r\n        .next(oldFieldIndexes => diffArrays(oldFieldIndexes, newFieldIndexes, fieldIndexSemanticComparator, fieldIndex => {\r\n        promises.push(indexManager.addFieldIndex(transaction, fieldIndex));\r\n    }, fieldIndex => {\r\n        promises.push(indexManager.deleteFieldIndex(transaction, fieldIndex));\r\n    }))\r\n        .next(() => PersistencePromise.waitFor(promises)));\r\n}\n\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * The Firestore query engine.\r\n *\r\n * Firestore queries can be executed in three modes. The Query Engine determines\r\n * what mode to use based on what data is persisted. The mode only determines\r\n * the runtime complexity of the query - the result set is equivalent across all\r\n * implementations.\r\n *\r\n * The Query engine will use indexed-based execution if a user has configured\r\n * any index that can be used to execute query (via `setIndexConfiguration()`).\r\n * Otherwise, the engine will try to optimize the query by re-using a previously\r\n * persisted query result. If that is not possible, the query will be executed\r\n * via a full collection scan.\r\n *\r\n * Index-based execution is the default when available. The query engine\r\n * supports partial indexed execution and merges the result from the index\r\n * lookup with documents that have not yet been indexed. The index evaluation\r\n * matches the backend's format and as such, the SDK can use indexing for all\r\n * queries that the backend supports.\r\n *\r\n * If no index exists, the query engine tries to take advantage of the target\r\n * document mapping in the TargetCache. These mappings exists for all queries\r\n * that have been synced with the backend at least once and allow the query\r\n * engine to only read documents that previously matched a query plus any\r\n * documents that were edited after the query was last listened to.\r\n *\r\n * There are some cases when this optimization is not guaranteed to produce\r\n * the same results as full collection scans. In these cases, query\r\n * processing falls back to full scans. These cases are:\r\n *\r\n * - Limit queries where a document that matched the query previously no longer\r\n *   matches the query.\r\n *\r\n * - Limit queries where a document edit may cause the document to sort below\r\n *   another document that is in the local cache.\r\n *\r\n * - Queries that have never been CURRENT or free of limbo documents.\r\n */\r\nclass QueryEngine {\r\n    constructor() {\r\n        this.initialized = false;\r\n    }\r\n    /** Sets the document view to query against. */\r\n    initialize(localDocuments, indexManager) {\r\n        this.localDocumentsView = localDocuments;\r\n        this.indexManager = indexManager;\r\n        this.initialized = true;\r\n    }\r\n    /** Returns all local documents matching the specified query. */\r\n    getDocumentsMatchingQuery(transaction, query, lastLimboFreeSnapshotVersion, remoteKeys) {\r\n        return this.performQueryUsingIndex(transaction, query)\r\n            .next(result => result\r\n            ? result\r\n            : this.performQueryUsingRemoteKeys(transaction, query, remoteKeys, lastLimboFreeSnapshotVersion))\r\n            .next(result => result ? result : this.executeFullCollectionScan(transaction, query));\r\n    }\r\n    /**\r\n     * Performs an indexed query that evaluates the query based on a collection's\r\n     * persisted index values. Returns `null` if an index is not available.\r\n     */\r\n    performQueryUsingIndex(transaction, query) {\r\n        if (queryMatchesAllDocuments(query)) {\r\n            // Queries that match all documents don't benefit from using\r\n            // key-based lookups. It is more efficient to scan all documents in a\r\n            // collection, rather than to perform individual lookups.\r\n            return PersistencePromise.resolve(null);\r\n        }\r\n        let target = queryToTarget(query);\r\n        return this.indexManager\r\n            .getIndexType(transaction, target)\r\n            .next(indexType => {\r\n            if (indexType === 0 /* NONE */) {\r\n                // The target cannot be served from any index.\r\n                return null;\r\n            }\r\n            if (query.limit !== null && indexType === 1 /* PARTIAL */) {\r\n                // We cannot apply a limit for targets that are served using a partial\r\n                // index. If a partial index will be used to serve the target, the\r\n                // query may return a superset of documents that match the target\r\n                // (e.g. if the index doesn't include all the target's filters), or\r\n                // may return the correct set of documents in the wrong order (e.g. if\r\n                // the index doesn't include a segment for one of the orderBys).\r\n                // Therefore, a limit should not be applied in such cases.\r\n                query = queryWithLimit(query, null, \"F\" /* First */);\r\n                target = queryToTarget(query);\r\n            }\r\n            return this.indexManager\r\n                .getDocumentsMatchingTarget(transaction, target)\r\n                .next(keys => {\r\n                const sortedKeys = documentKeySet(...keys);\r\n                return this.localDocumentsView\r\n                    .getDocuments(transaction, sortedKeys)\r\n                    .next(indexedDocuments => {\r\n                    return this.indexManager\r\n                        .getMinOffset(transaction, target)\r\n                        .next(offset => {\r\n                        const previousResults = this.applyQuery(query, indexedDocuments);\r\n                        if (this.needsRefill(query, previousResults, sortedKeys, offset.readTime)) {\r\n                            // A limit query whose boundaries change due to local\r\n                            // edits can be re-run against the cache by excluding the\r\n                            // limit. This ensures that all documents that match the\r\n                            // query's filters are included in the result set. The SDK\r\n                            // can then apply the limit once all local edits are\r\n                            // incorporated.\r\n                            return this.performQueryUsingIndex(transaction, queryWithLimit(query, null, \"F\" /* First */));\r\n                        }\r\n                        return this.appendRemainingResults(transaction, previousResults, query, offset);\r\n                    });\r\n                });\r\n            });\r\n        });\r\n    }\r\n    /**\r\n     * Performs a query based on the target's persisted query mapping. Returns\r\n     * `null` if the mapping is not available or cannot be used.\r\n     */\r\n    performQueryUsingRemoteKeys(transaction, query, remoteKeys, lastLimboFreeSnapshotVersion) {\r\n        if (queryMatchesAllDocuments(query)) {\r\n            // Queries that match all documents don't benefit from using\r\n            // key-based lookups. It is more efficient to scan all documents in a\r\n            // collection, rather than to perform individual lookups.\r\n            return this.executeFullCollectionScan(transaction, query);\r\n        }\r\n        // Queries that have never seen a snapshot without limbo free documents\r\n        // should also be run as a full collection scan.\r\n        if (lastLimboFreeSnapshotVersion.isEqual(SnapshotVersion.min())) {\r\n            return this.executeFullCollectionScan(transaction, query);\r\n        }\r\n        return this.localDocumentsView.getDocuments(transaction, remoteKeys).next(documents => {\r\n            const previousResults = this.applyQuery(query, documents);\r\n            if (this.needsRefill(query, previousResults, remoteKeys, lastLimboFreeSnapshotVersion)) {\r\n                return this.executeFullCollectionScan(transaction, query);\r\n            }\r\n            if (getLogLevel() <= LogLevel.DEBUG) {\r\n                logDebug('QueryEngine', 'Re-using previous result from %s to execute query: %s', lastLimboFreeSnapshotVersion.toString(), stringifyQuery(query));\r\n            }\r\n            // Retrieve all results for documents that were updated since the last\r\n            // limbo-document free remote snapshot.\r\n            return this.appendRemainingResults(transaction, previousResults, query, newIndexOffsetSuccessorFromReadTime(lastLimboFreeSnapshotVersion, INITIAL_LARGEST_BATCH_ID));\r\n        });\r\n    }\r\n    /** Applies the query filter and sorting to the provided documents.  */\r\n    applyQuery(query, documents) {\r\n        // Sort the documents and re-apply the query filter since previously\r\n        // matching documents do not necessarily still match the query.\r\n        let queryResults = new SortedSet(newQueryComparator(query));\r\n        documents.forEach((_, maybeDoc) => {\r\n            if (queryMatches(query, maybeDoc)) {\r\n                queryResults = queryResults.add(maybeDoc);\r\n            }\r\n        });\r\n        return queryResults;\r\n    }\r\n    /**\r\n     * Determines if a limit query needs to be refilled from cache, making it\r\n     * ineligible for index-free execution.\r\n     *\r\n     * @param query - The query.\r\n     * @param sortedPreviousResults - The documents that matched the query when it\r\n     * was last synchronized, sorted by the query's comparator.\r\n     * @param remoteKeys - The document keys that matched the query at the last\r\n     * snapshot.\r\n     * @param limboFreeSnapshotVersion - The version of the snapshot when the\r\n     * query was last synchronized.\r\n     */\r\n    needsRefill(query, sortedPreviousResults, remoteKeys, limboFreeSnapshotVersion) {\r\n        if (query.limit === null) {\r\n            // Queries without limits do not need to be refilled.\r\n            return false;\r\n        }\r\n        if (remoteKeys.size !== sortedPreviousResults.size) {\r\n            // The query needs to be refilled if a previously matching document no\r\n            // longer matches.\r\n            return true;\r\n        }\r\n        // Limit queries are not eligible for index-free query execution if there is\r\n        // a potential that an older document from cache now sorts before a document\r\n        // that was previously part of the limit. This, however, can only happen if\r\n        // the document at the edge of the limit goes out of limit.\r\n        // If a document that is not the limit boundary sorts differently,\r\n        // the boundary of the limit itself did not change and documents from cache\r\n        // will continue to be \"rejected\" by this boundary. Therefore, we can ignore\r\n        // any modifications that don't affect the last document.\r\n        const docAtLimitEdge = query.limitType === \"F\" /* First */\r\n            ? sortedPreviousResults.last()\r\n            : sortedPreviousResults.first();\r\n        if (!docAtLimitEdge) {\r\n            // We don't need to refill the query if there were already no documents.\r\n            return false;\r\n        }\r\n        return (docAtLimitEdge.hasPendingWrites ||\r\n            docAtLimitEdge.version.compareTo(limboFreeSnapshotVersion) > 0);\r\n    }\r\n    executeFullCollectionScan(transaction, query) {\r\n        if (getLogLevel() <= LogLevel.DEBUG) {\r\n            logDebug('QueryEngine', 'Using full collection scan to execute query:', stringifyQuery(query));\r\n        }\r\n        return this.localDocumentsView.getDocumentsMatchingQuery(transaction, query, IndexOffset.min());\r\n    }\r\n    /**\r\n     * Combines the results from an indexed execution with the remaining documents\r\n     * that have not yet been indexed.\r\n     */\r\n    appendRemainingResults(transaction, indexedResults, query, offset) {\r\n        // Retrieve all results for documents that were updated since the offset.\r\n        return this.localDocumentsView\r\n            .getDocumentsMatchingQuery(transaction, query, offset)\r\n            .next(remainingResults => {\r\n            // Merge with existing results\r\n            indexedResults.forEach(d => {\r\n                remainingResults = remainingResults.insert(d.key, d);\r\n            });\r\n            return remainingResults;\r\n        });\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// The format of the LocalStorage key that stores the client state is:\r\n//     firestore_clients_<persistence_prefix>_<instance_key>\r\nconst CLIENT_STATE_KEY_PREFIX = 'firestore_clients';\r\n/** Assembles the key for a client state in WebStorage */\r\nfunction createWebStorageClientStateKey(persistenceKey, clientId) {\r\n    return `${CLIENT_STATE_KEY_PREFIX}_${persistenceKey}_${clientId}`;\r\n}\r\n// The format of the WebStorage key that stores the mutation state is:\r\n//     firestore_mutations_<persistence_prefix>_<batch_id>\r\n//     (for unauthenticated users)\r\n// or: firestore_mutations_<persistence_prefix>_<batch_id>_<user_uid>\r\n//\r\n// 'user_uid' is last to avoid needing to escape '_' characters that it might\r\n// contain.\r\nconst MUTATION_BATCH_KEY_PREFIX = 'firestore_mutations';\r\n/** Assembles the key for a mutation batch in WebStorage */\r\nfunction createWebStorageMutationBatchKey(persistenceKey, user, batchId) {\r\n    let mutationKey = `${MUTATION_BATCH_KEY_PREFIX}_${persistenceKey}_${batchId}`;\r\n    if (user.isAuthenticated()) {\r\n        mutationKey += `_${user.uid}`;\r\n    }\r\n    return mutationKey;\r\n}\r\n// The format of the WebStorage key that stores a query target's metadata is:\r\n//     firestore_targets_<persistence_prefix>_<target_id>\r\nconst QUERY_TARGET_KEY_PREFIX = 'firestore_targets';\r\n/** Assembles the key for a query state in WebStorage */\r\nfunction createWebStorageQueryTargetMetadataKey(persistenceKey, targetId) {\r\n    return `${QUERY_TARGET_KEY_PREFIX}_${persistenceKey}_${targetId}`;\r\n}\r\n// The WebStorage prefix that stores the primary tab's online state. The\r\n// format of the key is:\r\n//     firestore_online_state_<persistence_prefix>\r\nconst ONLINE_STATE_KEY_PREFIX = 'firestore_online_state';\r\n/** Assembles the key for the online state of the primary tab. */\r\nfunction createWebStorageOnlineStateKey(persistenceKey) {\r\n    return `${ONLINE_STATE_KEY_PREFIX}_${persistenceKey}`;\r\n}\r\n// The WebStorage prefix that plays as a event to indicate the remote documents\r\n// might have changed due to some secondary tabs loading a bundle.\r\n// format of the key is:\r\n//     firestore_bundle_loaded_v2_<persistenceKey>\r\n// The version ending with \"v2\" stores the list of modified collection groups.\r\nconst BUNDLE_LOADED_KEY_PREFIX = 'firestore_bundle_loaded_v2';\r\nfunction createBundleLoadedKey(persistenceKey) {\r\n    return `${BUNDLE_LOADED_KEY_PREFIX}_${persistenceKey}`;\r\n}\r\n// The WebStorage key prefix for the key that stores the last sequence number allocated. The key\r\n// looks like 'firestore_sequence_number_<persistence_prefix>'.\r\nconst SEQUENCE_NUMBER_KEY_PREFIX = 'firestore_sequence_number';\r\n/** Assembles the key for the current sequence number. */\r\nfunction createWebStorageSequenceNumberKey(persistenceKey) {\r\n    return `${SEQUENCE_NUMBER_KEY_PREFIX}_${persistenceKey}`;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst LOG_TAG$a = 'SharedClientState';\r\n/**\r\n * Holds the state of a mutation batch, including its user ID, batch ID and\r\n * whether the batch is 'pending', 'acknowledged' or 'rejected'.\r\n */\r\n// Visible for testing\r\nclass MutationMetadata {\r\n    constructor(user, batchId, state, error) {\r\n        this.user = user;\r\n        this.batchId = batchId;\r\n        this.state = state;\r\n        this.error = error;\r\n    }\r\n    /**\r\n     * Parses a MutationMetadata from its JSON representation in WebStorage.\r\n     * Logs a warning and returns null if the format of the data is not valid.\r\n     */\r\n    static fromWebStorageEntry(user, batchId, value) {\r\n        const mutationBatch = JSON.parse(value);\r\n        let validData = typeof mutationBatch === 'object' &&\r\n            ['pending', 'acknowledged', 'rejected'].indexOf(mutationBatch.state) !==\r\n                -1 &&\r\n            (mutationBatch.error === undefined ||\r\n                typeof mutationBatch.error === 'object');\r\n        let firestoreError = undefined;\r\n        if (validData && mutationBatch.error) {\r\n            validData =\r\n                typeof mutationBatch.error.message === 'string' &&\r\n                    typeof mutationBatch.error.code === 'string';\r\n            if (validData) {\r\n                firestoreError = new FirestoreError(mutationBatch.error.code, mutationBatch.error.message);\r\n            }\r\n        }\r\n        if (validData) {\r\n            return new MutationMetadata(user, batchId, mutationBatch.state, firestoreError);\r\n        }\r\n        else {\r\n            logError(LOG_TAG$a, `Failed to parse mutation state for ID '${batchId}': ${value}`);\r\n            return null;\r\n        }\r\n    }\r\n    toWebStorageJSON() {\r\n        const batchMetadata = {\r\n            state: this.state,\r\n            updateTimeMs: Date.now() // Modify the existing value to trigger update.\r\n        };\r\n        if (this.error) {\r\n            batchMetadata.error = {\r\n                code: this.error.code,\r\n                message: this.error.message\r\n            };\r\n        }\r\n        return JSON.stringify(batchMetadata);\r\n    }\r\n}\r\n/**\r\n * Holds the state of a query target, including its target ID and whether the\r\n * target is 'not-current', 'current' or 'rejected'.\r\n */\r\n// Visible for testing\r\nclass QueryTargetMetadata {\r\n    constructor(targetId, state, error) {\r\n        this.targetId = targetId;\r\n        this.state = state;\r\n        this.error = error;\r\n    }\r\n    /**\r\n     * Parses a QueryTargetMetadata from its JSON representation in WebStorage.\r\n     * Logs a warning and returns null if the format of the data is not valid.\r\n     */\r\n    static fromWebStorageEntry(targetId, value) {\r\n        const targetState = JSON.parse(value);\r\n        let validData = typeof targetState === 'object' &&\r\n            ['not-current', 'current', 'rejected'].indexOf(targetState.state) !==\r\n                -1 &&\r\n            (targetState.error === undefined ||\r\n                typeof targetState.error === 'object');\r\n        let firestoreError = undefined;\r\n        if (validData && targetState.error) {\r\n            validData =\r\n                typeof targetState.error.message === 'string' &&\r\n                    typeof targetState.error.code === 'string';\r\n            if (validData) {\r\n                firestoreError = new FirestoreError(targetState.error.code, targetState.error.message);\r\n            }\r\n        }\r\n        if (validData) {\r\n            return new QueryTargetMetadata(targetId, targetState.state, firestoreError);\r\n        }\r\n        else {\r\n            logError(LOG_TAG$a, `Failed to parse target state for ID '${targetId}': ${value}`);\r\n            return null;\r\n        }\r\n    }\r\n    toWebStorageJSON() {\r\n        const targetState = {\r\n            state: this.state,\r\n            updateTimeMs: Date.now() // Modify the existing value to trigger update.\r\n        };\r\n        if (this.error) {\r\n            targetState.error = {\r\n                code: this.error.code,\r\n                message: this.error.message\r\n            };\r\n        }\r\n        return JSON.stringify(targetState);\r\n    }\r\n}\r\n/**\r\n * This class represents the immutable ClientState for a client read from\r\n * WebStorage, containing the list of active query targets.\r\n */\r\nclass RemoteClientState {\r\n    constructor(clientId, activeTargetIds) {\r\n        this.clientId = clientId;\r\n        this.activeTargetIds = activeTargetIds;\r\n    }\r\n    /**\r\n     * Parses a RemoteClientState from the JSON representation in WebStorage.\r\n     * Logs a warning and returns null if the format of the data is not valid.\r\n     */\r\n    static fromWebStorageEntry(clientId, value) {\r\n        const clientState = JSON.parse(value);\r\n        let validData = typeof clientState === 'object' &&\r\n            clientState.activeTargetIds instanceof Array;\r\n        let activeTargetIdsSet = targetIdSet();\r\n        for (let i = 0; validData && i < clientState.activeTargetIds.length; ++i) {\r\n            validData = isSafeInteger(clientState.activeTargetIds[i]);\r\n            activeTargetIdsSet = activeTargetIdsSet.add(clientState.activeTargetIds[i]);\r\n        }\r\n        if (validData) {\r\n            return new RemoteClientState(clientId, activeTargetIdsSet);\r\n        }\r\n        else {\r\n            logError(LOG_TAG$a, `Failed to parse client data for instance '${clientId}': ${value}`);\r\n            return null;\r\n        }\r\n    }\r\n}\r\n/**\r\n * This class represents the online state for all clients participating in\r\n * multi-tab. The online state is only written to by the primary client, and\r\n * used in secondary clients to update their query views.\r\n */\r\nclass SharedOnlineState {\r\n    constructor(clientId, onlineState) {\r\n        this.clientId = clientId;\r\n        this.onlineState = onlineState;\r\n    }\r\n    /**\r\n     * Parses a SharedOnlineState from its JSON representation in WebStorage.\r\n     * Logs a warning and returns null if the format of the data is not valid.\r\n     */\r\n    static fromWebStorageEntry(value) {\r\n        const onlineState = JSON.parse(value);\r\n        const validData = typeof onlineState === 'object' &&\r\n            ['Unknown', 'Online', 'Offline'].indexOf(onlineState.onlineState) !==\r\n                -1 &&\r\n            typeof onlineState.clientId === 'string';\r\n        if (validData) {\r\n            return new SharedOnlineState(onlineState.clientId, onlineState.onlineState);\r\n        }\r\n        else {\r\n            logError(LOG_TAG$a, `Failed to parse online state: ${value}`);\r\n            return null;\r\n        }\r\n    }\r\n}\r\n/**\r\n * Metadata state of the local client. Unlike `RemoteClientState`, this class is\r\n * mutable and keeps track of all pending mutations, which allows us to\r\n * update the range of pending mutation batch IDs as new mutations are added or\r\n * removed.\r\n *\r\n * The data in `LocalClientState` is not read from WebStorage and instead\r\n * updated via its instance methods. The updated state can be serialized via\r\n * `toWebStorageJSON()`.\r\n */\r\n// Visible for testing.\r\nclass LocalClientState {\r\n    constructor() {\r\n        this.activeTargetIds = targetIdSet();\r\n    }\r\n    addQueryTarget(targetId) {\r\n        this.activeTargetIds = this.activeTargetIds.add(targetId);\r\n    }\r\n    removeQueryTarget(targetId) {\r\n        this.activeTargetIds = this.activeTargetIds.delete(targetId);\r\n    }\r\n    /**\r\n     * Converts this entry into a JSON-encoded format we can use for WebStorage.\r\n     * Does not encode `clientId` as it is part of the key in WebStorage.\r\n     */\r\n    toWebStorageJSON() {\r\n        const data = {\r\n            activeTargetIds: this.activeTargetIds.toArray(),\r\n            updateTimeMs: Date.now() // Modify the existing value to trigger update.\r\n        };\r\n        return JSON.stringify(data);\r\n    }\r\n}\r\n/**\r\n * `WebStorageSharedClientState` uses WebStorage (window.localStorage) as the\r\n * backing store for the SharedClientState. It keeps track of all active\r\n * clients and supports modifications of the local client's data.\r\n */\r\nclass WebStorageSharedClientState {\r\n    constructor(window, queue, persistenceKey, localClientId, initialUser) {\r\n        this.window = window;\r\n        this.queue = queue;\r\n        this.persistenceKey = persistenceKey;\r\n        this.localClientId = localClientId;\r\n        this.syncEngine = null;\r\n        this.onlineStateHandler = null;\r\n        this.sequenceNumberHandler = null;\r\n        this.storageListener = this.handleWebStorageEvent.bind(this);\r\n        this.activeClients = new SortedMap(primitiveComparator);\r\n        this.started = false;\r\n        /**\r\n         * Captures WebStorage events that occur before `start()` is called. These\r\n         * events are replayed once `WebStorageSharedClientState` is started.\r\n         */\r\n        this.earlyEvents = [];\r\n        // Escape the special characters mentioned here:\r\n        // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions\r\n        const escapedPersistenceKey = persistenceKey.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&');\r\n        this.storage = this.window.localStorage;\r\n        this.currentUser = initialUser;\r\n        this.localClientStorageKey = createWebStorageClientStateKey(this.persistenceKey, this.localClientId);\r\n        this.sequenceNumberKey = createWebStorageSequenceNumberKey(this.persistenceKey);\r\n        this.activeClients = this.activeClients.insert(this.localClientId, new LocalClientState());\r\n        this.clientStateKeyRe = new RegExp(`^${CLIENT_STATE_KEY_PREFIX}_${escapedPersistenceKey}_([^_]*)$`);\r\n        this.mutationBatchKeyRe = new RegExp(`^${MUTATION_BATCH_KEY_PREFIX}_${escapedPersistenceKey}_(\\\\d+)(?:_(.*))?$`);\r\n        this.queryTargetKeyRe = new RegExp(`^${QUERY_TARGET_KEY_PREFIX}_${escapedPersistenceKey}_(\\\\d+)$`);\r\n        this.onlineStateKey = createWebStorageOnlineStateKey(this.persistenceKey);\r\n        this.bundleLoadedKey = createBundleLoadedKey(this.persistenceKey);\r\n        // Rather than adding the storage observer during start(), we add the\r\n        // storage observer during initialization. This ensures that we collect\r\n        // events before other components populate their initial state (during their\r\n        // respective start() calls). Otherwise, we might for example miss a\r\n        // mutation that is added after LocalStore's start() processed the existing\r\n        // mutations but before we observe WebStorage events.\r\n        this.window.addEventListener('storage', this.storageListener);\r\n    }\r\n    /** Returns 'true' if WebStorage is available in the current environment. */\r\n    static isAvailable(window) {\r\n        return !!(window && window.localStorage);\r\n    }\r\n    async start() {\r\n        // Retrieve the list of existing clients to backfill the data in\r\n        // SharedClientState.\r\n        const existingClients = await this.syncEngine.getActiveClients();\r\n        for (const clientId of existingClients) {\r\n            if (clientId === this.localClientId) {\r\n                continue;\r\n            }\r\n            const storageItem = this.getItem(createWebStorageClientStateKey(this.persistenceKey, clientId));\r\n            if (storageItem) {\r\n                const clientState = RemoteClientState.fromWebStorageEntry(clientId, storageItem);\r\n                if (clientState) {\r\n                    this.activeClients = this.activeClients.insert(clientState.clientId, clientState);\r\n                }\r\n            }\r\n        }\r\n        this.persistClientState();\r\n        // Check if there is an existing online state and call the callback handler\r\n        // if applicable.\r\n        const onlineStateJSON = this.storage.getItem(this.onlineStateKey);\r\n        if (onlineStateJSON) {\r\n            const onlineState = this.fromWebStorageOnlineState(onlineStateJSON);\r\n            if (onlineState) {\r\n                this.handleOnlineStateEvent(onlineState);\r\n            }\r\n        }\r\n        for (const event of this.earlyEvents) {\r\n            this.handleWebStorageEvent(event);\r\n        }\r\n        this.earlyEvents = [];\r\n        // Register a window unload hook to remove the client metadata entry from\r\n        // WebStorage even if `shutdown()` was not called.\r\n        this.window.addEventListener('pagehide', () => this.shutdown());\r\n        this.started = true;\r\n    }\r\n    writeSequenceNumber(sequenceNumber) {\r\n        this.setItem(this.sequenceNumberKey, JSON.stringify(sequenceNumber));\r\n    }\r\n    getAllActiveQueryTargets() {\r\n        return this.extractActiveQueryTargets(this.activeClients);\r\n    }\r\n    isActiveQueryTarget(targetId) {\r\n        let found = false;\r\n        this.activeClients.forEach((key, value) => {\r\n            if (value.activeTargetIds.has(targetId)) {\r\n                found = true;\r\n            }\r\n        });\r\n        return found;\r\n    }\r\n    addPendingMutation(batchId) {\r\n        this.persistMutationState(batchId, 'pending');\r\n    }\r\n    updateMutationState(batchId, state, error) {\r\n        this.persistMutationState(batchId, state, error);\r\n        // Once a final mutation result is observed by other clients, they no longer\r\n        // access the mutation's metadata entry. Since WebStorage replays events\r\n        // in order, it is safe to delete the entry right after updating it.\r\n        this.removeMutationState(batchId);\r\n    }\r\n    addLocalQueryTarget(targetId) {\r\n        let queryState = 'not-current';\r\n        // Lookup an existing query state if the target ID was already registered\r\n        // by another tab\r\n        if (this.isActiveQueryTarget(targetId)) {\r\n            const storageItem = this.storage.getItem(createWebStorageQueryTargetMetadataKey(this.persistenceKey, targetId));\r\n            if (storageItem) {\r\n                const metadata = QueryTargetMetadata.fromWebStorageEntry(targetId, storageItem);\r\n                if (metadata) {\r\n                    queryState = metadata.state;\r\n                }\r\n            }\r\n        }\r\n        this.localClientState.addQueryTarget(targetId);\r\n        this.persistClientState();\r\n        return queryState;\r\n    }\r\n    removeLocalQueryTarget(targetId) {\r\n        this.localClientState.removeQueryTarget(targetId);\r\n        this.persistClientState();\r\n    }\r\n    isLocalQueryTarget(targetId) {\r\n        return this.localClientState.activeTargetIds.has(targetId);\r\n    }\r\n    clearQueryState(targetId) {\r\n        this.removeItem(createWebStorageQueryTargetMetadataKey(this.persistenceKey, targetId));\r\n    }\r\n    updateQueryState(targetId, state, error) {\r\n        this.persistQueryTargetState(targetId, state, error);\r\n    }\r\n    handleUserChange(user, removedBatchIds, addedBatchIds) {\r\n        removedBatchIds.forEach(batchId => {\r\n            this.removeMutationState(batchId);\r\n        });\r\n        this.currentUser = user;\r\n        addedBatchIds.forEach(batchId => {\r\n            this.addPendingMutation(batchId);\r\n        });\r\n    }\r\n    setOnlineState(onlineState) {\r\n        this.persistOnlineState(onlineState);\r\n    }\r\n    notifyBundleLoaded(collectionGroups) {\r\n        this.persistBundleLoadedState(collectionGroups);\r\n    }\r\n    shutdown() {\r\n        if (this.started) {\r\n            this.window.removeEventListener('storage', this.storageListener);\r\n            this.removeItem(this.localClientStorageKey);\r\n            this.started = false;\r\n        }\r\n    }\r\n    getItem(key) {\r\n        const value = this.storage.getItem(key);\r\n        logDebug(LOG_TAG$a, 'READ', key, value);\r\n        return value;\r\n    }\r\n    setItem(key, value) {\r\n        logDebug(LOG_TAG$a, 'SET', key, value);\r\n        this.storage.setItem(key, value);\r\n    }\r\n    removeItem(key) {\r\n        logDebug(LOG_TAG$a, 'REMOVE', key);\r\n        this.storage.removeItem(key);\r\n    }\r\n    handleWebStorageEvent(event) {\r\n        // Note: The function is typed to take Event to be interface-compatible with\r\n        // `Window.addEventListener`.\r\n        const storageEvent = event;\r\n        if (storageEvent.storageArea === this.storage) {\r\n            logDebug(LOG_TAG$a, 'EVENT', storageEvent.key, storageEvent.newValue);\r\n            if (storageEvent.key === this.localClientStorageKey) {\r\n                logError('Received WebStorage notification for local change. Another client might have ' +\r\n                    'garbage-collected our state');\r\n                return;\r\n            }\r\n            this.queue.enqueueRetryable(async () => {\r\n                if (!this.started) {\r\n                    this.earlyEvents.push(storageEvent);\r\n                    return;\r\n                }\r\n                if (storageEvent.key === null) {\r\n                    return;\r\n                }\r\n                if (this.clientStateKeyRe.test(storageEvent.key)) {\r\n                    if (storageEvent.newValue != null) {\r\n                        const clientState = this.fromWebStorageClientState(storageEvent.key, storageEvent.newValue);\r\n                        if (clientState) {\r\n                            return this.handleClientStateEvent(clientState.clientId, clientState);\r\n                        }\r\n                    }\r\n                    else {\r\n                        const clientId = this.fromWebStorageClientStateKey(storageEvent.key);\r\n                        return this.handleClientStateEvent(clientId, null);\r\n                    }\r\n                }\r\n                else if (this.mutationBatchKeyRe.test(storageEvent.key)) {\r\n                    if (storageEvent.newValue !== null) {\r\n                        const mutationMetadata = this.fromWebStorageMutationMetadata(storageEvent.key, storageEvent.newValue);\r\n                        if (mutationMetadata) {\r\n                            return this.handleMutationBatchEvent(mutationMetadata);\r\n                        }\r\n                    }\r\n                }\r\n                else if (this.queryTargetKeyRe.test(storageEvent.key)) {\r\n                    if (storageEvent.newValue !== null) {\r\n                        const queryTargetMetadata = this.fromWebStorageQueryTargetMetadata(storageEvent.key, storageEvent.newValue);\r\n                        if (queryTargetMetadata) {\r\n                            return this.handleQueryTargetEvent(queryTargetMetadata);\r\n                        }\r\n                    }\r\n                }\r\n                else if (storageEvent.key === this.onlineStateKey) {\r\n                    if (storageEvent.newValue !== null) {\r\n                        const onlineState = this.fromWebStorageOnlineState(storageEvent.newValue);\r\n                        if (onlineState) {\r\n                            return this.handleOnlineStateEvent(onlineState);\r\n                        }\r\n                    }\r\n                }\r\n                else if (storageEvent.key === this.sequenceNumberKey) {\r\n                    const sequenceNumber = fromWebStorageSequenceNumber(storageEvent.newValue);\r\n                    if (sequenceNumber !== ListenSequence.INVALID) {\r\n                        this.sequenceNumberHandler(sequenceNumber);\r\n                    }\r\n                }\r\n                else if (storageEvent.key === this.bundleLoadedKey) {\r\n                    const collectionGroups = this.fromWebStoreBundleLoadedState(storageEvent.newValue);\r\n                    await Promise.all(collectionGroups.map(cg => this.syncEngine.synchronizeWithChangedDocuments(cg)));\r\n                }\r\n            });\r\n        }\r\n    }\r\n    get localClientState() {\r\n        return this.activeClients.get(this.localClientId);\r\n    }\r\n    persistClientState() {\r\n        this.setItem(this.localClientStorageKey, this.localClientState.toWebStorageJSON());\r\n    }\r\n    persistMutationState(batchId, state, error) {\r\n        const mutationState = new MutationMetadata(this.currentUser, batchId, state, error);\r\n        const mutationKey = createWebStorageMutationBatchKey(this.persistenceKey, this.currentUser, batchId);\r\n        this.setItem(mutationKey, mutationState.toWebStorageJSON());\r\n    }\r\n    removeMutationState(batchId) {\r\n        const mutationKey = createWebStorageMutationBatchKey(this.persistenceKey, this.currentUser, batchId);\r\n        this.removeItem(mutationKey);\r\n    }\r\n    persistOnlineState(onlineState) {\r\n        const entry = {\r\n            clientId: this.localClientId,\r\n            onlineState\r\n        };\r\n        this.storage.setItem(this.onlineStateKey, JSON.stringify(entry));\r\n    }\r\n    persistQueryTargetState(targetId, state, error) {\r\n        const targetKey = createWebStorageQueryTargetMetadataKey(this.persistenceKey, targetId);\r\n        const targetMetadata = new QueryTargetMetadata(targetId, state, error);\r\n        this.setItem(targetKey, targetMetadata.toWebStorageJSON());\r\n    }\r\n    persistBundleLoadedState(collectionGroups) {\r\n        const json = JSON.stringify(Array.from(collectionGroups));\r\n        this.setItem(this.bundleLoadedKey, json);\r\n    }\r\n    /**\r\n     * Parses a client state key in WebStorage. Returns null if the key does not\r\n     * match the expected key format.\r\n     */\r\n    fromWebStorageClientStateKey(key) {\r\n        const match = this.clientStateKeyRe.exec(key);\r\n        return match ? match[1] : null;\r\n    }\r\n    /**\r\n     * Parses a client state in WebStorage. Returns 'null' if the value could not\r\n     * be parsed.\r\n     */\r\n    fromWebStorageClientState(key, value) {\r\n        const clientId = this.fromWebStorageClientStateKey(key);\r\n        return RemoteClientState.fromWebStorageEntry(clientId, value);\r\n    }\r\n    /**\r\n     * Parses a mutation batch state in WebStorage. Returns 'null' if the value\r\n     * could not be parsed.\r\n     */\r\n    fromWebStorageMutationMetadata(key, value) {\r\n        const match = this.mutationBatchKeyRe.exec(key);\r\n        const batchId = Number(match[1]);\r\n        const userId = match[2] !== undefined ? match[2] : null;\r\n        return MutationMetadata.fromWebStorageEntry(new User(userId), batchId, value);\r\n    }\r\n    /**\r\n     * Parses a query target state from WebStorage. Returns 'null' if the value\r\n     * could not be parsed.\r\n     */\r\n    fromWebStorageQueryTargetMetadata(key, value) {\r\n        const match = this.queryTargetKeyRe.exec(key);\r\n        const targetId = Number(match[1]);\r\n        return QueryTargetMetadata.fromWebStorageEntry(targetId, value);\r\n    }\r\n    /**\r\n     * Parses an online state from WebStorage. Returns 'null' if the value\r\n     * could not be parsed.\r\n     */\r\n    fromWebStorageOnlineState(value) {\r\n        return SharedOnlineState.fromWebStorageEntry(value);\r\n    }\r\n    fromWebStoreBundleLoadedState(value) {\r\n        return JSON.parse(value);\r\n    }\r\n    async handleMutationBatchEvent(mutationBatch) {\r\n        if (mutationBatch.user.uid !== this.currentUser.uid) {\r\n            logDebug(LOG_TAG$a, `Ignoring mutation for non-active user ${mutationBatch.user.uid}`);\r\n            return;\r\n        }\r\n        return this.syncEngine.applyBatchState(mutationBatch.batchId, mutationBatch.state, mutationBatch.error);\r\n    }\r\n    handleQueryTargetEvent(targetMetadata) {\r\n        return this.syncEngine.applyTargetState(targetMetadata.targetId, targetMetadata.state, targetMetadata.error);\r\n    }\r\n    handleClientStateEvent(clientId, clientState) {\r\n        const updatedClients = clientState\r\n            ? this.activeClients.insert(clientId, clientState)\r\n            : this.activeClients.remove(clientId);\r\n        const existingTargets = this.extractActiveQueryTargets(this.activeClients);\r\n        const newTargets = this.extractActiveQueryTargets(updatedClients);\r\n        const addedTargets = [];\r\n        const removedTargets = [];\r\n        newTargets.forEach(targetId => {\r\n            if (!existingTargets.has(targetId)) {\r\n                addedTargets.push(targetId);\r\n            }\r\n        });\r\n        existingTargets.forEach(targetId => {\r\n            if (!newTargets.has(targetId)) {\r\n                removedTargets.push(targetId);\r\n            }\r\n        });\r\n        return this.syncEngine.applyActiveTargetsChange(addedTargets, removedTargets).then(() => {\r\n            this.activeClients = updatedClients;\r\n        });\r\n    }\r\n    handleOnlineStateEvent(onlineState) {\r\n        // We check whether the client that wrote this online state is still active\r\n        // by comparing its client ID to the list of clients kept active in\r\n        // IndexedDb. If a client does not update their IndexedDb client state\r\n        // within 5 seconds, it is considered inactive and we don't emit an online\r\n        // state event.\r\n        if (this.activeClients.get(onlineState.clientId)) {\r\n            this.onlineStateHandler(onlineState.onlineState);\r\n        }\r\n    }\r\n    extractActiveQueryTargets(clients) {\r\n        let activeTargets = targetIdSet();\r\n        clients.forEach((kev, value) => {\r\n            activeTargets = activeTargets.unionWith(value.activeTargetIds);\r\n        });\r\n        return activeTargets;\r\n    }\r\n}\r\nfunction fromWebStorageSequenceNumber(seqString) {\r\n    let sequenceNumber = ListenSequence.INVALID;\r\n    if (seqString != null) {\r\n        try {\r\n            const parsed = JSON.parse(seqString);\r\n            hardAssert(typeof parsed === 'number');\r\n            sequenceNumber = parsed;\r\n        }\r\n        catch (e) {\r\n            logError(LOG_TAG$a, 'Failed to read sequence number from WebStorage', e);\r\n        }\r\n    }\r\n    return sequenceNumber;\r\n}\r\n/**\r\n * `MemorySharedClientState` is a simple implementation of SharedClientState for\r\n * clients using memory persistence. The state in this class remains fully\r\n * isolated and no synchronization is performed.\r\n */\r\nclass MemorySharedClientState {\r\n    constructor() {\r\n        this.localState = new LocalClientState();\r\n        this.queryState = {};\r\n        this.onlineStateHandler = null;\r\n        this.sequenceNumberHandler = null;\r\n    }\r\n    addPendingMutation(batchId) {\r\n        // No op.\r\n    }\r\n    updateMutationState(batchId, state, error) {\r\n        // No op.\r\n    }\r\n    addLocalQueryTarget(targetId) {\r\n        this.localState.addQueryTarget(targetId);\r\n        return this.queryState[targetId] || 'not-current';\r\n    }\r\n    updateQueryState(targetId, state, error) {\r\n        this.queryState[targetId] = state;\r\n    }\r\n    removeLocalQueryTarget(targetId) {\r\n        this.localState.removeQueryTarget(targetId);\r\n    }\r\n    isLocalQueryTarget(targetId) {\r\n        return this.localState.activeTargetIds.has(targetId);\r\n    }\r\n    clearQueryState(targetId) {\r\n        delete this.queryState[targetId];\r\n    }\r\n    getAllActiveQueryTargets() {\r\n        return this.localState.activeTargetIds;\r\n    }\r\n    isActiveQueryTarget(targetId) {\r\n        return this.localState.activeTargetIds.has(targetId);\r\n    }\r\n    start() {\r\n        this.localState = new LocalClientState();\r\n        return Promise.resolve();\r\n    }\r\n    handleUserChange(user, removedBatchIds, addedBatchIds) {\r\n        // No op.\r\n    }\r\n    setOnlineState(onlineState) {\r\n        // No op.\r\n    }\r\n    shutdown() { }\r\n    writeSequenceNumber(sequenceNumber) { }\r\n    notifyBundleLoaded(collectionGroups) {\r\n        // No op.\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass NoopConnectivityMonitor {\r\n    addCallback(callback) {\r\n        // No-op.\r\n    }\r\n    shutdown() {\r\n        // No-op.\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Provides a simple helper class that implements the Stream interface to\r\n * bridge to other implementations that are streams but do not implement the\r\n * interface. The stream callbacks are invoked with the callOn... methods.\r\n */\r\nclass StreamBridge {\r\n    constructor(args) {\r\n        this.sendFn = args.sendFn;\r\n        this.closeFn = args.closeFn;\r\n    }\r\n    onOpen(callback) {\r\n        this.wrappedOnOpen = callback;\r\n    }\r\n    onClose(callback) {\r\n        this.wrappedOnClose = callback;\r\n    }\r\n    onMessage(callback) {\r\n        this.wrappedOnMessage = callback;\r\n    }\r\n    close() {\r\n        this.closeFn();\r\n    }\r\n    send(msg) {\r\n        this.sendFn(msg);\r\n    }\r\n    callOnOpen() {\r\n        this.wrappedOnOpen();\r\n    }\r\n    callOnClose(err) {\r\n        this.wrappedOnClose(err);\r\n    }\r\n    callOnMessage(msg) {\r\n        this.wrappedOnMessage(msg);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/*\r\n * Utilities for dealing with node.js-style APIs. See nodePromise for more\r\n * details.\r\n */\r\n/**\r\n * Creates a node-style callback that resolves or rejects a new Promise. The\r\n * callback is passed to the given action which can then use the callback as\r\n * a parameter to a node-style function.\r\n *\r\n * The intent is to directly bridge a node-style function (which takes a\r\n * callback) into a Promise without manually converting between the node-style\r\n * callback and the promise at each call.\r\n *\r\n * In effect it allows you to convert:\r\n *\r\n * @example\r\n * new Promise((resolve: (value?: fs.Stats) => void,\r\n *              reject: (error?: any) => void) => {\r\n *   fs.stat(path, (error?: any, stat?: fs.Stats) => {\r\n *     if (error) {\r\n *       reject(error);\r\n *     } else {\r\n *       resolve(stat);\r\n *     }\r\n *   });\r\n * });\r\n *\r\n * Into\r\n * @example\r\n * nodePromise((callback: NodeCallback<fs.Stats>) => {\r\n *   fs.stat(path, callback);\r\n * });\r\n *\r\n * @param action - a function that takes a node-style callback as an argument\r\n *     and then uses that callback to invoke some node-style API.\r\n * @returns a new Promise which will be rejected if the callback is given the\r\n *     first Error parameter or will resolve to the value given otherwise.\r\n */\r\nfunction nodePromise(action) {\r\n    return new Promise((resolve, reject) => {\r\n        action((error, value) => {\r\n            if (error) {\r\n                reject(error);\r\n            }\r\n            else {\r\n                resolve(value);\r\n            }\r\n        });\r\n    });\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// TODO: Fetch runtime version from grpc-js/package.json instead\r\n// when there's a cleaner way to dynamic require JSON in both Node ESM and CJS\r\nconst grpcVersion = '1.3.7';\r\nconst LOG_TAG$9 = 'Connection';\r\nconst X_GOOG_API_CLIENT_VALUE = `gl-node/${process.versions.node} fire/${SDK_VERSION} grpc/${grpcVersion}`;\r\nfunction createMetadata(databasePath, authToken, appCheckToken, appId) {\r\n    hardAssert(authToken === null || authToken.type === 'OAuth');\r\n    const metadata = new grpc.Metadata();\r\n    if (authToken) {\r\n        authToken.headers.forEach((value, key) => metadata.set(key, value));\r\n    }\r\n    if (appCheckToken) {\r\n        appCheckToken.headers.forEach((value, key) => metadata.set(key, value));\r\n    }\r\n    if (appId) {\r\n        metadata.set('X-Firebase-GMPID', appId);\r\n    }\r\n    metadata.set('X-Goog-Api-Client', X_GOOG_API_CLIENT_VALUE);\r\n    // These headers are used to improve routing and project isolation by the\r\n    // backend.\r\n    // TODO(b/199767712): We are keeping 'Google-Cloud-Resource-Prefix' until Emulators can be\r\n    // released with cl/428820046. Currently blocked because Emulators are now built with Java\r\n    // 11 from Google3.\r\n    metadata.set('Google-Cloud-Resource-Prefix', databasePath);\r\n    metadata.set('x-goog-request-params', databasePath);\r\n    return metadata;\r\n}\r\n/**\r\n * A Connection implemented by GRPC-Node.\r\n */\r\nclass GrpcConnection {\r\n    constructor(protos, databaseInfo) {\r\n        this.databaseInfo = databaseInfo;\r\n        // We cache stubs for the most-recently-used token.\r\n        this.cachedStub = null;\r\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n        this.firestore = protos['google']['firestore']['v1'];\r\n        this.databasePath = `projects/${databaseInfo.databaseId.projectId}/databases/${databaseInfo.databaseId.database}`;\r\n    }\r\n    ensureActiveStub() {\r\n        if (!this.cachedStub) {\r\n            logDebug(LOG_TAG$9, 'Creating Firestore stub.');\r\n            const credentials = this.databaseInfo.ssl\r\n                ? grpc.credentials.createSsl()\r\n                : grpc.credentials.createInsecure();\r\n            this.cachedStub = new this.firestore.Firestore(this.databaseInfo.host, credentials);\r\n        }\r\n        return this.cachedStub;\r\n    }\r\n    invokeRPC(rpcName, path, request, authToken, appCheckToken) {\r\n        const stub = this.ensureActiveStub();\r\n        const metadata = createMetadata(this.databasePath, authToken, appCheckToken, this.databaseInfo.appId);\r\n        const jsonRequest = Object.assign({ database: this.databasePath }, request);\r\n        return nodePromise((callback) => {\r\n            logDebug(LOG_TAG$9, `RPC '${rpcName}' invoked with request:`, request);\r\n            return stub[rpcName](jsonRequest, metadata, (grpcError, value) => {\r\n                if (grpcError) {\r\n                    logDebug(LOG_TAG$9, `RPC '${rpcName}' failed with error:`, grpcError);\r\n                    callback(new FirestoreError(mapCodeFromRpcCode(grpcError.code), grpcError.message));\r\n                }\r\n                else {\r\n                    logDebug(LOG_TAG$9, `RPC '${rpcName}' completed with response:`, value);\r\n                    callback(undefined, value);\r\n                }\r\n            });\r\n        });\r\n    }\r\n    invokeStreamingRPC(rpcName, path, request, authToken, appCheckToken, expectedResponseCount) {\r\n        const results = [];\r\n        const responseDeferred = new Deferred();\r\n        logDebug(LOG_TAG$9, `RPC '${rpcName}' invoked (streaming) with request:`, request);\r\n        const stub = this.ensureActiveStub();\r\n        const metadata = createMetadata(this.databasePath, authToken, appCheckToken, this.databaseInfo.appId);\r\n        const jsonRequest = Object.assign(Object.assign({}, request), { database: this.databasePath });\r\n        const stream = stub[rpcName](jsonRequest, metadata);\r\n        let callbackFired = false;\r\n        stream.on('data', (response) => {\r\n            logDebug(LOG_TAG$9, `RPC ${rpcName} received result:`, response);\r\n            results.push(response);\r\n            if (expectedResponseCount !== undefined &&\r\n                results.length === expectedResponseCount) {\r\n                callbackFired = true;\r\n                responseDeferred.resolve(results);\r\n            }\r\n        });\r\n        stream.on('end', () => {\r\n            logDebug(LOG_TAG$9, `RPC '${rpcName}' completed.`);\r\n            if (!callbackFired) {\r\n                callbackFired = true;\r\n                responseDeferred.resolve(results);\r\n            }\r\n        });\r\n        stream.on('error', (grpcError) => {\r\n            logDebug(LOG_TAG$9, `RPC '${rpcName}' failed with error:`, grpcError);\r\n            const code = mapCodeFromRpcCode(grpcError.code);\r\n            responseDeferred.reject(new FirestoreError(code, grpcError.message));\r\n        });\r\n        return responseDeferred.promise;\r\n    }\r\n    // TODO(mikelehen): This \"method\" is a monster. Should be refactored.\r\n    openStream(rpcName, authToken, appCheckToken) {\r\n        const stub = this.ensureActiveStub();\r\n        const metadata = createMetadata(this.databasePath, authToken, appCheckToken, this.databaseInfo.appId);\r\n        const grpcStream = stub[rpcName](metadata);\r\n        let closed = false;\r\n        const close = (err) => {\r\n            if (!closed) {\r\n                closed = true;\r\n                stream.callOnClose(err);\r\n                grpcStream.end();\r\n            }\r\n        };\r\n        const stream = new StreamBridge({\r\n            sendFn: (msg) => {\r\n                if (!closed) {\r\n                    logDebug(LOG_TAG$9, 'GRPC stream sending:', msg);\r\n                    try {\r\n                        grpcStream.write(msg);\r\n                    }\r\n                    catch (e) {\r\n                        // This probably means we didn't conform to the proto.  Make sure to\r\n                        // log the message we sent.\r\n                        logError('Failure sending:', msg);\r\n                        logError('Error:', e);\r\n                        throw e;\r\n                    }\r\n                }\r\n                else {\r\n                    logDebug(LOG_TAG$9, 'Not sending because gRPC stream is closed:', msg);\r\n                }\r\n            },\r\n            closeFn: () => {\r\n                logDebug(LOG_TAG$9, 'GRPC stream closed locally via close().');\r\n                close();\r\n            }\r\n        });\r\n        grpcStream.on('data', (msg) => {\r\n            if (!closed) {\r\n                logDebug(LOG_TAG$9, 'GRPC stream received:', msg);\r\n                stream.callOnMessage(msg);\r\n            }\r\n        });\r\n        grpcStream.on('end', () => {\r\n            logDebug(LOG_TAG$9, 'GRPC stream ended.');\r\n            close();\r\n        });\r\n        grpcStream.on('error', (grpcError) => {\r\n            if (!closed) {\r\n                logWarn(LOG_TAG$9, 'GRPC stream error. Code:', grpcError.code, 'Message:', grpcError.message);\r\n                const code = mapCodeFromRpcCode(grpcError.code);\r\n                close(new FirestoreError(code, grpcError.message));\r\n            }\r\n        });\r\n        logDebug(LOG_TAG$9, 'Opening GRPC stream');\r\n        // TODO(dimond): Since grpc has no explicit open status (or does it?) we\r\n        // simulate an onOpen in the next loop after the stream had it's listeners\r\n        // registered\r\n        setTimeout(() => {\r\n            stream.callOnOpen();\r\n        }, 0);\r\n        return stream;\r\n    }\r\n}\n\nconst nested = {\n\tgoogle: {\n\t\tnested: {\n\t\t\tprotobuf: {\n\t\t\t\toptions: {\n\t\t\t\t\tcsharp_namespace: \"Google.Protobuf.WellKnownTypes\",\n\t\t\t\t\tgo_package: \"github.com/golang/protobuf/ptypes/wrappers\",\n\t\t\t\t\tjava_package: \"com.google.protobuf\",\n\t\t\t\t\tjava_outer_classname: \"WrappersProto\",\n\t\t\t\t\tjava_multiple_files: true,\n\t\t\t\t\tobjc_class_prefix: \"GPB\",\n\t\t\t\t\tcc_enable_arenas: true,\n\t\t\t\t\toptimize_for: \"SPEED\"\n\t\t\t\t},\n\t\t\t\tnested: {\n\t\t\t\t\tTimestamp: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tseconds: {\n\t\t\t\t\t\t\t\ttype: \"int64\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tnanos: {\n\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tFileDescriptorSet: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tfile: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"FileDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tFileDescriptorProto: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tname: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"package\": {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tdependency: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tpublicDependency: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\tid: 10,\n\t\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\t\tpacked: false\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tweakDependency: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\tid: 11,\n\t\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\t\tpacked: false\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tmessageType: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"DescriptorProto\",\n\t\t\t\t\t\t\t\tid: 4\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tenumType: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"EnumDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 5\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tservice: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"ServiceDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 6\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\textension: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"FieldDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 7\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\ttype: \"FileOptions\",\n\t\t\t\t\t\t\t\tid: 8\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tsourceCodeInfo: {\n\t\t\t\t\t\t\t\ttype: \"SourceCodeInfo\",\n\t\t\t\t\t\t\t\tid: 9\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tsyntax: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 12\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tDescriptorProto: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tname: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tfield: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"FieldDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\textension: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"FieldDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 6\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tnestedType: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"DescriptorProto\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tenumType: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"EnumDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 4\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\textensionRange: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"ExtensionRange\",\n\t\t\t\t\t\t\t\tid: 5\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toneofDecl: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"OneofDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 8\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\ttype: \"MessageOptions\",\n\t\t\t\t\t\t\t\tid: 7\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\treservedRange: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"ReservedRange\",\n\t\t\t\t\t\t\t\tid: 9\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\treservedName: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 10\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\tnested: {\n\t\t\t\t\t\t\tExtensionRange: {\n\t\t\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\t\t\tstart: {\n\t\t\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\tend: {\n\t\t\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tReservedRange: {\n\t\t\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\t\t\tstart: {\n\t\t\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\tend: {\n\t\t\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tFieldDescriptorProto: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tname: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tnumber: {\n\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tlabel: {\n\t\t\t\t\t\t\t\ttype: \"Label\",\n\t\t\t\t\t\t\t\tid: 4\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\ttype: {\n\t\t\t\t\t\t\t\ttype: \"Type\",\n\t\t\t\t\t\t\t\tid: 5\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\ttypeName: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 6\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\textendee: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tdefaultValue: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 7\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toneofIndex: {\n\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\tid: 9\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tjsonName: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 10\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\ttype: \"FieldOptions\",\n\t\t\t\t\t\t\t\tid: 8\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\tnested: {\n\t\t\t\t\t\t\tType: {\n\t\t\t\t\t\t\t\tvalues: {\n\t\t\t\t\t\t\t\t\tTYPE_DOUBLE: 1,\n\t\t\t\t\t\t\t\t\tTYPE_FLOAT: 2,\n\t\t\t\t\t\t\t\t\tTYPE_INT64: 3,\n\t\t\t\t\t\t\t\t\tTYPE_UINT64: 4,\n\t\t\t\t\t\t\t\t\tTYPE_INT32: 5,\n\t\t\t\t\t\t\t\t\tTYPE_FIXED64: 6,\n\t\t\t\t\t\t\t\t\tTYPE_FIXED32: 7,\n\t\t\t\t\t\t\t\t\tTYPE_BOOL: 8,\n\t\t\t\t\t\t\t\t\tTYPE_STRING: 9,\n\t\t\t\t\t\t\t\t\tTYPE_GROUP: 10,\n\t\t\t\t\t\t\t\t\tTYPE_MESSAGE: 11,\n\t\t\t\t\t\t\t\t\tTYPE_BYTES: 12,\n\t\t\t\t\t\t\t\t\tTYPE_UINT32: 13,\n\t\t\t\t\t\t\t\t\tTYPE_ENUM: 14,\n\t\t\t\t\t\t\t\t\tTYPE_SFIXED32: 15,\n\t\t\t\t\t\t\t\t\tTYPE_SFIXED64: 16,\n\t\t\t\t\t\t\t\t\tTYPE_SINT32: 17,\n\t\t\t\t\t\t\t\t\tTYPE_SINT64: 18\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tLabel: {\n\t\t\t\t\t\t\t\tvalues: {\n\t\t\t\t\t\t\t\t\tLABEL_OPTIONAL: 1,\n\t\t\t\t\t\t\t\t\tLABEL_REQUIRED: 2,\n\t\t\t\t\t\t\t\t\tLABEL_REPEATED: 3\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tOneofDescriptorProto: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tname: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\ttype: \"OneofOptions\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tEnumDescriptorProto: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tname: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tvalue: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"EnumValueDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\ttype: \"EnumOptions\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tEnumValueDescriptorProto: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tname: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tnumber: {\n\t\t\t\t\t\t\t\ttype: \"int32\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\ttype: \"EnumValueOptions\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tServiceDescriptorProto: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tname: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tmethod: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"MethodDescriptorProto\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\ttype: \"ServiceOptions\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tMethodDescriptorProto: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tname: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tinputType: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toutputType: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\ttype: \"MethodOptions\",\n\t\t\t\t\t\t\t\tid: 4\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tclientStreaming: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 5\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tserverStreaming: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 6\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tFileOptions: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tjavaPackage: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tjavaOuterClassname: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 8\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tjavaMultipleFiles: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 10\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tjavaGenerateEqualsAndHash: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 20,\n\t\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\t\tdeprecated: true\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tjavaStringCheckUtf8: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 27\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\toptimizeFor: {\n\t\t\t\t\t\t\t\ttype: \"OptimizeMode\",\n\t\t\t\t\t\t\t\tid: 9,\n\t\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\t\t\"default\": \"SPEED\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tgoPackage: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 11\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tccGenericServices: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 16\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tjavaGenericServices: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 17\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tpyGenericServices: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 18\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tdeprecated: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 23\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tccEnableArenas: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 31\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tobjcClassPrefix: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 36\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tcsharpNamespace: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 37\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tuninterpretedOption: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"UninterpretedOption\",\n\t\t\t\t\t\t\t\tid: 999\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\textensions: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t1000,\n\t\t\t\t\t\t\t\t536870911\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t],\n\t\t\t\t\t\treserved: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t38,\n\t\t\t\t\t\t\t\t38\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t],\n\t\t\t\t\t\tnested: {\n\t\t\t\t\t\t\tOptimizeMode: {\n\t\t\t\t\t\t\t\tvalues: {\n\t\t\t\t\t\t\t\t\tSPEED: 1,\n\t\t\t\t\t\t\t\t\tCODE_SIZE: 2,\n\t\t\t\t\t\t\t\t\tLITE_RUNTIME: 3\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tMessageOptions: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tmessageSetWireFormat: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tnoStandardDescriptorAccessor: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tdeprecated: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tmapEntry: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 7\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tuninterpretedOption: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"UninterpretedOption\",\n\t\t\t\t\t\t\t\tid: 999\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\textensions: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t1000,\n\t\t\t\t\t\t\t\t536870911\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t],\n\t\t\t\t\t\treserved: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t8,\n\t\t\t\t\t\t\t\t8\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t]\n\t\t\t\t\t},\n\t\t\t\t\tFieldOptions: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tctype: {\n\t\t\t\t\t\t\t\ttype: \"CType\",\n\t\t\t\t\t\t\t\tid: 1,\n\t\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\t\t\"default\": \"STRING\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tpacked: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tjstype: {\n\t\t\t\t\t\t\t\ttype: \"JSType\",\n\t\t\t\t\t\t\t\tid: 6,\n\t\t\t\t\t\t\t\toptions: {\n\t\t\t\t\t\t\t\t\t\"default\": \"JS_NORMAL\"\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tlazy: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 5\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tdeprecated: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tweak: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 10\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tuninterpretedOption: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"UninterpretedOption\",\n\t\t\t\t\t\t\t\tid: 999\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\textensions: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t1000,\n\t\t\t\t\t\t\t\t536870911\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t],\n\t\t\t\t\t\treserved: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t4,\n\t\t\t\t\t\t\t\t4\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t],\n\t\t\t\t\t\tnested: {\n\t\t\t\t\t\t\tCType: {\n\t\t\t\t\t\t\t\tvalues: {\n\t\t\t\t\t\t\t\t\tSTRING: 0,\n\t\t\t\t\t\t\t\t\tCORD: 1,\n\t\t\t\t\t\t\t\t\tSTRING_PIECE: 2\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tJSType: {\n\t\t\t\t\t\t\t\tvalues: {\n\t\t\t\t\t\t\t\t\tJS_NORMAL: 0,\n\t\t\t\t\t\t\t\t\tJS_STRING: 1,\n\t\t\t\t\t\t\t\t\tJS_NUMBER: 2\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\tOneofOptions: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tuninterpretedOption: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"UninterpretedOption\",\n\t\t\t\t\t\t\t\tid: 999\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\textensions: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t1000,\n\t\t\t\t\t\t\t\t536870911\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t]\n\t\t\t\t\t},\n\t\t\t\t\tEnumOptions: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tallowAlias: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tdeprecated: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tuninterpretedOption: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"UninterpretedOption\",\n\t\t\t\t\t\t\t\tid: 999\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\textensions: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t1000,\n\t\t\t\t\t\t\t\t536870911\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t]\n\t\t\t\t\t},\n\t\t\t\t\tEnumValueOptions: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tdeprecated: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 1\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tuninterpretedOption: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"UninterpretedOption\",\n\t\t\t\t\t\t\t\tid: 999\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\textensions: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t1000,\n\t\t\t\t\t\t\t\t536870911\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t]\n\t\t\t\t\t},\n\t\t\t\t\tServiceOptions: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tdeprecated: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 33\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tuninterpretedOption: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"UninterpretedOption\",\n\t\t\t\t\t\t\t\tid: 999\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\textensions: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t1000,\n\t\t\t\t\t\t\t\t536870911\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t]\n\t\t\t\t\t},\n\t\t\t\t\tMethodOptions: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tdeprecated: {\n\t\t\t\t\t\t\t\ttype: \"bool\",\n\t\t\t\t\t\t\t\tid: 33\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tuninterpretedOption: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"UninterpretedOption\",\n\t\t\t\t\t\t\t\tid: 999\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\textensions: [\n\t\t\t\t\t\t\t[\n\t\t\t\t\t\t\t\t1000,\n\t\t\t\t\t\t\t\t536870911\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t]\n\t\t\t\t\t},\n\t\t\t\t\tUninterpretedOption: {\n\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\tname: {\n\t\t\t\t\t\t\t\trule: \"repeated\",\n\t\t\t\t\t\t\t\ttype: \"NamePart\",\n\t\t\t\t\t\t\t\tid: 2\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tidentifierValue: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 3\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tpositiveIntValue: {\n\t\t\t\t\t\t\t\ttype: \"uint64\",\n\t\t\t\t\t\t\t\tid: 4\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tnegativeIntValue: {\n\t\t\t\t\t\t\t\ttype: \"int64\",\n\t\t\t\t\t\t\t\tid: 5\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tdoubleValue: {\n\t\t\t\t\t\t\t\ttype: \"double\",\n\t\t\t\t\t\t\t\tid: 6\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tstringValue: {\n\t\t\t\t\t\t\t\ttype: \"bytes\",\n\t\t\t\t\t\t\t\tid: 7\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\taggregateValue: {\n\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\tid: 8\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t},\n\t\t\t\t\t\tnested: {\n\t\t\t\t\t\t\tNamePart: {\n\t\t\t\t\t\t\t\tfields: {\n\t\t\t\t\t\t\t\t\tnamePart: {\n\t\t\t\t\t\t\t\t\t\trule: \"required\",\n\t\t\t\t\t\t\t\t\t\ttype: \"string\",\n\t\t\t\t\t\t\t\t\t\tid: 